{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思路\n",
    "\n",
    "- 将同一篇文章split为2段，相似度为1，不同的文章的相似度为0\n",
    "- jieba分词，将用词向量模型，得到每个词的词向量。\n",
    "- 将词向量输入到rnn，训练相似度模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、数据处理\n",
    "- 数据地址：链接:https://pan.baidu.com/s/1kQsaRgpEAMDib1LRa2QHqw  密码:bq52\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train/体育/300.txt\n"
     ]
    }
   ],
   "source": [
    "# 1、处理数据\n",
    "import os\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import jieba\n",
    "\n",
    "\n",
    "data_dict = defaultdict(list)\n",
    "single_data_dict = defaultdict(list)\n",
    "path = 'data/train'\n",
    "root_dirs = os.listdir(path)\n",
    "\n",
    "\n",
    "for dir_ in root_dirs:\n",
    "    fir_dir = os.path.join(path, dir_)\n",
    "    for root, sec_dir, files in os.walk(fir_dir):\n",
    "        for file in files:\n",
    "            file_abs_path = os.path.join(root, file)\n",
    "            try:\n",
    "                data = open(file_abs_path, 'r', encoding='gbk')\n",
    "                for line in data:\n",
    "                    line = line.replace('$LOTOzf$', '')\n",
    "                    if len(line) < 80:\n",
    "                        single_data_dict[dir_].append(line)\n",
    "                        continue\n",
    "                    fir_sent = line[:len(line) // 2]\n",
    "                    sec_sent = line[len(line) // 2:]\n",
    "                    data_dict[dir_].append(copy.deepcopy([fir_sent, sec_sent]))\n",
    "            except:\n",
    "                print(file_abs_path)\n",
    "                continue          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuhe/Application/python36/lib/python3.6/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmp6ru9qwp9' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.706 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "    \n",
    "def cut_words(text):\n",
    "    return list(jieba.cut(text))\n",
    "    \n",
    "    \n",
    "classify_dict = ['文学', '体育', '女性', '校园']\n",
    "writer = open('data/semantic_disambiguation_lstm.txt', 'w')\n",
    "temp_dict = {}\n",
    "\n",
    "for k, v in data_dict.items():\n",
    "    temp = ['文学', '体育', '女性', '校园']\n",
    "    cur_classify = classify_dict.index(k)\n",
    "    temp.pop(cur_classify)\n",
    "    other_data = [d for i, data in data_dict.items() if i != k for da in data for d in da]\n",
    "    for sents in v:\n",
    "        fir_sent = sents[0]\n",
    "        sec_sent = sents[1]\n",
    "        single_data_dict[k].extend([fir_sent, sec_sent])\n",
    "        random_index = random.randint(0, len(single_data_dict[k])-1)\n",
    "        \n",
    "        random_index2 = random.randint(0,len(other_data)-1)\n",
    "        neg_fir_sent = single_data_dict[k].pop(random_index)\n",
    "        neg_sec_sent = other_data[random_index2]\n",
    "        \n",
    "        fir_sent = cut_words(fir_sent)\n",
    "        sec_sent = cut_words(sec_sent)\n",
    "        neg_fir_sent = cut_words(neg_fir_sent)\n",
    "        neg_sec_sent = cut_words(neg_sec_sent)\n",
    "        \n",
    "        writer_str = f'{k}\\t{fir_sent}\\t{sec_sent}\\t{1}\\n'\n",
    "        writer.write(writer_str)\n",
    "        writer_str = f'{k}\\t{neg_fir_sent}\\t{neg_sec_sent}\\t{0}\\n'\n",
    "        writer.write(writer_str)\n",
    "\n",
    "writer.close()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文词向量模型地址：\n",
    "- 链接:https://pan.baidu.com/s/1vYXKfkNjOSIhUICa_mgl6A  密码:lv08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2、处理词向量,作词向量字典\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "\n",
    "vec_path = 'word_vector/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5'\n",
    "vector_dict = {}\n",
    "with open(vec_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        line = line.split()\n",
    "        vector_dict[line[0]] = np.array(line[1:])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "data_set = open('data/semantic_disambiguation_lstm.txt', 'r').readlines()\n",
    "random.shuffle(data_set)\n",
    "\n",
    "data_array = []\n",
    "for d in data_set:\n",
    "    try:\n",
    "        temp_data = d.split('\\t')\n",
    "        a = eval(temp_data[1])\n",
    "        b = eval(temp_data[2])\n",
    "        lable = int(temp_data[-1].strip('\\n'))\n",
    "        vec_a = np.concatenate([vector_dict.get(word).reshape(300, -1).astype(float) for word in a if vector_dict.get(word) is not None], axis=1)\n",
    "        vec_b = np.concatenate([vector_dict.get(word).reshape(300, -1).astype(float) for word in b if vector_dict.get(word) is not None], axis=1)\n",
    "    except:\n",
    "        continue\n",
    "    data_array.append([vec_a, vec_b, lable])\n",
    "\n",
    "test_set = data_array[:len(data_array) // 7]\n",
    "train_set = data_array[len(data_array) // 7:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123) #保证每次运行初始化的随机数相同\n",
    "\n",
    "# vocab_size = 5000   #词表大小\n",
    "embedding_size = 300   #词向量维度\n",
    "num_classes = 2    #二分类\n",
    "# sentence_max_len = 64  #单个句子的长度\n",
    "hidden_size = 256\n",
    "\n",
    "num_layers = 1  #一层lstm\n",
    "num_directions = 2  #双向lstm\n",
    "lr = 1e-3\n",
    "batch_size = 1  \n",
    "epochs = 6\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Bi-LSTM模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embedding_size,hidden_size, num_layers, num_directions, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.input_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = (num_directions == 2))\n",
    "        self.attention_weights_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.liner = nn.Linear(hidden_size, num_classes)\n",
    "        self.act_func = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #lstm的输入维度为 [seq_len, batch, input_size]\n",
    "        #x [batch_size, sentence_length, embedding_size]\n",
    "        x = x.permute(1, 0, 2)         #[sentence_length, batch_size, embedding_size]\n",
    "        \n",
    "        #由于数据集不一定是预先设置的batch_size的整数倍，所以用size(1)获取当前数据实际的batch\n",
    "        batch_size = x.size(1)\n",
    "        \n",
    "        #设置lstm最初的前项输出\n",
    "        h_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        #out[seq_len, batch, num_directions * hidden_size]。多层lstm，out只保存最后一层每个时间步t的输出h_t\n",
    "        #h_n, c_n [num_layers * num_directions, batch, hidden_size]\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        #将双向lstm的输出拆分为前向输出和后向输出\n",
    "        (forward_out, backward_out) = torch.chunk(out, 2, dim = 2)\n",
    "        out = forward_out + backward_out  #[seq_len, batch, hidden_size]\n",
    "        out = out.permute(1, 0, 2)  #[batch, seq_len, hidden_size]\n",
    "        \n",
    "        #为了使用到lstm最后一个时间步时，每层lstm的表达，用h_n生成attention的权重\n",
    "        h_n = h_n.permute(1, 0, 2)  #[batch, num_layers * num_directions,  hidden_size]\n",
    "        h_n = torch.sum(h_n, dim=1) #[batch, 1,  hidden_size]\n",
    "        h_n = h_n.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        \n",
    "        attention_w = self.attention_weights_layer(h_n)  #[batch, hidden_size]\n",
    "        attention_w = attention_w.unsqueeze(dim=1) #[batch, 1, hidden_size]\n",
    "        # [batch, 1, hidden_size]  * [batch, seq_len, hidden_size]\n",
    "        attention_context = torch.bmm(attention_w, out.transpose(1, 2))  \n",
    "        softmax_w = F.softmax(attention_context, dim=-1)  #[batch, 1, seq_len],权重归一化\n",
    "        x = torch.bmm(softmax_w, out)  #[batch, 1, hidden_size]\n",
    "        x = x.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        x = self.liner(x)\n",
    "        x = self.act_func(x)\n",
    "        return x\n",
    "     \n",
    "        \n",
    "def test(model, test_loader, loss_func):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    corrects = 0.0\n",
    "    for datas in test_loader:\n",
    "        inputs_0 = torch.from_numpy(datas[0]).float()\n",
    "        inputs_1 = torch.from_numpy(datas[1]).float()\n",
    "        if datas[2] == 1:\n",
    "            labels = torch.LongTensor([0, 1]).unsqueeze(0).float()\n",
    "        else:\n",
    "            labels = torch.LongTensor([1, 0]).unsqueeze(0).float()\n",
    "        inputs = torch.cat([inputs_0, inputs_1], dim=1).permute(1, 0).unsqueeze(0) # len x 300\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        preds = model(inputs)\n",
    "        loss = loss_func(preds, labels)\n",
    "        \n",
    "        loss_val += loss.item() * inputs.size(0)\n",
    "        \n",
    "        #获取预测的最大概率出现的位置\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        corrects += torch.sum(preds == labels).item()\n",
    "    test_loss = loss_val / len(test_loader)\n",
    "    test_acc = corrects / len(test_loader)\n",
    "    print(\"Test Loss: {}, Test Acc: {}\".format(test_loss, test_acc))\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "def train(model, train_loader,test_loader, optimizer, loss_func, epochs, model_save_path):\n",
    "    best_val_acc = 0.0\n",
    "    best_model_params = copy.deepcopy(model.state_dict())\n",
    "    train_loss = 0\n",
    "    step = 0\n",
    "    total_train_loss = []\n",
    "    total_eval_loss = []\n",
    "    mean_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_val = 0.0\n",
    "        corrects = 0.0\n",
    "        \n",
    "        for datas in train_loader:\n",
    "            inputs_0 = torch.from_numpy(datas[0]).float()\n",
    "            inputs_1 = torch.from_numpy(datas[1]).float()\n",
    "            if datas[2] == 1:\n",
    "                labels = torch.LongTensor([0, 1]).unsqueeze(0).float()\n",
    "            else:\n",
    "                labels = torch.LongTensor([1, 0]).unsqueeze(0).float()\n",
    "            inputs = torch.cat([inputs_0, inputs_1], dim=1).permute(1, 0).unsqueeze(0) # len x 300\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            preds = model(inputs)\n",
    "            loss = loss_func(preds, labels)\n",
    "            step += 1\n",
    "            train_loss += loss.mean().item()\n",
    "            if step % 10 == 0:\n",
    "                mean_loss.append(train_loss / step)\n",
    "                print(f'Epoch {epoch}/ {step} train_loss  {train_loss / step}')\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_val += loss.item() * inputs.size(0)\n",
    "            total_train_loss.append(loss.item())            \n",
    "            \n",
    "            #获取预测的最大概率出现的位置\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            corrects += torch.sum(preds == labels).item()\n",
    "        train_loss = loss_val / len(train_loader)\n",
    "        train_acc = corrects / len(train_loader)\n",
    "        \n",
    "        if(epoch % 1 == 0):\n",
    "            print(\"Train Loss: {}, Train Acc: {}\".format(train_loss, train_acc))\n",
    "            test_acc = test(model, test_loader, loss_func)\n",
    "            if(best_val_acc < test_acc):\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f'保存模型成功! 路径为: ’{model_save_path}‘')\n",
    "                best_val_acc = test_acc\n",
    "                best_model_params = copy.deepcopy(model.state_dict())\n",
    "    model.load_state_dict(best_model_params)\n",
    "    \n",
    "    return model, total_train_loss, mean_loss\n",
    "\n",
    "\n",
    "def predict(model, origin, entity_descs:list):\n",
    "    \"\"\"\n",
    "    origin: 原始文本\n",
    "    entity_descs:原始文本中具有歧义的实体词的多个异项的相关描述放到entity_descs中。\n",
    "    比如：origin：苹果12马上就要发布了。\n",
    "    歧义实体为”苹果“\n",
    "    entity_descs：\n",
    "    ['《苹果》是由李玉执导，范冰冰、佟大为、梁家辉、金燕玲领衔主演的黑色幽默剧情电影。于2007年5月18日在中国大陆上映。影片讲述了两个贫富家庭之间离奇诡异的矛盾冲突和感情错位的戏剧性故事。'\n",
    ", 'iPhone 12是美国苹果公司研发的iPhone手机，采用了直面边框设计，支持5G，搭载A14 Bionic芯片，双镜头后置摄像头系统。支持北斗导航 [1]  ，有黑色、白色、红色、绿色、蓝色五种配色。'\n",
    "]\n",
    "    entity_descs中的句子分别和origin做相似度得分，得分高的为正确意项\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for entity_desc in entity_descs:\n",
    "        origin_vector = np.concatenate([vector_dict.get(word).reshape(300, -1).astype(float) for word in origin if vector_dict.get(word) is not None], axis=1)\n",
    "        origin_vector = torch.from_numpy(origin_vector).float()\n",
    "        entity_desc_vector = np.concatenate([vector_dict.get(word).reshape(300, -1).astype(float) for word in entity_desc if vector_dict.get(word) is not None], axis=1)\n",
    "        entity_desc_vector = torch.from_numpy(entity_desc_vector).float()\n",
    "        inputs = torch.cat([origin_vector, entity_desc_vector], dim=1).permute(1, 0).unsqueeze(0) # bsz x len x 300\n",
    "        inputs = inputs.to(device)\n",
    "        preds = model(inputs)\n",
    "        score = preds.view(-1).tolist()[1]\n",
    "#         print(preds)\n",
    "        scores.append(score)\n",
    "    scores = torch.tensor(scores)\n",
    "    index = scores.argmax()\n",
    "    return entity_descs[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/ 10 train_loss  0.7609254479408264\n",
      "Epoch 0/ 20 train_loss  0.7499568283557891\n",
      "Epoch 0/ 30 train_loss  0.7316730082035064\n",
      "Epoch 0/ 40 train_loss  0.7303471058607102\n",
      "Epoch 0/ 50 train_loss  0.7237918889522552\n",
      "Epoch 0/ 60 train_loss  0.7180019507805506\n",
      "Epoch 0/ 70 train_loss  0.7168742792946952\n",
      "Epoch 0/ 80 train_loss  0.7173353753983974\n",
      "Epoch 0/ 90 train_loss  0.7143663909700182\n",
      "Epoch 0/ 100 train_loss  0.7075330072641373\n",
      "Epoch 0/ 110 train_loss  0.7070197755640203\n",
      "Epoch 0/ 120 train_loss  0.7114212090770403\n",
      "Epoch 0/ 130 train_loss  0.7098383688009702\n",
      "Epoch 0/ 140 train_loss  0.7037482129676002\n",
      "Epoch 0/ 150 train_loss  0.7035306203365326\n",
      "Epoch 0/ 160 train_loss  0.7040739387273789\n",
      "Epoch 0/ 170 train_loss  0.7044844048864701\n",
      "Epoch 0/ 180 train_loss  0.7009855667750041\n",
      "Epoch 0/ 190 train_loss  0.7045519050798918\n",
      "Epoch 0/ 200 train_loss  0.7066385893523693\n",
      "Epoch 0/ 210 train_loss  0.7044750371149608\n",
      "Epoch 0/ 220 train_loss  0.7037273194302213\n",
      "Epoch 0/ 230 train_loss  0.7018118168996728\n",
      "Epoch 0/ 240 train_loss  0.7018807464589675\n",
      "Epoch 0/ 250 train_loss  0.6991500899791717\n",
      "Epoch 0/ 260 train_loss  0.6982157009152266\n",
      "Epoch 0/ 270 train_loss  0.6972905506690343\n",
      "Epoch 0/ 280 train_loss  0.6972547406596797\n",
      "Epoch 0/ 290 train_loss  0.6989637001835066\n",
      "Epoch 0/ 300 train_loss  0.6952872354288896\n",
      "Epoch 0/ 310 train_loss  0.6910044938325882\n",
      "Epoch 0/ 320 train_loss  0.6929082318674773\n",
      "Epoch 0/ 330 train_loss  0.6902200295618086\n",
      "Epoch 0/ 340 train_loss  0.6865087296594592\n",
      "Epoch 0/ 350 train_loss  0.684355698951653\n",
      "Epoch 0/ 360 train_loss  0.6873015344349874\n",
      "Epoch 0/ 370 train_loss  0.6859410580952425\n",
      "Epoch 0/ 380 train_loss  0.6850817321358542\n",
      "Epoch 0/ 390 train_loss  0.6894567954425629\n",
      "Epoch 0/ 400 train_loss  0.688379474747926\n",
      "Epoch 0/ 410 train_loss  0.6870482183265977\n",
      "Epoch 0/ 420 train_loss  0.6901874079058568\n",
      "Epoch 0/ 430 train_loss  0.6892447367137254\n",
      "Epoch 0/ 440 train_loss  0.6903584242713723\n",
      "Epoch 0/ 450 train_loss  0.6911576821572251\n",
      "Epoch 0/ 460 train_loss  0.69176053205586\n",
      "Epoch 0/ 470 train_loss  0.690485958422118\n",
      "Epoch 0/ 480 train_loss  0.6893247766575465\n",
      "Epoch 0/ 490 train_loss  0.6858664487849693\n",
      "Epoch 0/ 500 train_loss  0.6876796074658632\n",
      "Epoch 0/ 510 train_loss  0.689231989620363\n",
      "Epoch 0/ 520 train_loss  0.6905401486187027\n",
      "Epoch 0/ 530 train_loss  0.6891163158669786\n",
      "Epoch 0/ 540 train_loss  0.6896170644434514\n",
      "Epoch 0/ 550 train_loss  0.6889390727200292\n",
      "Epoch 0/ 560 train_loss  0.6881039733998477\n",
      "Epoch 0/ 570 train_loss  0.6892849045495192\n",
      "Epoch 0/ 580 train_loss  0.6888798118793759\n",
      "Epoch 0/ 590 train_loss  0.6856468900911888\n",
      "Epoch 0/ 600 train_loss  0.6866565621023377\n",
      "Epoch 0/ 610 train_loss  0.6852393224224692\n",
      "Epoch 0/ 620 train_loss  0.686058562573406\n",
      "Epoch 0/ 630 train_loss  0.6866267336857698\n",
      "Epoch 0/ 640 train_loss  0.6860177344758995\n",
      "Epoch 0/ 650 train_loss  0.6864285686039008\n",
      "Epoch 0/ 660 train_loss  0.6865265984088182\n",
      "Epoch 0/ 670 train_loss  0.685771180945101\n",
      "Epoch 0/ 680 train_loss  0.684603874089525\n",
      "Epoch 0/ 690 train_loss  0.6861450860573761\n",
      "Epoch 0/ 700 train_loss  0.6873576275046382\n",
      "Epoch 0/ 710 train_loss  0.6864883238685803\n",
      "Epoch 0/ 720 train_loss  0.6857520119287074\n",
      "Epoch 0/ 730 train_loss  0.6851093375213342\n",
      "Epoch 0/ 740 train_loss  0.6843348950248312\n",
      "Epoch 0/ 750 train_loss  0.6837629633843899\n",
      "Epoch 0/ 760 train_loss  0.6830676249473503\n",
      "Epoch 0/ 770 train_loss  0.6824795106498452\n",
      "Epoch 0/ 780 train_loss  0.6818794660174694\n",
      "Epoch 0/ 790 train_loss  0.68186411900045\n",
      "Epoch 0/ 800 train_loss  0.6813900356087834\n",
      "Epoch 0/ 810 train_loss  0.6794678044999823\n",
      "Epoch 0/ 820 train_loss  0.6806476334337054\n",
      "Epoch 0/ 830 train_loss  0.6800470861595079\n",
      "Epoch 0/ 840 train_loss  0.6802768270795544\n",
      "Epoch 0/ 850 train_loss  0.6805942432582378\n",
      "Epoch 0/ 860 train_loss  0.6798656112492778\n",
      "Epoch 0/ 870 train_loss  0.6795219316136563\n",
      "Epoch 0/ 880 train_loss  0.6782495558515869\n",
      "Epoch 0/ 890 train_loss  0.6770822545701868\n",
      "Epoch 0/ 900 train_loss  0.6762509097821182\n",
      "Epoch 0/ 910 train_loss  0.6768402881756589\n",
      "Epoch 0/ 920 train_loss  0.6745515253475827\n",
      "Epoch 0/ 930 train_loss  0.675412585746537\n",
      "Epoch 0/ 940 train_loss  0.6764227357554309\n",
      "Epoch 0/ 950 train_loss  0.6774757273808906\n",
      "Epoch 0/ 960 train_loss  0.6770480065528924\n",
      "Epoch 0/ 970 train_loss  0.6769679493126796\n",
      "Epoch 0/ 980 train_loss  0.6765507337314134\n",
      "Epoch 0/ 990 train_loss  0.6750064815761465\n",
      "Epoch 0/ 1000 train_loss  0.6739118053838611\n",
      "Epoch 0/ 1010 train_loss  0.6733750288660574\n",
      "Epoch 0/ 1020 train_loss  0.6719557127458792\n",
      "Epoch 0/ 1030 train_loss  0.6718754053477524\n",
      "Epoch 0/ 1040 train_loss  0.6712801329003504\n",
      "Epoch 0/ 1050 train_loss  0.6704356720121134\n",
      "Epoch 0/ 1060 train_loss  0.6699239209645762\n",
      "Epoch 0/ 1070 train_loss  0.6676612875380805\n",
      "Epoch 0/ 1080 train_loss  0.6692925823990393\n",
      "Epoch 0/ 1090 train_loss  0.6675412611198535\n",
      "Epoch 0/ 1100 train_loss  0.6648326908797025\n",
      "Epoch 0/ 1110 train_loss  0.6621707357197731\n",
      "Epoch 0/ 1120 train_loss  0.6598736453834655\n",
      "Epoch 0/ 1130 train_loss  0.6598919263620556\n",
      "Epoch 0/ 1140 train_loss  0.6612878038466238\n",
      "Epoch 0/ 1150 train_loss  0.6652070168859285\n",
      "Epoch 0/ 1160 train_loss  0.6669697137388947\n",
      "Epoch 0/ 1170 train_loss  0.6679021704941988\n",
      "Epoch 0/ 1180 train_loss  0.6679505661035241\n",
      "Epoch 0/ 1190 train_loss  0.6714771299395992\n",
      "Epoch 0/ 1200 train_loss  0.6724348525547733\n",
      "Epoch 0/ 1210 train_loss  0.6727284179831093\n",
      "Epoch 0/ 1220 train_loss  0.6722194201603043\n",
      "Epoch 0/ 1230 train_loss  0.6735853227385418\n",
      "Epoch 0/ 1240 train_loss  0.6736394266177329\n",
      "Epoch 0/ 1250 train_loss  0.6730727303713561\n",
      "Epoch 0/ 1260 train_loss  0.6734968487675937\n",
      "Epoch 0/ 1270 train_loss  0.6750787665495488\n",
      "Epoch 0/ 1280 train_loss  0.6740188680676511\n",
      "Epoch 0/ 1290 train_loss  0.6730808177053236\n",
      "Epoch 0/ 1300 train_loss  0.6736622153881651\n",
      "Epoch 0/ 1310 train_loss  0.674390769172602\n",
      "Epoch 0/ 1320 train_loss  0.6747053810225969\n",
      "Epoch 0/ 1330 train_loss  0.6748784631798814\n",
      "Epoch 0/ 1340 train_loss  0.6739418339456863\n",
      "Epoch 0/ 1350 train_loss  0.6739575321668828\n",
      "Epoch 0/ 1360 train_loss  0.6737158722381162\n",
      "Epoch 0/ 1370 train_loss  0.6737669818891878\n",
      "Epoch 0/ 1380 train_loss  0.6738014509577466\n",
      "Epoch 0/ 1390 train_loss  0.6742322184279453\n",
      "Epoch 0/ 1400 train_loss  0.6752528386962201\n",
      "Epoch 0/ 1410 train_loss  0.6764460660966364\n",
      "Epoch 0/ 1420 train_loss  0.6764279567938245\n",
      "Epoch 0/ 1430 train_loss  0.6765916197528163\n",
      "Epoch 0/ 1440 train_loss  0.6764360714988369\n",
      "Epoch 0/ 1450 train_loss  0.677166163189144\n",
      "Epoch 0/ 1460 train_loss  0.6766963135374532\n",
      "Epoch 0/ 1470 train_loss  0.6762034804718632\n",
      "Epoch 0/ 1480 train_loss  0.6758125003200729\n",
      "Epoch 0/ 1490 train_loss  0.6762995936201522\n",
      "Epoch 0/ 1500 train_loss  0.6761651639615496\n",
      "Epoch 0/ 1510 train_loss  0.6762418618339379\n",
      "Epoch 0/ 1520 train_loss  0.6765639115590603\n",
      "Epoch 0/ 1530 train_loss  0.676535690032871\n",
      "Epoch 0/ 1540 train_loss  0.6758368736036219\n",
      "Epoch 0/ 1550 train_loss  0.6752162799215125\n",
      "Epoch 0/ 1560 train_loss  0.6749010346901532\n",
      "Epoch 0/ 1570 train_loss  0.6741631906384685\n",
      "Epoch 0/ 1580 train_loss  0.6742700047767426\n",
      "Epoch 0/ 1590 train_loss  0.67407161682235\n",
      "Epoch 0/ 1600 train_loss  0.6737675914238207\n",
      "Epoch 0/ 1610 train_loss  0.6736705193307644\n",
      "Epoch 0/ 1620 train_loss  0.6736727538369136\n",
      "Epoch 0/ 1630 train_loss  0.6729847184788047\n",
      "Epoch 0/ 1640 train_loss  0.6720654114603815\n",
      "Epoch 0/ 1650 train_loss  0.6710904303441445\n",
      "Epoch 0/ 1660 train_loss  0.6706455411748534\n",
      "Epoch 0/ 1670 train_loss  0.6698902477114322\n",
      "Epoch 0/ 1680 train_loss  0.669604816471803\n",
      "Epoch 0/ 1690 train_loss  0.669253291557791\n",
      "Epoch 0/ 1700 train_loss  0.6682178359785501\n",
      "Epoch 0/ 1710 train_loss  0.6697292599848836\n",
      "Epoch 0/ 1720 train_loss  0.6697157062936661\n",
      "Epoch 0/ 1730 train_loss  0.6690245357724284\n",
      "Epoch 0/ 1740 train_loss  0.6688845914756429\n",
      "Epoch 0/ 1750 train_loss  0.6681681700008256\n",
      "Epoch 0/ 1760 train_loss  0.6674339420331473\n",
      "Epoch 0/ 1770 train_loss  0.6670610239024216\n",
      "Epoch 0/ 1780 train_loss  0.6655145266836279\n",
      "Epoch 0/ 1790 train_loss  0.6649453700088256\n",
      "Epoch 0/ 1800 train_loss  0.6647535506677297\n",
      "Epoch 0/ 1810 train_loss  0.664183597962336\n",
      "Epoch 0/ 1820 train_loss  0.6646813135123351\n",
      "Epoch 0/ 1830 train_loss  0.6650094455606788\n",
      "Epoch 0/ 1840 train_loss  0.6642991238212942\n",
      "Epoch 0/ 1850 train_loss  0.6637444852232127\n",
      "Epoch 0/ 1860 train_loss  0.664521909799547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/ 1870 train_loss  0.664039981203841\n",
      "Epoch 0/ 1880 train_loss  0.6636209924586435\n",
      "Epoch 0/ 1890 train_loss  0.6629181957059594\n",
      "Epoch 0/ 1900 train_loss  0.6627698415968764\n",
      "Epoch 0/ 1910 train_loss  0.6630606487334867\n",
      "Epoch 0/ 1920 train_loss  0.6616936132331224\n",
      "Epoch 0/ 1930 train_loss  0.6617556044802919\n",
      "Epoch 0/ 1940 train_loss  0.660892544615745\n",
      "Epoch 0/ 1950 train_loss  0.6603791097169504\n",
      "Epoch 0/ 1960 train_loss  0.6589679203300299\n",
      "Epoch 0/ 1970 train_loss  0.6584047248426277\n",
      "Epoch 0/ 1980 train_loss  0.658219849805564\n",
      "Epoch 0/ 1990 train_loss  0.6583045758325701\n",
      "Epoch 0/ 2000 train_loss  0.6584837984871119\n",
      "Epoch 0/ 2010 train_loss  0.6575571612728324\n",
      "Epoch 0/ 2020 train_loss  0.6569949178430852\n",
      "Epoch 0/ 2030 train_loss  0.6558462835462957\n",
      "Epoch 0/ 2040 train_loss  0.6552847784761267\n",
      "Epoch 0/ 2050 train_loss  0.654744112849599\n",
      "Epoch 0/ 2060 train_loss  0.6538363493599214\n",
      "Epoch 0/ 2070 train_loss  0.6529251705891125\n",
      "Epoch 0/ 2080 train_loss  0.6512663467017074\n",
      "Epoch 0/ 2090 train_loss  0.6497879252176393\n",
      "Epoch 0/ 2100 train_loss  0.6495045430958271\n",
      "Epoch 0/ 2110 train_loss  0.6513556908798444\n",
      "Epoch 0/ 2120 train_loss  0.651114198431935\n",
      "Epoch 0/ 2130 train_loss  0.6507500871980973\n",
      "Epoch 0/ 2140 train_loss  0.6508066523472004\n",
      "Epoch 0/ 2150 train_loss  0.6497855289522991\n",
      "Epoch 0/ 2160 train_loss  0.6498837124349343\n",
      "Epoch 0/ 2170 train_loss  0.649734242689637\n",
      "Epoch 0/ 2180 train_loss  0.6490311429827311\n",
      "Epoch 0/ 2190 train_loss  0.6474214540020516\n",
      "Epoch 0/ 2200 train_loss  0.6466177224943583\n",
      "Epoch 0/ 2210 train_loss  0.6457069872663571\n",
      "Epoch 0/ 2220 train_loss  0.6456809318112628\n",
      "Epoch 0/ 2230 train_loss  0.6443672916137785\n",
      "Epoch 0/ 2240 train_loss  0.6436621439036182\n",
      "Epoch 0/ 2250 train_loss  0.6443619185893072\n",
      "Epoch 0/ 2260 train_loss  0.6449332531830049\n",
      "Epoch 0/ 2270 train_loss  0.6448317728173563\n",
      "Epoch 0/ 2280 train_loss  0.6446151856847696\n",
      "Epoch 0/ 2290 train_loss  0.6447679181603561\n",
      "Epoch 0/ 2300 train_loss  0.6450972701989762\n",
      "Epoch 0/ 2310 train_loss  0.644865231892473\n",
      "Epoch 0/ 2320 train_loss  0.6462533352824313\n",
      "Epoch 0/ 2330 train_loss  0.6465140251250929\n",
      "Epoch 0/ 2340 train_loss  0.6466718164766128\n",
      "Epoch 0/ 2350 train_loss  0.6467878802271282\n",
      "Epoch 0/ 2360 train_loss  0.6468544642733\n",
      "Epoch 0/ 2370 train_loss  0.6465706727982259\n",
      "Epoch 0/ 2380 train_loss  0.6463525433429838\n",
      "Epoch 0/ 2390 train_loss  0.6458447799951649\n",
      "Epoch 0/ 2400 train_loss  0.6454989878887621\n",
      "Epoch 0/ 2410 train_loss  0.6457022195465523\n",
      "Epoch 0/ 2420 train_loss  0.6453165833915253\n",
      "Epoch 0/ 2430 train_loss  0.6455434711435388\n",
      "Epoch 0/ 2440 train_loss  0.6454125548048769\n",
      "Epoch 0/ 2450 train_loss  0.6451727750403237\n",
      "Epoch 0/ 2460 train_loss  0.645244343668919\n",
      "Epoch 0/ 2470 train_loss  0.6454591239926967\n",
      "Epoch 0/ 2480 train_loss  0.6451421723903847\n",
      "Epoch 0/ 2490 train_loss  0.6445994589156115\n",
      "Epoch 0/ 2500 train_loss  0.6441077055387199\n",
      "Epoch 0/ 2510 train_loss  0.6439866633759255\n",
      "Epoch 0/ 2520 train_loss  0.6440573973899028\n",
      "Epoch 0/ 2530 train_loss  0.6433979211934647\n",
      "Epoch 0/ 2540 train_loss  0.6432906603711973\n",
      "Epoch 0/ 2550 train_loss  0.6421057557058977\n",
      "Epoch 0/ 2560 train_loss  0.6414932167208462\n",
      "Epoch 0/ 2570 train_loss  0.6402815999245771\n",
      "Epoch 0/ 2580 train_loss  0.6398381233987824\n",
      "Epoch 0/ 2590 train_loss  0.6402925141896463\n",
      "Epoch 0/ 2600 train_loss  0.6398117941639457\n",
      "Epoch 0/ 2610 train_loss  0.6396027976318737\n",
      "Epoch 0/ 2620 train_loss  0.6402676753322984\n",
      "Epoch 0/ 2630 train_loss  0.64021249629288\n",
      "Epoch 0/ 2640 train_loss  0.6391426933132056\n",
      "Epoch 0/ 2650 train_loss  0.6382258084325014\n",
      "Epoch 0/ 2660 train_loss  0.6377651091517978\n",
      "Epoch 0/ 2670 train_loss  0.638351890700782\n",
      "Epoch 0/ 2680 train_loss  0.6386161191090107\n",
      "Epoch 0/ 2690 train_loss  0.6382737244877508\n",
      "Epoch 0/ 2700 train_loss  0.6375800987919448\n",
      "Epoch 0/ 2710 train_loss  0.6381687686733929\n",
      "Epoch 0/ 2720 train_loss  0.637202111987041\n",
      "Epoch 0/ 2730 train_loss  0.6364509424604066\n",
      "Epoch 0/ 2740 train_loss  0.6361928198615728\n",
      "Epoch 0/ 2750 train_loss  0.6357146670919928\n",
      "Epoch 0/ 2760 train_loss  0.6346077485597166\n",
      "Epoch 0/ 2770 train_loss  0.6339603111033567\n",
      "Epoch 0/ 2780 train_loss  0.6333662012598659\n",
      "Epoch 0/ 2790 train_loss  0.6343224115849984\n",
      "Epoch 0/ 2800 train_loss  0.6335711921651714\n",
      "Epoch 0/ 2810 train_loss  0.6325241252872762\n",
      "Epoch 0/ 2820 train_loss  0.6325513672942282\n",
      "Epoch 0/ 2830 train_loss  0.6320769797668335\n",
      "Epoch 0/ 2840 train_loss  0.6324034684167151\n",
      "Epoch 0/ 2850 train_loss  0.6316457994243032\n",
      "Epoch 0/ 2860 train_loss  0.631842224402809\n",
      "Epoch 0/ 2870 train_loss  0.6311233533513878\n",
      "Epoch 0/ 2880 train_loss  0.6300267455452639\n",
      "Epoch 0/ 2890 train_loss  0.6307998410583986\n",
      "Epoch 0/ 2900 train_loss  0.6302866053709696\n",
      "Epoch 0/ 2910 train_loss  0.629859212419831\n",
      "Epoch 0/ 2920 train_loss  0.6301026725733321\n",
      "Epoch 0/ 2930 train_loss  0.6311976252435214\n",
      "Epoch 0/ 2940 train_loss  0.630715482050971\n",
      "Epoch 0/ 2950 train_loss  0.630546060143891\n",
      "Epoch 0/ 2960 train_loss  0.6301647688073383\n",
      "Epoch 0/ 2970 train_loss  0.6300407103403951\n",
      "Epoch 0/ 2980 train_loss  0.6299619894437242\n",
      "Epoch 0/ 2990 train_loss  0.6296995466955031\n",
      "Epoch 0/ 3000 train_loss  0.6290975236520171\n",
      "Epoch 0/ 3010 train_loss  0.6283250073410348\n",
      "Epoch 0/ 3020 train_loss  0.627811778696957\n",
      "Epoch 0/ 3030 train_loss  0.6295994345602816\n",
      "Epoch 0/ 3040 train_loss  0.6294812052853798\n",
      "Epoch 0/ 3050 train_loss  0.6284732985716375\n",
      "Epoch 0/ 3060 train_loss  0.6284555312099994\n",
      "Epoch 0/ 3070 train_loss  0.629487631783035\n",
      "Epoch 0/ 3080 train_loss  0.6294136434188717\n",
      "Epoch 0/ 3090 train_loss  0.6288260648382713\n",
      "Epoch 0/ 3100 train_loss  0.6282690863864075\n",
      "Epoch 0/ 3110 train_loss  0.6282262040248254\n",
      "Epoch 0/ 3120 train_loss  0.6274121614173055\n",
      "Epoch 0/ 3130 train_loss  0.6271132596158467\n",
      "Epoch 0/ 3140 train_loss  0.6273405318961117\n",
      "Epoch 0/ 3150 train_loss  0.6265747664927963\n",
      "Epoch 0/ 3160 train_loss  0.6258624846799464\n",
      "Epoch 0/ 3170 train_loss  0.6252610353547315\n",
      "Epoch 0/ 3180 train_loss  0.6245085086509492\n",
      "Epoch 0/ 3190 train_loss  0.6246134363418463\n",
      "Epoch 0/ 3200 train_loss  0.6240227085584774\n",
      "Epoch 0/ 3210 train_loss  0.6226956615021388\n",
      "Epoch 0/ 3220 train_loss  0.6224722306654783\n",
      "Epoch 0/ 3230 train_loss  0.6226375586021657\n",
      "Epoch 0/ 3240 train_loss  0.6247046314415602\n",
      "Epoch 0/ 3250 train_loss  0.625035597666525\n",
      "Epoch 0/ 3260 train_loss  0.6247224768549432\n",
      "Epoch 0/ 3270 train_loss  0.6249383332378684\n",
      "Epoch 0/ 3280 train_loss  0.6254844757733957\n",
      "Epoch 0/ 3290 train_loss  0.6253157002432413\n",
      "Epoch 0/ 3300 train_loss  0.625655398132336\n",
      "Epoch 0/ 3310 train_loss  0.6261132803191276\n",
      "Epoch 0/ 3320 train_loss  0.626001776663407\n",
      "Epoch 0/ 3330 train_loss  0.6258437223290583\n",
      "Epoch 0/ 3340 train_loss  0.6261152132882731\n",
      "Epoch 0/ 3350 train_loss  0.6256904157223319\n",
      "Epoch 0/ 3360 train_loss  0.6265391533202048\n",
      "Epoch 0/ 3370 train_loss  0.6268736079981888\n",
      "Epoch 0/ 3380 train_loss  0.6272566620927282\n",
      "Epoch 0/ 3390 train_loss  0.6277483317775756\n",
      "Epoch 0/ 3400 train_loss  0.6274299705012099\n",
      "Epoch 0/ 3410 train_loss  0.6273316699022815\n",
      "Epoch 0/ 3420 train_loss  0.6271509819282818\n",
      "Epoch 0/ 3430 train_loss  0.6278153201217598\n",
      "Epoch 0/ 3440 train_loss  0.6281309638156144\n",
      "Epoch 0/ 3450 train_loss  0.6281216390652286\n",
      "Epoch 0/ 3460 train_loss  0.6277292542961679\n",
      "Epoch 0/ 3470 train_loss  0.62849941765809\n",
      "Epoch 0/ 3480 train_loss  0.6288382677709277\n",
      "Epoch 0/ 3490 train_loss  0.6288021136133967\n",
      "Epoch 0/ 3500 train_loss  0.6291311201692692\n",
      "Epoch 0/ 3510 train_loss  0.6290094188629435\n",
      "Epoch 0/ 3520 train_loss  0.6301722359743012\n",
      "Epoch 0/ 3530 train_loss  0.6298541908629935\n",
      "Epoch 0/ 3540 train_loss  0.6292575517264168\n",
      "Epoch 0/ 3550 train_loss  0.6291739898610493\n",
      "Epoch 0/ 3560 train_loss  0.6289234955182947\n",
      "Epoch 0/ 3570 train_loss  0.6290648314908814\n",
      "Epoch 0/ 3580 train_loss  0.6293647994401776\n",
      "Epoch 0/ 3590 train_loss  0.629251097298868\n",
      "Epoch 0/ 3600 train_loss  0.6291049536343457\n",
      "Epoch 0/ 3610 train_loss  0.6295218622528657\n",
      "Epoch 0/ 3620 train_loss  0.629880857577249\n",
      "Epoch 0/ 3630 train_loss  0.630214833485132\n",
      "Epoch 0/ 3640 train_loss  0.6301412445774486\n",
      "Epoch 0/ 3650 train_loss  0.6302844820146078\n",
      "Epoch 0/ 3660 train_loss  0.6303240264195928\n",
      "Epoch 0/ 3670 train_loss  0.6306281237561022\n",
      "Epoch 0/ 3680 train_loss  0.631046848661651\n",
      "Epoch 0/ 3690 train_loss  0.630960180449393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/ 3700 train_loss  0.6309560789273598\n",
      "Train Loss: 0.6311649910361984, Train Acc: 0.6458670988654781\n",
      "Test Loss: 0.5892059767449443, Test Acc: 0.7012987012987013\n",
      "保存模型成功! 路径为: ’model/word_sense_disambiguation.bin‘\n",
      "Epoch 1/ 3710 train_loss  0.0012216358796324751\n",
      "Epoch 1/ 3720 train_loss  0.003153974120435093\n",
      "Epoch 1/ 3730 train_loss  0.004975258854794637\n",
      "Epoch 1/ 3740 train_loss  0.006527682607126943\n",
      "Epoch 1/ 3750 train_loss  0.008329082848807214\n",
      "Epoch 1/ 3760 train_loss  0.00980991002863792\n",
      "Epoch 1/ 3770 train_loss  0.011340435377107255\n",
      "Epoch 1/ 3780 train_loss  0.013456869647872707\n",
      "Epoch 1/ 3790 train_loss  0.014707755159989705\n",
      "Epoch 1/ 3800 train_loss  0.016032492826428105\n",
      "Epoch 1/ 3810 train_loss  0.017250208863846782\n",
      "Epoch 1/ 3820 train_loss  0.01835474719296345\n",
      "Epoch 1/ 3830 train_loss  0.020077475253910876\n",
      "Epoch 1/ 3840 train_loss  0.021205495058461907\n",
      "Epoch 1/ 3850 train_loss  0.022185186024977828\n",
      "Epoch 1/ 3860 train_loss  0.02378269226785386\n",
      "Epoch 1/ 3870 train_loss  0.025135020912829487\n",
      "Epoch 1/ 3880 train_loss  0.02587134284416659\n",
      "Epoch 1/ 3890 train_loss  0.027792449463283377\n",
      "Epoch 1/ 3900 train_loss  0.029034274903608475\n",
      "Epoch 1/ 3910 train_loss  0.03085305268424174\n",
      "Epoch 1/ 3920 train_loss  0.03217093402403886\n",
      "Epoch 1/ 3930 train_loss  0.03343967264843279\n",
      "Epoch 1/ 3940 train_loss  0.03449617365893302\n",
      "Epoch 1/ 3950 train_loss  0.035456925893867355\n",
      "Epoch 1/ 3960 train_loss  0.03678426124605194\n",
      "Epoch 1/ 3970 train_loss  0.03810682218604413\n",
      "Epoch 1/ 3980 train_loss  0.039948630702836316\n",
      "Epoch 1/ 3990 train_loss  0.041513475969153724\n",
      "Epoch 1/ 4000 train_loss  0.043167501977812536\n",
      "Epoch 1/ 4010 train_loss  0.04409177658116063\n",
      "Epoch 1/ 4020 train_loss  0.0453273839240162\n",
      "Epoch 1/ 4030 train_loss  0.046234816589348846\n",
      "Epoch 1/ 4040 train_loss  0.04724902273771716\n",
      "Epoch 1/ 4050 train_loss  0.04820580462907203\n",
      "Epoch 1/ 4060 train_loss  0.04996867881544635\n",
      "Epoch 1/ 4070 train_loss  0.0512689938994642\n",
      "Epoch 1/ 4080 train_loss  0.05271938337924364\n",
      "Epoch 1/ 4090 train_loss  0.05406982275812656\n",
      "Epoch 1/ 4100 train_loss  0.055380325874522726\n",
      "Epoch 1/ 4110 train_loss  0.05647275736730395\n",
      "Epoch 1/ 4120 train_loss  0.057920032225314166\n",
      "Epoch 1/ 4130 train_loss  0.05940125967701111\n",
      "Epoch 1/ 4140 train_loss  0.06101173118834842\n",
      "Epoch 1/ 4150 train_loss  0.0622923990654838\n",
      "Epoch 1/ 4160 train_loss  0.06330119772973618\n",
      "Epoch 1/ 4170 train_loss  0.06440742487903482\n",
      "Epoch 1/ 4180 train_loss  0.06515156009549092\n",
      "Epoch 1/ 4190 train_loss  0.06628133389216835\n",
      "Epoch 1/ 4200 train_loss  0.06765314537426903\n",
      "Epoch 1/ 4210 train_loss  0.06938046204504027\n",
      "Epoch 1/ 4220 train_loss  0.0707019655370988\n",
      "Epoch 1/ 4230 train_loss  0.07150102205519913\n",
      "Epoch 1/ 4240 train_loss  0.07238551909559111\n",
      "Epoch 1/ 4250 train_loss  0.07315632173522434\n",
      "Epoch 1/ 4260 train_loss  0.0739382397708584\n",
      "Epoch 1/ 4270 train_loss  0.07517180575871849\n",
      "Epoch 1/ 4280 train_loss  0.0760524090824945\n",
      "Epoch 1/ 4290 train_loss  0.07734887778670432\n",
      "Epoch 1/ 4300 train_loss  0.07782752858533766\n",
      "Epoch 1/ 4310 train_loss  0.07872972942714145\n",
      "Epoch 1/ 4320 train_loss  0.0800613384729907\n",
      "Epoch 1/ 4330 train_loss  0.0812262845336875\n",
      "Epoch 1/ 4340 train_loss  0.08229224968390877\n",
      "Epoch 1/ 4350 train_loss  0.0829718623637297\n",
      "Epoch 1/ 4360 train_loss  0.08470268473813501\n",
      "Epoch 1/ 4370 train_loss  0.08534046475294324\n",
      "Epoch 1/ 4380 train_loss  0.08621871947713165\n",
      "Epoch 1/ 4390 train_loss  0.08778710611061045\n",
      "Epoch 1/ 4400 train_loss  0.08905412944302982\n",
      "Epoch 1/ 4410 train_loss  0.09011730027575547\n",
      "Epoch 1/ 4420 train_loss  0.09085434903047679\n",
      "Epoch 1/ 4430 train_loss  0.09155840792079747\n",
      "Epoch 1/ 4440 train_loss  0.09241841984508344\n",
      "Epoch 1/ 4450 train_loss  0.09333176652256696\n",
      "Epoch 1/ 4460 train_loss  0.0940045631734379\n",
      "Epoch 1/ 4470 train_loss  0.09470987595441094\n",
      "Epoch 1/ 4480 train_loss  0.09496615796872152\n",
      "Epoch 1/ 4490 train_loss  0.09590317247230522\n",
      "Epoch 1/ 4500 train_loss  0.09690014953222544\n",
      "Epoch 1/ 4510 train_loss  0.09768959435374538\n",
      "Epoch 1/ 4520 train_loss  0.0981362123913338\n",
      "Epoch 1/ 4530 train_loss  0.0992654386633806\n",
      "Epoch 1/ 4540 train_loss  0.10035683105012921\n",
      "Epoch 1/ 4550 train_loss  0.10115777344222256\n",
      "Epoch 1/ 4560 train_loss  0.10197640222536501\n",
      "Epoch 1/ 4570 train_loss  0.10276644282603815\n",
      "Epoch 1/ 4580 train_loss  0.10381422510271382\n",
      "Epoch 1/ 4590 train_loss  0.10459725591033896\n",
      "Epoch 1/ 4600 train_loss  0.10511401250932531\n",
      "Epoch 1/ 4610 train_loss  0.10660455067759614\n",
      "Epoch 1/ 4620 train_loss  0.10798384820153835\n",
      "Epoch 1/ 4630 train_loss  0.10903380580832199\n",
      "Epoch 1/ 4640 train_loss  0.1099733079949651\n",
      "Epoch 1/ 4650 train_loss  0.11213465702203658\n",
      "Epoch 1/ 4660 train_loss  0.1125033105365261\n",
      "Epoch 1/ 4670 train_loss  0.11417428703765674\n",
      "Epoch 1/ 4680 train_loss  0.11477193285438882\n",
      "Epoch 1/ 4690 train_loss  0.11541305667038035\n",
      "Epoch 1/ 4700 train_loss  0.11622290047613235\n",
      "Epoch 1/ 4710 train_loss  0.11709468258151713\n",
      "Epoch 1/ 4720 train_loss  0.1176143377866849\n",
      "Epoch 1/ 4730 train_loss  0.11834988978136547\n",
      "Epoch 1/ 4740 train_loss  0.11947585654651695\n",
      "Epoch 1/ 4750 train_loss  0.12012824197558984\n",
      "Epoch 1/ 4760 train_loss  0.12061476202562706\n",
      "Epoch 1/ 4770 train_loss  0.12109581750853136\n",
      "Epoch 1/ 4780 train_loss  0.12163052516243576\n",
      "Epoch 1/ 4790 train_loss  0.12214088689837617\n",
      "Epoch 1/ 4800 train_loss  0.1222413589783479\n",
      "Epoch 1/ 4810 train_loss  0.12294155972812662\n",
      "Epoch 1/ 4820 train_loss  0.12304941764041878\n",
      "Epoch 1/ 4830 train_loss  0.12386588430022882\n",
      "Epoch 1/ 4840 train_loss  0.12445791590768725\n",
      "Epoch 1/ 4850 train_loss  0.12547799604757087\n",
      "Epoch 1/ 4860 train_loss  0.126761657208028\n",
      "Epoch 1/ 4870 train_loss  0.12741863457385064\n",
      "Epoch 1/ 4880 train_loss  0.12863243152687454\n",
      "Epoch 1/ 4890 train_loss  0.12996800262818464\n",
      "Epoch 1/ 4900 train_loss  0.13069427617117008\n",
      "Epoch 1/ 4910 train_loss  0.13124468739835318\n",
      "Epoch 1/ 4920 train_loss  0.1320045953343388\n",
      "Epoch 1/ 4930 train_loss  0.13275195231266762\n",
      "Epoch 1/ 4940 train_loss  0.13342413263699338\n",
      "Epoch 1/ 4950 train_loss  0.1342608365897654\n",
      "Epoch 1/ 4960 train_loss  0.13531156757000026\n",
      "Epoch 1/ 4970 train_loss  0.13670657609604767\n",
      "Epoch 1/ 4980 train_loss  0.13747479401852744\n",
      "Epoch 1/ 4990 train_loss  0.1382180113718138\n",
      "Epoch 1/ 5000 train_loss  0.1387770221299902\n",
      "Epoch 1/ 5010 train_loss  0.1393395078998141\n",
      "Epoch 1/ 5020 train_loss  0.140296829269917\n",
      "Epoch 1/ 5030 train_loss  0.14102514972145974\n",
      "Epoch 1/ 5040 train_loss  0.1417633746439695\n",
      "Epoch 1/ 5050 train_loss  0.14249674271086216\n",
      "Epoch 1/ 5060 train_loss  0.1434049524822799\n",
      "Epoch 1/ 5070 train_loss  0.14415009793410674\n",
      "Epoch 1/ 5080 train_loss  0.14489021143306227\n",
      "Epoch 1/ 5090 train_loss  0.14529906309301618\n",
      "Epoch 1/ 5100 train_loss  0.14573907211344767\n",
      "Epoch 1/ 5110 train_loss  0.14622530018406288\n",
      "Epoch 1/ 5120 train_loss  0.14676280073539968\n",
      "Epoch 1/ 5130 train_loss  0.14706496828199744\n",
      "Epoch 1/ 5140 train_loss  0.14747184842370512\n",
      "Epoch 1/ 5150 train_loss  0.14866778066286332\n",
      "Epoch 1/ 5160 train_loss  0.1489726748739319\n",
      "Epoch 1/ 5170 train_loss  0.14968497347067217\n",
      "Epoch 1/ 5180 train_loss  0.15070162285797475\n",
      "Epoch 1/ 5190 train_loss  0.1514696421395478\n",
      "Epoch 1/ 5200 train_loss  0.15204475919675228\n",
      "Epoch 1/ 5210 train_loss  0.15233973827737168\n",
      "Epoch 1/ 5220 train_loss  0.15269542523258695\n",
      "Epoch 1/ 5230 train_loss  0.15389795184636182\n",
      "Epoch 1/ 5240 train_loss  0.15487887930894942\n",
      "Epoch 1/ 5250 train_loss  0.15533957918373606\n",
      "Epoch 1/ 5260 train_loss  0.15623044266976674\n",
      "Epoch 1/ 5270 train_loss  0.15670213518496712\n",
      "Epoch 1/ 5280 train_loss  0.15709398624704535\n",
      "Epoch 1/ 5290 train_loss  0.1574914479094271\n",
      "Epoch 1/ 5300 train_loss  0.15817740633182653\n",
      "Epoch 1/ 5310 train_loss  0.1586105035847218\n",
      "Epoch 1/ 5320 train_loss  0.1589741569589614\n",
      "Epoch 1/ 5330 train_loss  0.1592223565354035\n",
      "Epoch 1/ 5340 train_loss  0.15947553518603555\n",
      "Epoch 1/ 5350 train_loss  0.16017003946593297\n",
      "Epoch 1/ 5360 train_loss  0.1604485111321017\n",
      "Epoch 1/ 5370 train_loss  0.16193448907657906\n",
      "Epoch 1/ 5380 train_loss  0.1625193641866769\n",
      "Epoch 1/ 5390 train_loss  0.16287086345976245\n",
      "Epoch 1/ 5400 train_loss  0.16359484098812363\n",
      "Epoch 1/ 5410 train_loss  0.16439539111849757\n",
      "Epoch 1/ 5420 train_loss  0.1646897857843304\n",
      "Epoch 1/ 5430 train_loss  0.1651127920137379\n",
      "Epoch 1/ 5440 train_loss  0.16541831185967967\n",
      "Epoch 1/ 5450 train_loss  0.1654773950350736\n",
      "Epoch 1/ 5460 train_loss  0.16584843731866028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 5470 train_loss  0.16592355909628262\n",
      "Epoch 1/ 5480 train_loss  0.16601314850038015\n",
      "Epoch 1/ 5490 train_loss  0.1661102887154345\n",
      "Epoch 1/ 5500 train_loss  0.16604951882560826\n",
      "Epoch 1/ 5510 train_loss  0.1676156196357402\n",
      "Epoch 1/ 5520 train_loss  0.1682502151456311\n",
      "Epoch 1/ 5530 train_loss  0.1689096730953086\n",
      "Epoch 1/ 5540 train_loss  0.16944393456004403\n",
      "Epoch 1/ 5550 train_loss  0.1704832966108379\n",
      "Epoch 1/ 5560 train_loss  0.17135404372288307\n",
      "Epoch 1/ 5570 train_loss  0.17147781304923462\n",
      "Epoch 1/ 5580 train_loss  0.1717384247239012\n",
      "Epoch 1/ 5590 train_loss  0.17248142716671763\n",
      "Epoch 1/ 5600 train_loss  0.17295512744087863\n",
      "Epoch 1/ 5610 train_loss  0.1736820905966927\n",
      "Epoch 1/ 5620 train_loss  0.17428361927055064\n",
      "Epoch 1/ 5630 train_loss  0.17490612090929525\n",
      "Epoch 1/ 5640 train_loss  0.17532174992089739\n",
      "Epoch 1/ 5650 train_loss  0.1760890125293252\n",
      "Epoch 1/ 5660 train_loss  0.17608345071945092\n",
      "Epoch 1/ 5670 train_loss  0.17662645262797783\n",
      "Epoch 1/ 5680 train_loss  0.17698645109853803\n",
      "Epoch 1/ 5690 train_loss  0.17752602749822013\n",
      "Epoch 1/ 5700 train_loss  0.17824547408421837\n",
      "Epoch 1/ 5710 train_loss  0.1784992183986334\n",
      "Epoch 1/ 5720 train_loss  0.17883695849054984\n",
      "Epoch 1/ 5730 train_loss  0.17881461955873823\n",
      "Epoch 1/ 5740 train_loss  0.1791047486396643\n",
      "Epoch 1/ 5750 train_loss  0.1798689675060174\n",
      "Epoch 1/ 5760 train_loss  0.18013097065506092\n",
      "Epoch 1/ 5770 train_loss  0.17999922799303536\n",
      "Epoch 1/ 5780 train_loss  0.1806878908019655\n",
      "Epoch 1/ 5790 train_loss  0.18052994966171107\n",
      "Epoch 1/ 5800 train_loss  0.18165920345702308\n",
      "Epoch 1/ 5810 train_loss  0.18223405633030684\n",
      "Epoch 1/ 5820 train_loss  0.18280778226335745\n",
      "Epoch 1/ 5830 train_loss  0.1835718730893849\n",
      "Epoch 1/ 5840 train_loss  0.18419322431583776\n",
      "Epoch 1/ 5850 train_loss  0.18447717450697965\n",
      "Epoch 1/ 5860 train_loss  0.1853359320930018\n",
      "Epoch 1/ 5870 train_loss  0.18603925019342715\n",
      "Epoch 1/ 5880 train_loss  0.1865470071023419\n",
      "Epoch 1/ 5890 train_loss  0.18666468879001152\n",
      "Epoch 1/ 5900 train_loss  0.18741627759516746\n",
      "Epoch 1/ 5910 train_loss  0.18837025908767777\n",
      "Epoch 1/ 5920 train_loss  0.1889296313541237\n",
      "Epoch 1/ 5930 train_loss  0.18954540799127215\n",
      "Epoch 1/ 5940 train_loss  0.18967743723630057\n",
      "Epoch 1/ 5950 train_loss  0.19023662711591033\n",
      "Epoch 1/ 5960 train_loss  0.19100411215233276\n",
      "Epoch 1/ 5970 train_loss  0.19206446608605252\n",
      "Epoch 1/ 5980 train_loss  0.19260787505817206\n",
      "Epoch 1/ 5990 train_loss  0.19298941594356042\n",
      "Epoch 1/ 6000 train_loss  0.19321500989060852\n",
      "Epoch 1/ 6010 train_loss  0.19335878584266158\n",
      "Epoch 1/ 6020 train_loss  0.19418835165330592\n",
      "Epoch 1/ 6030 train_loss  0.19467727851572375\n",
      "Epoch 1/ 6040 train_loss  0.19574006683148742\n",
      "Epoch 1/ 6050 train_loss  0.19613905393275596\n",
      "Epoch 1/ 6060 train_loss  0.19673808708724264\n",
      "Epoch 1/ 6070 train_loss  0.1974869234350432\n",
      "Epoch 1/ 6080 train_loss  0.1975438647794317\n",
      "Epoch 1/ 6090 train_loss  0.19793469783890083\n",
      "Epoch 1/ 6100 train_loss  0.19804854442239056\n",
      "Epoch 1/ 6110 train_loss  0.19849920322650927\n",
      "Epoch 1/ 6120 train_loss  0.19891926628347562\n",
      "Epoch 1/ 6130 train_loss  0.1993842629671316\n",
      "Epoch 1/ 6140 train_loss  0.19973561705407425\n",
      "Epoch 1/ 6150 train_loss  0.2000921705814518\n",
      "Epoch 1/ 6160 train_loss  0.20063218700940205\n",
      "Epoch 1/ 6170 train_loss  0.20079478636074882\n",
      "Epoch 1/ 6180 train_loss  0.20137858685767915\n",
      "Epoch 1/ 6190 train_loss  0.20165953092733235\n",
      "Epoch 1/ 6200 train_loss  0.20186410287466022\n",
      "Epoch 1/ 6210 train_loss  0.20217504087132487\n",
      "Epoch 1/ 6220 train_loss  0.20214536832865942\n",
      "Epoch 1/ 6230 train_loss  0.20235133220065227\n",
      "Epoch 1/ 6240 train_loss  0.20284236778981557\n",
      "Epoch 1/ 6250 train_loss  0.20296495715092425\n",
      "Epoch 1/ 6260 train_loss  0.20284072961490426\n",
      "Epoch 1/ 6270 train_loss  0.20323503818611308\n",
      "Epoch 1/ 6280 train_loss  0.20334082563436195\n",
      "Epoch 1/ 6290 train_loss  0.2038638821613097\n",
      "Epoch 1/ 6300 train_loss  0.2039162809411685\n",
      "Epoch 1/ 6310 train_loss  0.20427367387057865\n",
      "Epoch 1/ 6320 train_loss  0.20496725117580236\n",
      "Epoch 1/ 6330 train_loss  0.20559885643556788\n",
      "Epoch 1/ 6340 train_loss  0.20547791037172647\n",
      "Epoch 1/ 6350 train_loss  0.20534714154616354\n",
      "Epoch 1/ 6360 train_loss  0.2053460942677533\n",
      "Epoch 1/ 6370 train_loss  0.20572875506464203\n",
      "Epoch 1/ 6380 train_loss  0.2061666418605172\n",
      "Epoch 1/ 6390 train_loss  0.20650663995478707\n",
      "Epoch 1/ 6400 train_loss  0.20642962892397484\n",
      "Epoch 1/ 6410 train_loss  0.20709907426102475\n",
      "Epoch 1/ 6420 train_loss  0.207449576093824\n",
      "Epoch 1/ 6430 train_loss  0.20744071935602174\n",
      "Epoch 1/ 6440 train_loss  0.2075582573334542\n",
      "Epoch 1/ 6450 train_loss  0.20811402144931665\n",
      "Epoch 1/ 6460 train_loss  0.20835452556127437\n",
      "Epoch 1/ 6470 train_loss  0.2082871806018596\n",
      "Epoch 1/ 6480 train_loss  0.20827481666170525\n",
      "Epoch 1/ 6490 train_loss  0.20871393006730066\n",
      "Epoch 1/ 6500 train_loss  0.208725654200539\n",
      "Epoch 1/ 6510 train_loss  0.2088086502246313\n",
      "Epoch 1/ 6520 train_loss  0.20983525332612235\n",
      "Epoch 1/ 6530 train_loss  0.2103227762452956\n",
      "Epoch 1/ 6540 train_loss  0.2120872804891085\n",
      "Epoch 1/ 6550 train_loss  0.2121824080151826\n",
      "Epoch 1/ 6560 train_loss  0.21268083572280086\n",
      "Epoch 1/ 6570 train_loss  0.21289013533089227\n",
      "Epoch 1/ 6580 train_loss  0.21313252095999566\n",
      "Epoch 1/ 6590 train_loss  0.21327215533562527\n",
      "Epoch 1/ 6600 train_loss  0.21380811670332528\n",
      "Epoch 1/ 6610 train_loss  0.21397348836154798\n",
      "Epoch 1/ 6620 train_loss  0.21436689027976472\n",
      "Epoch 1/ 6630 train_loss  0.21492672179917194\n",
      "Epoch 1/ 6640 train_loss  0.21534186526420754\n",
      "Epoch 1/ 6650 train_loss  0.21580018142969595\n",
      "Epoch 1/ 6660 train_loss  0.21599357087131837\n",
      "Epoch 1/ 6670 train_loss  0.21614455285134715\n",
      "Epoch 1/ 6680 train_loss  0.21655092437902446\n",
      "Epoch 1/ 6690 train_loss  0.21683919421248474\n",
      "Epoch 1/ 6700 train_loss  0.2169197291874202\n",
      "Epoch 1/ 6710 train_loss  0.2172218187448726\n",
      "Epoch 1/ 6720 train_loss  0.21752072865509575\n",
      "Epoch 1/ 6730 train_loss  0.217669613311654\n",
      "Epoch 1/ 6740 train_loss  0.21853985202982637\n",
      "Epoch 1/ 6750 train_loss  0.21857172952995535\n",
      "Epoch 1/ 6760 train_loss  0.21878650635621685\n",
      "Epoch 1/ 6770 train_loss  0.21926296766839395\n",
      "Epoch 1/ 6780 train_loss  0.21965449061769732\n",
      "Epoch 1/ 6790 train_loss  0.22026620375596276\n",
      "Epoch 1/ 6800 train_loss  0.22058195105565112\n",
      "Epoch 1/ 6810 train_loss  0.22063663547831522\n",
      "Epoch 1/ 6820 train_loss  0.22096556553297853\n",
      "Epoch 1/ 6830 train_loss  0.22137277288628557\n",
      "Epoch 1/ 6840 train_loss  0.22183681045197584\n",
      "Epoch 1/ 6850 train_loss  0.22207136289613855\n",
      "Epoch 1/ 6860 train_loss  0.22214372986733763\n",
      "Epoch 1/ 6870 train_loss  0.22243242512332628\n",
      "Epoch 1/ 6880 train_loss  0.2227380059753859\n",
      "Epoch 1/ 6890 train_loss  0.22308259405629882\n",
      "Epoch 1/ 6900 train_loss  0.22330368937974718\n",
      "Epoch 1/ 6910 train_loss  0.22321998646772873\n",
      "Epoch 1/ 6920 train_loss  0.22360613344131158\n",
      "Epoch 1/ 6930 train_loss  0.22349588695794217\n",
      "Epoch 1/ 6940 train_loss  0.2235081268589493\n",
      "Epoch 1/ 6950 train_loss  0.2236498661963363\n",
      "Epoch 1/ 6960 train_loss  0.22344413133516333\n",
      "Epoch 1/ 6970 train_loss  0.2238034170858156\n",
      "Epoch 1/ 6980 train_loss  0.22407026531646046\n",
      "Epoch 1/ 6990 train_loss  0.2245700944121421\n",
      "Epoch 1/ 7000 train_loss  0.22492770747304497\n",
      "Epoch 1/ 7010 train_loss  0.2251345473326658\n",
      "Epoch 1/ 7020 train_loss  0.22524672466833306\n",
      "Epoch 1/ 7030 train_loss  0.2256864976625026\n",
      "Epoch 1/ 7040 train_loss  0.22576873445567489\n",
      "Epoch 1/ 7050 train_loss  0.22636024055008033\n",
      "Epoch 1/ 7060 train_loss  0.22717982794843067\n",
      "Epoch 1/ 7070 train_loss  0.2284068659711603\n",
      "Epoch 1/ 7080 train_loss  0.22865467128198133\n",
      "Epoch 1/ 7090 train_loss  0.229372841623353\n",
      "Epoch 1/ 7100 train_loss  0.22968971034455785\n",
      "Epoch 1/ 7110 train_loss  0.23002313671999322\n",
      "Epoch 1/ 7120 train_loss  0.23029296615241057\n",
      "Epoch 1/ 7130 train_loss  0.23052928018201077\n",
      "Epoch 1/ 7140 train_loss  0.23084574045012354\n",
      "Epoch 1/ 7150 train_loss  0.23099055010079297\n",
      "Epoch 1/ 7160 train_loss  0.23184748438857083\n",
      "Epoch 1/ 7170 train_loss  0.23229398967024342\n",
      "Epoch 1/ 7180 train_loss  0.2328642689966309\n",
      "Epoch 1/ 7190 train_loss  0.2331613441064892\n",
      "Epoch 1/ 7200 train_loss  0.23345931027947617\n",
      "Epoch 1/ 7210 train_loss  0.23380203491806903\n",
      "Epoch 1/ 7220 train_loss  0.23473526177694637\n",
      "Epoch 1/ 7230 train_loss  0.23504293156516218\n",
      "Epoch 1/ 7240 train_loss  0.23511522602793142\n",
      "Epoch 1/ 7250 train_loss  0.23524504059382556\n",
      "Epoch 1/ 7260 train_loss  0.23540852292842612\n",
      "Epoch 1/ 7270 train_loss  0.2356505607551648\n",
      "Epoch 1/ 7280 train_loss  0.23573338295326948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 7290 train_loss  0.2364444716932822\n",
      "Epoch 1/ 7300 train_loss  0.23649718051198618\n",
      "Epoch 1/ 7310 train_loss  0.23680223693670108\n",
      "Epoch 1/ 7320 train_loss  0.23693162676546461\n",
      "Epoch 1/ 7330 train_loss  0.23713751008329054\n",
      "Epoch 1/ 7340 train_loss  0.23740956616173287\n",
      "Epoch 1/ 7350 train_loss  0.23790080715360085\n",
      "Epoch 1/ 7360 train_loss  0.2381592187985839\n",
      "Epoch 1/ 7370 train_loss  0.2383608029678545\n",
      "Epoch 1/ 7380 train_loss  0.2387759972567794\n",
      "Epoch 1/ 7390 train_loss  0.238966502638874\n",
      "Epoch 1/ 7400 train_loss  0.239200404198503\n",
      "Train Loss: 0.4784020120376114, Train Acc: 0.7809292274446246\n",
      "Test Loss: 0.4600255056446896, Test Acc: 0.7792207792207793\n",
      "保存模型成功! 路径为: ’model/word_sense_disambiguation.bin‘\n",
      "Epoch 2/ 7410 train_loss  0.0002683803073411497\n",
      "Epoch 2/ 7420 train_loss  0.0010571555494321453\n",
      "Epoch 2/ 7430 train_loss  0.002133090404010273\n",
      "Epoch 2/ 7440 train_loss  0.00263792844639562\n",
      "Epoch 2/ 7450 train_loss  0.003197120435837248\n",
      "Epoch 2/ 7460 train_loss  0.003843172429125803\n",
      "Epoch 2/ 7470 train_loss  0.004484712621422833\n",
      "Epoch 2/ 7480 train_loss  0.005554171936914023\n",
      "Epoch 2/ 7490 train_loss  0.00602528999674107\n",
      "Epoch 2/ 7500 train_loss  0.006718149252423462\n",
      "Epoch 2/ 7510 train_loss  0.007031990780363722\n",
      "Epoch 2/ 7520 train_loss  0.0074516905319984145\n",
      "Epoch 2/ 7530 train_loss  0.007823332188439537\n",
      "Epoch 2/ 7540 train_loss  0.008397967688955522\n",
      "Epoch 2/ 7550 train_loss  0.008890743011684032\n",
      "Epoch 2/ 7560 train_loss  0.009410471409787218\n",
      "Epoch 2/ 7570 train_loss  0.009800345247152573\n",
      "Epoch 2/ 7580 train_loss  0.010210167143266655\n",
      "Epoch 2/ 7590 train_loss  0.010743514883176186\n",
      "Epoch 2/ 7600 train_loss  0.011024434919942138\n",
      "Epoch 2/ 7610 train_loss  0.011610857722914932\n",
      "Epoch 2/ 7620 train_loss  0.012133360869775584\n",
      "Epoch 2/ 7630 train_loss  0.012594213404634185\n",
      "Epoch 2/ 7640 train_loss  0.013239654591271757\n",
      "Epoch 2/ 7650 train_loss  0.013504017441796532\n",
      "Epoch 2/ 7660 train_loss  0.014538799002147818\n",
      "Epoch 2/ 7670 train_loss  0.01501741836429246\n",
      "Epoch 2/ 7680 train_loss  0.01580175337737098\n",
      "Epoch 2/ 7690 train_loss  0.01631277179635065\n",
      "Epoch 2/ 7700 train_loss  0.017165386052259198\n",
      "Epoch 2/ 7710 train_loss  0.01749049097646382\n",
      "Epoch 2/ 7720 train_loss  0.018021405604005725\n",
      "Epoch 2/ 7730 train_loss  0.0185596027782014\n",
      "Epoch 2/ 7740 train_loss  0.018984149370490557\n",
      "Epoch 2/ 7750 train_loss  0.019365429060174397\n",
      "Epoch 2/ 7760 train_loss  0.020303022212496562\n",
      "Epoch 2/ 7770 train_loss  0.020778002402299368\n",
      "Epoch 2/ 7780 train_loss  0.021102968833511065\n",
      "Epoch 2/ 7790 train_loss  0.02136812480208524\n",
      "Epoch 2/ 7800 train_loss  0.021925627676866868\n",
      "Epoch 2/ 7810 train_loss  0.022174014939800226\n",
      "Epoch 2/ 7820 train_loss  0.0224475596988404\n",
      "Epoch 2/ 7830 train_loss  0.022895243615707227\n",
      "Epoch 2/ 7840 train_loss  0.023833977095816677\n",
      "Epoch 2/ 7850 train_loss  0.024276363575686973\n",
      "Epoch 2/ 7860 train_loss  0.024656136547564295\n",
      "Epoch 2/ 7870 train_loss  0.024757937614329747\n",
      "Epoch 2/ 7880 train_loss  0.024929777174840063\n",
      "Epoch 2/ 7890 train_loss  0.025522563291708347\n",
      "Epoch 2/ 7900 train_loss  0.025838522209766488\n",
      "Epoch 2/ 7910 train_loss  0.026471103905738477\n",
      "Epoch 2/ 7920 train_loss  0.027055284668439285\n",
      "Epoch 2/ 7930 train_loss  0.027687294987688658\n",
      "Epoch 2/ 7940 train_loss  0.027843538529879502\n",
      "Epoch 2/ 7950 train_loss  0.028288278129705694\n",
      "Epoch 2/ 7960 train_loss  0.0286077357229149\n",
      "Epoch 2/ 7970 train_loss  0.029541248654719465\n",
      "Epoch 2/ 7980 train_loss  0.029869361180550176\n",
      "Epoch 2/ 7990 train_loss  0.03047882411263374\n",
      "Epoch 2/ 8000 train_loss  0.030592091205371024\n",
      "Epoch 2/ 8010 train_loss  0.03088054041526276\n",
      "Epoch 2/ 8020 train_loss  0.031136767524821245\n",
      "Epoch 2/ 8030 train_loss  0.031743135664495555\n",
      "Epoch 2/ 8040 train_loss  0.032094711729917916\n",
      "Epoch 2/ 8050 train_loss  0.03270000885149747\n",
      "Epoch 2/ 8060 train_loss  0.03301971256081656\n",
      "Epoch 2/ 8070 train_loss  0.033129511181513874\n",
      "Epoch 2/ 8080 train_loss  0.033297389273439004\n",
      "Epoch 2/ 8090 train_loss  0.03426841392991518\n",
      "Epoch 2/ 8100 train_loss  0.03480502705292517\n",
      "Epoch 2/ 8110 train_loss  0.03520431909642823\n",
      "Epoch 2/ 8120 train_loss  0.03559186763654691\n",
      "Epoch 2/ 8130 train_loss  0.03583802486210666\n",
      "Epoch 2/ 8140 train_loss  0.03616593901334961\n",
      "Epoch 2/ 8150 train_loss  0.03659443019162355\n",
      "Epoch 2/ 8160 train_loss  0.03676817140337508\n",
      "Epoch 2/ 8170 train_loss  0.03723768469790171\n",
      "Epoch 2/ 8180 train_loss  0.03735563833106367\n",
      "Epoch 2/ 8190 train_loss  0.03753435353332708\n",
      "Epoch 2/ 8200 train_loss  0.03832462422718788\n",
      "Epoch 2/ 8210 train_loss  0.03842427040495486\n",
      "Epoch 2/ 8220 train_loss  0.038770466334334404\n",
      "Epoch 2/ 8230 train_loss  0.03900306005728915\n",
      "Epoch 2/ 8240 train_loss  0.03942158420295381\n",
      "Epoch 2/ 8250 train_loss  0.039971474452044194\n",
      "Epoch 2/ 8260 train_loss  0.040051620759194564\n",
      "Epoch 2/ 8270 train_loss  0.04047399637121386\n",
      "Epoch 2/ 8280 train_loss  0.040801429195633857\n",
      "Epoch 2/ 8290 train_loss  0.04146561757300619\n",
      "Epoch 2/ 8300 train_loss  0.04180737822780963\n",
      "Epoch 2/ 8310 train_loss  0.04263932657209382\n",
      "Epoch 2/ 8320 train_loss  0.043313484742403055\n",
      "Epoch 2/ 8330 train_loss  0.04357212144720328\n",
      "Epoch 2/ 8340 train_loss  0.04414024216393003\n",
      "Epoch 2/ 8350 train_loss  0.04519024197574164\n",
      "Epoch 2/ 8360 train_loss  0.045858174656743446\n",
      "Epoch 2/ 8370 train_loss  0.046238112154158094\n",
      "Epoch 2/ 8380 train_loss  0.04665136950446003\n",
      "Epoch 2/ 8390 train_loss  0.047078260622020085\n",
      "Epoch 2/ 8400 train_loss  0.04755569050891075\n",
      "Epoch 2/ 8410 train_loss  0.047921257208318004\n",
      "Epoch 2/ 8420 train_loss  0.04802557850059519\n",
      "Epoch 2/ 8430 train_loss  0.04842388812326451\n",
      "Epoch 2/ 8440 train_loss  0.04917131621666287\n",
      "Epoch 2/ 8450 train_loss  0.04957682102955938\n",
      "Epoch 2/ 8460 train_loss  0.04989797986099165\n",
      "Epoch 2/ 8470 train_loss  0.050059070896428276\n",
      "Epoch 2/ 8480 train_loss  0.05026436931241522\n",
      "Epoch 2/ 8490 train_loss  0.05037840837298857\n",
      "Epoch 2/ 8500 train_loss  0.050585736937534445\n",
      "Epoch 2/ 8510 train_loss  0.050864456611487974\n",
      "Epoch 2/ 8520 train_loss  0.05103791421168749\n",
      "Epoch 2/ 8530 train_loss  0.05147360653193795\n",
      "Epoch 2/ 8540 train_loss  0.05158512815292025\n",
      "Epoch 2/ 8550 train_loss  0.052101885354003175\n",
      "Epoch 2/ 8560 train_loss  0.052509180522646276\n",
      "Epoch 2/ 8570 train_loss  0.052652037437327694\n",
      "Epoch 2/ 8580 train_loss  0.053198125788108404\n",
      "Epoch 2/ 8590 train_loss  0.053650414499854325\n",
      "Epoch 2/ 8600 train_loss  0.05394056414662931\n",
      "Epoch 2/ 8610 train_loss  0.054009944085399574\n",
      "Epoch 2/ 8620 train_loss  0.05411939215897793\n",
      "Epoch 2/ 8630 train_loss  0.05470122320764851\n",
      "Epoch 2/ 8640 train_loss  0.05501979262833877\n",
      "Epoch 2/ 8650 train_loss  0.055469836188411406\n",
      "Epoch 2/ 8660 train_loss  0.0559201146644018\n",
      "Epoch 2/ 8670 train_loss  0.056814612295064\n",
      "Epoch 2/ 8680 train_loss  0.057304749372019376\n",
      "Epoch 2/ 8690 train_loss  0.0574751197513187\n",
      "Epoch 2/ 8700 train_loss  0.057879578300495955\n",
      "Epoch 2/ 8710 train_loss  0.058471137306351485\n",
      "Epoch 2/ 8720 train_loss  0.05867279170592718\n",
      "Epoch 2/ 8730 train_loss  0.059198574401345475\n",
      "Epoch 2/ 8740 train_loss  0.059473399026213944\n",
      "Epoch 2/ 8750 train_loss  0.05992957557631502\n",
      "Epoch 2/ 8760 train_loss  0.060347099725124115\n",
      "Epoch 2/ 8770 train_loss  0.060600380606721065\n",
      "Epoch 2/ 8780 train_loss  0.06088648406928835\n",
      "Epoch 2/ 8790 train_loss  0.061084136221478444\n",
      "Epoch 2/ 8800 train_loss  0.061193655222994116\n",
      "Epoch 2/ 8810 train_loss  0.06151518235490615\n",
      "Epoch 2/ 8820 train_loss  0.06161945486180553\n",
      "Epoch 2/ 8830 train_loss  0.0618053834349042\n",
      "Epoch 2/ 8840 train_loss  0.062052539107232824\n",
      "Epoch 2/ 8850 train_loss  0.06277748286424284\n",
      "Epoch 2/ 8860 train_loss  0.0628786613241742\n",
      "Epoch 2/ 8870 train_loss  0.0630356651385341\n",
      "Epoch 2/ 8880 train_loss  0.06366791991156669\n",
      "Epoch 2/ 8890 train_loss  0.06380816310275896\n",
      "Epoch 2/ 8900 train_loss  0.06387044254757883\n",
      "Epoch 2/ 8910 train_loss  0.06416044831709178\n",
      "Epoch 2/ 8920 train_loss  0.06450544406732182\n",
      "Epoch 2/ 8930 train_loss  0.0647457763279759\n",
      "Epoch 2/ 8940 train_loss  0.06561344549473681\n",
      "Epoch 2/ 8950 train_loss  0.06581263471123684\n",
      "Epoch 2/ 8960 train_loss  0.06635653984284914\n",
      "Epoch 2/ 8970 train_loss  0.06690915827921788\n",
      "Epoch 2/ 8980 train_loss  0.06701427293432138\n",
      "Epoch 2/ 8990 train_loss  0.0677322706862043\n",
      "Epoch 2/ 9000 train_loss  0.06798735265016742\n",
      "Epoch 2/ 9010 train_loss  0.06831158516278364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 9020 train_loss  0.06863856079578255\n",
      "Epoch 2/ 9030 train_loss  0.06875065820281304\n",
      "Epoch 2/ 9040 train_loss  0.0691495667592068\n",
      "Epoch 2/ 9050 train_loss  0.06963058845867583\n",
      "Epoch 2/ 9060 train_loss  0.07007233530899723\n",
      "Epoch 2/ 9070 train_loss  0.07042264312830175\n",
      "Epoch 2/ 9080 train_loss  0.07062198610410139\n",
      "Epoch 2/ 9090 train_loss  0.07117287950055413\n",
      "Epoch 2/ 9100 train_loss  0.0716785944431486\n",
      "Epoch 2/ 9110 train_loss  0.07207119407831507\n",
      "Epoch 2/ 9120 train_loss  0.07226571059979789\n",
      "Epoch 2/ 9130 train_loss  0.0723477571482716\n",
      "Epoch 2/ 9140 train_loss  0.07261732220753632\n",
      "Epoch 2/ 9150 train_loss  0.0728519108782271\n",
      "Epoch 2/ 9160 train_loss  0.07342375174820541\n",
      "Epoch 2/ 9170 train_loss  0.07355749805413182\n",
      "Epoch 2/ 9180 train_loss  0.07371676593817028\n",
      "Epoch 2/ 9190 train_loss  0.07378996184216316\n",
      "Epoch 2/ 9200 train_loss  0.07379866421928244\n",
      "Epoch 2/ 9210 train_loss  0.07414042410214212\n",
      "Epoch 2/ 9220 train_loss  0.07443719672845704\n",
      "Epoch 2/ 9230 train_loss  0.07444982424946452\n",
      "Epoch 2/ 9240 train_loss  0.07447309802191057\n",
      "Epoch 2/ 9250 train_loss  0.07465890183542785\n",
      "Epoch 2/ 9260 train_loss  0.07565240246491797\n",
      "Epoch 2/ 9270 train_loss  0.07605909910100841\n",
      "Epoch 2/ 9280 train_loss  0.07636699147363488\n",
      "Epoch 2/ 9290 train_loss  0.07678108421383496\n",
      "Epoch 2/ 9300 train_loss  0.07703263118780594\n",
      "Epoch 2/ 9310 train_loss  0.07732719690671594\n",
      "Epoch 2/ 9320 train_loss  0.07796745454748226\n",
      "Epoch 2/ 9330 train_loss  0.07841868785677716\n",
      "Epoch 2/ 9340 train_loss  0.07859869798452536\n",
      "Epoch 2/ 9350 train_loss  0.0788702871577008\n",
      "Epoch 2/ 9360 train_loss  0.07891995953756756\n",
      "Epoch 2/ 9370 train_loss  0.07912152563418161\n",
      "Epoch 2/ 9380 train_loss  0.07943003444154582\n",
      "Epoch 2/ 9390 train_loss  0.07965164763956703\n",
      "Epoch 2/ 9400 train_loss  0.08005422717081784\n",
      "Epoch 2/ 9410 train_loss  0.08038102740186442\n",
      "Epoch 2/ 9420 train_loss  0.0806460765794013\n",
      "Epoch 2/ 9430 train_loss  0.08066821456827675\n",
      "Epoch 2/ 9440 train_loss  0.08091417397203578\n",
      "Epoch 2/ 9450 train_loss  0.0810037185569925\n",
      "Epoch 2/ 9460 train_loss  0.08126412770890903\n",
      "Epoch 2/ 9470 train_loss  0.08133805434541967\n",
      "Epoch 2/ 9480 train_loss  0.08150972554907021\n",
      "Epoch 2/ 9490 train_loss  0.08152766890211323\n",
      "Epoch 2/ 9500 train_loss  0.0820638938822034\n",
      "Epoch 2/ 9510 train_loss  0.08227468490047135\n",
      "Epoch 2/ 9520 train_loss  0.08239801943823435\n",
      "Epoch 2/ 9530 train_loss  0.08263172432935507\n",
      "Epoch 2/ 9540 train_loss  0.08316199979920676\n",
      "Epoch 2/ 9550 train_loss  0.08387885850371593\n",
      "Epoch 2/ 9560 train_loss  0.08448376510322593\n",
      "Epoch 2/ 9570 train_loss  0.08487090421327304\n",
      "Epoch 2/ 9580 train_loss  0.08518404666245055\n",
      "Epoch 2/ 9590 train_loss  0.08535301610118616\n",
      "Epoch 2/ 9600 train_loss  0.08573021786821099\n",
      "Epoch 2/ 9610 train_loss  0.08616650912544475\n",
      "Epoch 2/ 9620 train_loss  0.08637258324594572\n",
      "Epoch 2/ 9630 train_loss  0.08683401047944184\n",
      "Epoch 2/ 9640 train_loss  0.08714711793272904\n",
      "Epoch 2/ 9650 train_loss  0.08738069901838903\n",
      "Epoch 2/ 9660 train_loss  0.08783959986776596\n",
      "Epoch 2/ 9670 train_loss  0.08818823476411183\n",
      "Epoch 2/ 9680 train_loss  0.08893084066334639\n",
      "Epoch 2/ 9690 train_loss  0.08911075829059828\n",
      "Epoch 2/ 9700 train_loss  0.08940522210370158\n",
      "Epoch 2/ 9710 train_loss  0.08945954237047044\n",
      "Epoch 2/ 9720 train_loss  0.0899196811596233\n",
      "Epoch 2/ 9730 train_loss  0.09031092808218266\n",
      "Epoch 2/ 9740 train_loss  0.09072536328674664\n",
      "Epoch 2/ 9750 train_loss  0.09105282588785567\n",
      "Epoch 2/ 9760 train_loss  0.09159265568601183\n",
      "Epoch 2/ 9770 train_loss  0.09213344000633658\n",
      "Epoch 2/ 9780 train_loss  0.09216582447027218\n",
      "Epoch 2/ 9790 train_loss  0.0923467791979606\n",
      "Epoch 2/ 9800 train_loss  0.09241562908731633\n",
      "Epoch 2/ 9810 train_loss  0.09276094016788587\n",
      "Epoch 2/ 9820 train_loss  0.0929457351086195\n",
      "Epoch 2/ 9830 train_loss  0.09300538989820069\n",
      "Epoch 2/ 9840 train_loss  0.09342166986380428\n",
      "Epoch 2/ 9850 train_loss  0.09364398014620708\n",
      "Epoch 2/ 9860 train_loss  0.09387843681215625\n",
      "Epoch 2/ 9870 train_loss  0.09417785547837514\n",
      "Epoch 2/ 9880 train_loss  0.09450999991885893\n",
      "Epoch 2/ 9890 train_loss  0.09480988691253564\n",
      "Epoch 2/ 9900 train_loss  0.09494627411985361\n",
      "Epoch 2/ 9910 train_loss  0.0951530262001816\n",
      "Epoch 2/ 9920 train_loss  0.0952232949831439\n",
      "Epoch 2/ 9930 train_loss  0.09521744359582049\n",
      "Epoch 2/ 9940 train_loss  0.09554944455598427\n",
      "Epoch 2/ 9950 train_loss  0.0954993208344268\n",
      "Epoch 2/ 9960 train_loss  0.0957213754510427\n",
      "Epoch 2/ 9970 train_loss  0.09635092379827125\n",
      "Epoch 2/ 9980 train_loss  0.09639600837320553\n",
      "Epoch 2/ 9990 train_loss  0.09643170673990378\n",
      "Epoch 2/ 10000 train_loss  0.09654618641235964\n",
      "Epoch 2/ 10010 train_loss  0.0966280805396873\n",
      "Epoch 2/ 10020 train_loss  0.0970331637376089\n",
      "Epoch 2/ 10030 train_loss  0.09733006454617113\n",
      "Epoch 2/ 10040 train_loss  0.09745440805113909\n",
      "Epoch 2/ 10050 train_loss  0.09741027989035653\n",
      "Epoch 2/ 10060 train_loss  0.09742656722873227\n",
      "Epoch 2/ 10070 train_loss  0.09771875595642109\n",
      "Epoch 2/ 10080 train_loss  0.09803276979805729\n",
      "Epoch 2/ 10090 train_loss  0.09824946918937948\n",
      "Epoch 2/ 10100 train_loss  0.09827926598717601\n",
      "Epoch 2/ 10110 train_loss  0.09853774038062718\n",
      "Epoch 2/ 10120 train_loss  0.09875750430027329\n",
      "Epoch 2/ 10130 train_loss  0.09892382251396725\n",
      "Epoch 2/ 10140 train_loss  0.09896212854547637\n",
      "Epoch 2/ 10150 train_loss  0.09906832235411199\n",
      "Epoch 2/ 10160 train_loss  0.09918740522254017\n",
      "Epoch 2/ 10170 train_loss  0.09919989800216743\n",
      "Epoch 2/ 10180 train_loss  0.09931053235948141\n",
      "Epoch 2/ 10190 train_loss  0.09948207517444792\n",
      "Epoch 2/ 10200 train_loss  0.09952225861717202\n",
      "Epoch 2/ 10210 train_loss  0.0994604976698011\n",
      "Epoch 2/ 10220 train_loss  0.10009749994753776\n",
      "Epoch 2/ 10230 train_loss  0.1002797832269582\n",
      "Epoch 2/ 10240 train_loss  0.10049667852794737\n",
      "Epoch 2/ 10250 train_loss  0.1008049496375545\n",
      "Epoch 2/ 10260 train_loss  0.10103900670442216\n",
      "Epoch 2/ 10270 train_loss  0.10123426913643024\n",
      "Epoch 2/ 10280 train_loss  0.10143598331484718\n",
      "Epoch 2/ 10290 train_loss  0.10152188262288711\n",
      "Epoch 2/ 10300 train_loss  0.1018193007229743\n",
      "Epoch 2/ 10310 train_loss  0.10187216301535514\n",
      "Epoch 2/ 10320 train_loss  0.10235257201084826\n",
      "Epoch 2/ 10330 train_loss  0.10262996580552178\n",
      "Epoch 2/ 10340 train_loss  0.10267705438250053\n",
      "Epoch 2/ 10350 train_loss  0.10299706805392053\n",
      "Epoch 2/ 10360 train_loss  0.10314455138453259\n",
      "Epoch 2/ 10370 train_loss  0.1031648198728691\n",
      "Epoch 2/ 10380 train_loss  0.1036853230770119\n",
      "Epoch 2/ 10390 train_loss  0.10381134055802638\n",
      "Epoch 2/ 10400 train_loss  0.10396087931686616\n",
      "Epoch 2/ 10410 train_loss  0.10409115308504227\n",
      "Epoch 2/ 10420 train_loss  0.10405487862727468\n",
      "Epoch 2/ 10430 train_loss  0.10432449934072718\n",
      "Epoch 2/ 10440 train_loss  0.10454962499918685\n",
      "Epoch 2/ 10450 train_loss  0.10465234872112289\n",
      "Epoch 2/ 10460 train_loss  0.10472408908314619\n",
      "Epoch 2/ 10470 train_loss  0.10489002646128871\n",
      "Epoch 2/ 10480 train_loss  0.10506821931157995\n",
      "Epoch 2/ 10490 train_loss  0.10524121519150001\n",
      "Epoch 2/ 10500 train_loss  0.10542766329834409\n",
      "Epoch 2/ 10510 train_loss  0.10549318531129616\n",
      "Epoch 2/ 10520 train_loss  0.10584949744626529\n",
      "Epoch 2/ 10530 train_loss  0.10584908294397756\n",
      "Epoch 2/ 10540 train_loss  0.10613932428041695\n",
      "Epoch 2/ 10550 train_loss  0.10660149350310867\n",
      "Epoch 2/ 10560 train_loss  0.10681374352776825\n",
      "Epoch 2/ 10570 train_loss  0.10709076028691718\n",
      "Epoch 2/ 10580 train_loss  0.10738767755873686\n",
      "Epoch 2/ 10590 train_loss  0.10782213705171524\n",
      "Epoch 2/ 10600 train_loss  0.10793227851727821\n",
      "Epoch 2/ 10610 train_loss  0.10796224687139222\n",
      "Epoch 2/ 10620 train_loss  0.10796794296066067\n",
      "Epoch 2/ 10630 train_loss  0.10821491685536529\n",
      "Epoch 2/ 10640 train_loss  0.10819228372836866\n",
      "Epoch 2/ 10650 train_loss  0.10822102784145084\n",
      "Epoch 2/ 10660 train_loss  0.10818064662986551\n",
      "Epoch 2/ 10670 train_loss  0.10840641911041671\n",
      "Epoch 2/ 10680 train_loss  0.10866988982285902\n",
      "Epoch 2/ 10690 train_loss  0.10881737507854505\n",
      "Epoch 2/ 10700 train_loss  0.1090908262512746\n",
      "Epoch 2/ 10710 train_loss  0.1090217065863162\n",
      "Epoch 2/ 10720 train_loss  0.10909414881286977\n",
      "Epoch 2/ 10730 train_loss  0.10957358506483472\n",
      "Epoch 2/ 10740 train_loss  0.10963742919207511\n",
      "Epoch 2/ 10750 train_loss  0.1099027288611473\n",
      "Epoch 2/ 10760 train_loss  0.11015985746993603\n",
      "Epoch 2/ 10770 train_loss  0.11059553239763208\n",
      "Epoch 2/ 10780 train_loss  0.11070356628953379\n",
      "Epoch 2/ 10790 train_loss  0.1109210055534038\n",
      "Epoch 2/ 10800 train_loss  0.11112201211144386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 10810 train_loss  0.11112898486017572\n",
      "Epoch 2/ 10820 train_loss  0.11156554400409947\n",
      "Epoch 2/ 10830 train_loss  0.11175937567030782\n",
      "Epoch 2/ 10840 train_loss  0.11202241024207646\n",
      "Epoch 2/ 10850 train_loss  0.11208744155688405\n",
      "Epoch 2/ 10860 train_loss  0.11255629429064007\n",
      "Epoch 2/ 10870 train_loss  0.1130124534688106\n",
      "Epoch 2/ 10880 train_loss  0.11318773245405318\n",
      "Epoch 2/ 10890 train_loss  0.11339770673678168\n",
      "Epoch 2/ 10900 train_loss  0.11367685809087016\n",
      "Epoch 2/ 10910 train_loss  0.11396166864078087\n",
      "Epoch 2/ 10920 train_loss  0.11466738836377315\n",
      "Epoch 2/ 10930 train_loss  0.1149662975646696\n",
      "Epoch 2/ 10940 train_loss  0.11513199746344707\n",
      "Epoch 2/ 10950 train_loss  0.11529760200880688\n",
      "Epoch 2/ 10960 train_loss  0.11543658259946728\n",
      "Epoch 2/ 10970 train_loss  0.11561131597410806\n",
      "Epoch 2/ 10980 train_loss  0.11583874642787881\n",
      "Epoch 2/ 10990 train_loss  0.11608638243193808\n",
      "Epoch 2/ 11000 train_loss  0.11614365841513483\n",
      "Epoch 2/ 11010 train_loss  0.11642475946035961\n",
      "Epoch 2/ 11020 train_loss  0.11662537177402836\n",
      "Epoch 2/ 11030 train_loss  0.11674535564227344\n",
      "Epoch 2/ 11040 train_loss  0.11688877349967615\n",
      "Epoch 2/ 11050 train_loss  0.11719453895437763\n",
      "Epoch 2/ 11060 train_loss  0.11736052375028287\n",
      "Epoch 2/ 11070 train_loss  0.11756771454701138\n",
      "Epoch 2/ 11080 train_loss  0.11788832874397061\n",
      "Epoch 2/ 11090 train_loss  0.11805287248566768\n",
      "Epoch 2/ 11100 train_loss  0.11823594781465871\n",
      "Train Loss: 0.3552438208378399, Train Acc: 0.844138303619665\n",
      "Test Loss: 0.4356268344835099, Test Acc: 0.8051948051948052\n",
      "保存模型成功! 路径为: ’model/word_sense_disambiguation.bin‘\n",
      "Epoch 3/ 11110 train_loss  0.0001905942107945754\n",
      "Epoch 3/ 11120 train_loss  0.0008242641591359888\n",
      "Epoch 3/ 11130 train_loss  0.001559318661466438\n",
      "Epoch 3/ 11140 train_loss  0.002143813857087323\n",
      "Epoch 3/ 11150 train_loss  0.0024231870535202454\n",
      "Epoch 3/ 11160 train_loss  0.002709141850365609\n",
      "Epoch 3/ 11170 train_loss  0.002985616284059982\n",
      "Epoch 3/ 11180 train_loss  0.0036712059140046405\n",
      "Epoch 3/ 11190 train_loss  0.004017413620578289\n",
      "Epoch 3/ 11200 train_loss  0.004298593714730453\n",
      "Epoch 3/ 11210 train_loss  0.004514390595601535\n",
      "Epoch 3/ 11220 train_loss  0.004657824388320351\n",
      "Epoch 3/ 11230 train_loss  0.0049301423182563095\n",
      "Epoch 3/ 11240 train_loss  0.005173356337672787\n",
      "Epoch 3/ 11250 train_loss  0.005588260681034265\n",
      "Epoch 3/ 11260 train_loss  0.0058157157170437125\n",
      "Epoch 3/ 11270 train_loss  0.005933125638490896\n",
      "Epoch 3/ 11280 train_loss  0.006112703317851758\n",
      "Epoch 3/ 11290 train_loss  0.006434901479931406\n",
      "Epoch 3/ 11300 train_loss  0.0065637532670836845\n",
      "Epoch 3/ 11310 train_loss  0.006827053598065154\n",
      "Epoch 3/ 11320 train_loss  0.007241920698612124\n",
      "Epoch 3/ 11330 train_loss  0.007360933832525324\n",
      "Epoch 3/ 11340 train_loss  0.00787587469653626\n",
      "Epoch 3/ 11350 train_loss  0.008025429283495521\n",
      "Epoch 3/ 11360 train_loss  0.008483623356853\n",
      "Epoch 3/ 11370 train_loss  0.008987913079038332\n",
      "Epoch 3/ 11380 train_loss  0.009274664669157991\n",
      "Epoch 3/ 11390 train_loss  0.009900246623674048\n",
      "Epoch 3/ 11400 train_loss  0.010199020611131536\n",
      "Epoch 3/ 11410 train_loss  0.010328805176346427\n",
      "Epoch 3/ 11420 train_loss  0.010827047868376821\n",
      "Epoch 3/ 11430 train_loss  0.011175712700282486\n",
      "Epoch 3/ 11440 train_loss  0.011381622933526495\n",
      "Epoch 3/ 11450 train_loss  0.011694668717455725\n",
      "Epoch 3/ 11460 train_loss  0.011988167983479282\n",
      "Epoch 3/ 11470 train_loss  0.01252580326451741\n",
      "Epoch 3/ 11480 train_loss  0.012695611856298052\n",
      "Epoch 3/ 11490 train_loss  0.012871278511268992\n",
      "Epoch 3/ 11500 train_loss  0.013134542982516215\n",
      "Epoch 3/ 11510 train_loss  0.01336669455774459\n",
      "Epoch 3/ 11520 train_loss  0.013487416228576464\n",
      "Epoch 3/ 11530 train_loss  0.01378502277348136\n",
      "Epoch 3/ 11540 train_loss  0.014362615194072696\n",
      "Epoch 3/ 11550 train_loss  0.014612281332011286\n",
      "Epoch 3/ 11560 train_loss  0.014681223339167038\n",
      "Epoch 3/ 11570 train_loss  0.014856911563897573\n",
      "Epoch 3/ 11580 train_loss  0.014917591637112527\n",
      "Epoch 3/ 11590 train_loss  0.015328208424571497\n",
      "Epoch 3/ 11600 train_loss  0.015407847395657598\n",
      "Epoch 3/ 11610 train_loss  0.015625371483855986\n",
      "Epoch 3/ 11620 train_loss  0.016188283253313143\n",
      "Epoch 3/ 11630 train_loss  0.016418057250801803\n",
      "Epoch 3/ 11640 train_loss  0.016470520871150827\n",
      "Epoch 3/ 11650 train_loss  0.01665136319914614\n",
      "Epoch 3/ 11660 train_loss  0.016897945469270367\n",
      "Epoch 3/ 11670 train_loss  0.017330378868563142\n",
      "Epoch 3/ 11680 train_loss  0.017452222544657937\n",
      "Epoch 3/ 11690 train_loss  0.017647451597065426\n",
      "Epoch 3/ 11700 train_loss  0.01774480967354297\n",
      "Epoch 3/ 11710 train_loss  0.017833366901746087\n",
      "Epoch 3/ 11720 train_loss  0.018084646439643345\n",
      "Epoch 3/ 11730 train_loss  0.018271395136624746\n",
      "Epoch 3/ 11740 train_loss  0.018776529742456673\n",
      "Epoch 3/ 11750 train_loss  0.019037580165850644\n",
      "Epoch 3/ 11760 train_loss  0.019113571402597288\n",
      "Epoch 3/ 11770 train_loss  0.019161453257437004\n",
      "Epoch 3/ 11780 train_loss  0.019426156538301546\n",
      "Epoch 3/ 11790 train_loss  0.020040533673942523\n",
      "Epoch 3/ 11800 train_loss  0.0203001644708517\n",
      "Epoch 3/ 11810 train_loss  0.020689406952100038\n",
      "Epoch 3/ 11820 train_loss  0.02087963034188094\n",
      "Epoch 3/ 11830 train_loss  0.02108036492250173\n",
      "Epoch 3/ 11840 train_loss  0.021140482006417646\n",
      "Epoch 3/ 11850 train_loss  0.021668126359407167\n",
      "Epoch 3/ 11860 train_loss  0.02174472745775543\n",
      "Epoch 3/ 11870 train_loss  0.022079018463439373\n",
      "Epoch 3/ 11880 train_loss  0.02211958004069513\n",
      "Epoch 3/ 11890 train_loss  0.02222304528041695\n",
      "Epoch 3/ 11900 train_loss  0.02233182497941526\n",
      "Epoch 3/ 11910 train_loss  0.022636566751042943\n",
      "Epoch 3/ 11920 train_loss  0.022664272065586834\n",
      "Epoch 3/ 11930 train_loss  0.023099854776165704\n",
      "Epoch 3/ 11940 train_loss  0.02350590874302624\n",
      "Epoch 3/ 11950 train_loss  0.0237290468559159\n",
      "Epoch 3/ 11960 train_loss  0.023939156295687172\n",
      "Epoch 3/ 11970 train_loss  0.024217409329708176\n",
      "Epoch 3/ 11980 train_loss  0.024437330370229285\n",
      "Epoch 3/ 11990 train_loss  0.024587845560492493\n",
      "Epoch 3/ 12000 train_loss  0.024711001072561183\n",
      "Epoch 3/ 12010 train_loss  0.02537329557894693\n",
      "Epoch 3/ 12020 train_loss  0.02562395944474789\n",
      "Epoch 3/ 12030 train_loss  0.0259401107661929\n",
      "Epoch 3/ 12040 train_loss  0.026212090205230514\n",
      "Epoch 3/ 12050 train_loss  0.026604004278756897\n",
      "Epoch 3/ 12060 train_loss  0.027352564955655623\n",
      "Epoch 3/ 12070 train_loss  0.027536021003361017\n",
      "Epoch 3/ 12080 train_loss  0.027859536810453014\n",
      "Epoch 3/ 12090 train_loss  0.02812744558694576\n",
      "Epoch 3/ 12100 train_loss  0.028333105697132127\n",
      "Epoch 3/ 12110 train_loss  0.02850178791145175\n",
      "Epoch 3/ 12120 train_loss  0.028572758408637055\n",
      "Epoch 3/ 12130 train_loss  0.02890310661075396\n",
      "Epoch 3/ 12140 train_loss  0.02932439803186206\n",
      "Epoch 3/ 12150 train_loss  0.0294987426096078\n",
      "Epoch 3/ 12160 train_loss  0.029618943618197197\n",
      "Epoch 3/ 12170 train_loss  0.02987597239954959\n",
      "Epoch 3/ 12180 train_loss  0.02994049446607703\n",
      "Epoch 3/ 12190 train_loss  0.030120721668016725\n",
      "Epoch 3/ 12200 train_loss  0.030168988298427315\n",
      "Epoch 3/ 12210 train_loss  0.030187669128899867\n",
      "Epoch 3/ 12220 train_loss  0.03034396107845972\n",
      "Epoch 3/ 12230 train_loss  0.03047283321513288\n",
      "Epoch 3/ 12240 train_loss  0.030647296983366604\n",
      "Epoch 3/ 12250 train_loss  0.030884512686234395\n",
      "Epoch 3/ 12260 train_loss  0.031081041513430836\n",
      "Epoch 3/ 12270 train_loss  0.031127753085729175\n",
      "Epoch 3/ 12280 train_loss  0.031294004251073836\n",
      "Epoch 3/ 12290 train_loss  0.03152010779024491\n",
      "Epoch 3/ 12300 train_loss  0.03176261433925227\n",
      "Epoch 3/ 12310 train_loss  0.03188082189555175\n",
      "Epoch 3/ 12320 train_loss  0.032168015952201534\n",
      "Epoch 3/ 12330 train_loss  0.03245776607342668\n",
      "Epoch 3/ 12340 train_loss  0.03255442772173094\n",
      "Epoch 3/ 12350 train_loss  0.03278905913538425\n",
      "Epoch 3/ 12360 train_loss  0.03307526309532574\n",
      "Epoch 3/ 12370 train_loss  0.03323713699951641\n",
      "Epoch 3/ 12380 train_loss  0.03364394143449549\n",
      "Epoch 3/ 12390 train_loss  0.03373784148521129\n",
      "Epoch 3/ 12400 train_loss  0.033839394801299195\n",
      "Epoch 3/ 12410 train_loss  0.03415209857021927\n",
      "Epoch 3/ 12420 train_loss  0.03431206982839923\n",
      "Epoch 3/ 12430 train_loss  0.034467779351968805\n",
      "Epoch 3/ 12440 train_loss  0.03460295759674587\n",
      "Epoch 3/ 12450 train_loss  0.03492508776420193\n",
      "Epoch 3/ 12460 train_loss  0.03502532129452619\n",
      "Epoch 3/ 12470 train_loss  0.03515902446101297\n",
      "Epoch 3/ 12480 train_loss  0.03523161794531271\n",
      "Epoch 3/ 12490 train_loss  0.03535319880669829\n",
      "Epoch 3/ 12500 train_loss  0.0354528438563059\n",
      "Epoch 3/ 12510 train_loss  0.035477844856268766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 12520 train_loss  0.035562951310636214\n",
      "Epoch 3/ 12530 train_loss  0.03558222216524496\n",
      "Epoch 3/ 12540 train_loss  0.035857253512002384\n",
      "Epoch 3/ 12550 train_loss  0.03625674648487261\n",
      "Epoch 3/ 12560 train_loss  0.036362016534875294\n",
      "Epoch 3/ 12570 train_loss  0.0363567705677714\n",
      "Epoch 3/ 12580 train_loss  0.03657224482962912\n",
      "Epoch 3/ 12590 train_loss  0.03670478214855595\n",
      "Epoch 3/ 12600 train_loss  0.0369510262905552\n",
      "Epoch 3/ 12610 train_loss  0.037150300333254456\n",
      "Epoch 3/ 12620 train_loss  0.0373708389653159\n",
      "Epoch 3/ 12630 train_loss  0.03753189274147953\n",
      "Epoch 3/ 12640 train_loss  0.037565288687984186\n",
      "Epoch 3/ 12650 train_loss  0.03768175851087865\n",
      "Epoch 3/ 12660 train_loss  0.038181578050115246\n",
      "Epoch 3/ 12670 train_loss  0.03871958433553483\n",
      "Epoch 3/ 12680 train_loss  0.039044923529548206\n",
      "Epoch 3/ 12690 train_loss  0.03939258600605422\n",
      "Epoch 3/ 12700 train_loss  0.0394049188147094\n",
      "Epoch 3/ 12710 train_loss  0.03955598790968027\n",
      "Epoch 3/ 12720 train_loss  0.03975369503782808\n",
      "Epoch 3/ 12730 train_loss  0.039747528285778384\n",
      "Epoch 3/ 12740 train_loss  0.040002713459491655\n",
      "Epoch 3/ 12750 train_loss  0.040239501740008114\n",
      "Epoch 3/ 12760 train_loss  0.04037850237947704\n",
      "Epoch 3/ 12770 train_loss  0.04056595005402301\n",
      "Epoch 3/ 12780 train_loss  0.04077902357685252\n",
      "Epoch 3/ 12790 train_loss  0.0411127876542517\n",
      "Epoch 3/ 12800 train_loss  0.04162249840811531\n",
      "Epoch 3/ 12810 train_loss  0.041946287174641925\n",
      "Epoch 3/ 12820 train_loss  0.0420017103333591\n",
      "Epoch 3/ 12830 train_loss  0.04207659704226602\n",
      "Epoch 3/ 12840 train_loss  0.042183277274493954\n",
      "Epoch 3/ 12850 train_loss  0.0423604147784974\n",
      "Epoch 3/ 12860 train_loss  0.04278669491364888\n",
      "Epoch 3/ 12870 train_loss  0.04283853326878089\n",
      "Epoch 3/ 12880 train_loss  0.04299020409832317\n",
      "Epoch 3/ 12890 train_loss  0.04303211835325974\n",
      "Epoch 3/ 12900 train_loss  0.04304812776178465\n",
      "Epoch 3/ 12910 train_loss  0.043296029182603675\n",
      "Epoch 3/ 12920 train_loss  0.043866753561346214\n",
      "Epoch 3/ 12930 train_loss  0.043979512816481256\n",
      "Epoch 3/ 12940 train_loss  0.04400027094277056\n",
      "Epoch 3/ 12950 train_loss  0.044086721309296945\n",
      "Epoch 3/ 12960 train_loss  0.044274096653083725\n",
      "Epoch 3/ 12970 train_loss  0.04472108567493997\n",
      "Epoch 3/ 12980 train_loss  0.04492713213750421\n",
      "Epoch 3/ 12990 train_loss  0.04508258526675888\n",
      "Epoch 3/ 13000 train_loss  0.045305617449514855\n",
      "Epoch 3/ 13010 train_loss  0.045531148782735696\n",
      "Epoch 3/ 13020 train_loss  0.046010132909321495\n",
      "Epoch 3/ 13030 train_loss  0.04623493912837315\n",
      "Epoch 3/ 13040 train_loss  0.046336036879695744\n",
      "Epoch 3/ 13050 train_loss  0.04660044246874254\n",
      "Epoch 3/ 13060 train_loss  0.04661032980469174\n",
      "Epoch 3/ 13070 train_loss  0.04669017810175055\n",
      "Epoch 3/ 13080 train_loss  0.046821316281641906\n",
      "Epoch 3/ 13090 train_loss  0.04698712999190473\n",
      "Epoch 3/ 13100 train_loss  0.04720917284588283\n",
      "Epoch 3/ 13110 train_loss  0.047489067601572836\n",
      "Epoch 3/ 13120 train_loss  0.04752806765885449\n",
      "Epoch 3/ 13130 train_loss  0.04755133678206627\n",
      "Epoch 3/ 13140 train_loss  0.04769810726750139\n",
      "Epoch 3/ 13150 train_loss  0.047745822901331884\n",
      "Epoch 3/ 13160 train_loss  0.047763052396397394\n",
      "Epoch 3/ 13170 train_loss  0.04787364134425584\n",
      "Epoch 3/ 13180 train_loss  0.047909083504685236\n",
      "Epoch 3/ 13190 train_loss  0.04809883320964986\n",
      "Epoch 3/ 13200 train_loss  0.04841518242579384\n",
      "Epoch 3/ 13210 train_loss  0.04872509496581193\n",
      "Epoch 3/ 13220 train_loss  0.048880643418207255\n",
      "Epoch 3/ 13230 train_loss  0.04909839548299727\n",
      "Epoch 3/ 13240 train_loss  0.04924079686078634\n",
      "Epoch 3/ 13250 train_loss  0.04956441342949783\n",
      "Epoch 3/ 13260 train_loss  0.04986353663798916\n",
      "Epoch 3/ 13270 train_loss  0.05013599707907825\n",
      "Epoch 3/ 13280 train_loss  0.050430610322099494\n",
      "Epoch 3/ 13290 train_loss  0.05074294854101954\n",
      "Epoch 3/ 13300 train_loss  0.050957784790233954\n",
      "Epoch 3/ 13310 train_loss  0.05127418896628727\n",
      "Epoch 3/ 13320 train_loss  0.05144191019281426\n",
      "Epoch 3/ 13330 train_loss  0.05169959850811553\n",
      "Epoch 3/ 13340 train_loss  0.051820165232257326\n",
      "Epoch 3/ 13350 train_loss  0.052070650289333824\n",
      "Epoch 3/ 13360 train_loss  0.05242892841262308\n",
      "Epoch 3/ 13370 train_loss  0.05267088241312672\n",
      "Epoch 3/ 13380 train_loss  0.05286040064521323\n",
      "Epoch 3/ 13390 train_loss  0.05300407376944381\n",
      "Epoch 3/ 13400 train_loss  0.053218611051288375\n",
      "Epoch 3/ 13410 train_loss  0.05327442104988256\n",
      "Epoch 3/ 13420 train_loss  0.053579689843726415\n",
      "Epoch 3/ 13430 train_loss  0.053781544079352796\n",
      "Epoch 3/ 13440 train_loss  0.053942083179083694\n",
      "Epoch 3/ 13450 train_loss  0.05421925113245236\n",
      "Epoch 3/ 13460 train_loss  0.05449536124117351\n",
      "Epoch 3/ 13470 train_loss  0.0546960281578369\n",
      "Epoch 3/ 13480 train_loss  0.05487238240758847\n",
      "Epoch 3/ 13490 train_loss  0.05495571805926038\n",
      "Epoch 3/ 13500 train_loss  0.05499689806323071\n",
      "Epoch 3/ 13510 train_loss  0.05519520216795163\n",
      "Epoch 3/ 13520 train_loss  0.05532583046592923\n",
      "Epoch 3/ 13530 train_loss  0.055355651641752456\n",
      "Epoch 3/ 13540 train_loss  0.05579345967046603\n",
      "Epoch 3/ 13550 train_loss  0.05587527805244763\n",
      "Epoch 3/ 13560 train_loss  0.05589636489350244\n",
      "Epoch 3/ 13570 train_loss  0.05606614248112981\n",
      "Epoch 3/ 13580 train_loss  0.056252149481697225\n",
      "Epoch 3/ 13590 train_loss  0.05624926820027987\n",
      "Epoch 3/ 13600 train_loss  0.05631176374971285\n",
      "Epoch 3/ 13610 train_loss  0.05652361866106039\n",
      "Epoch 3/ 13620 train_loss  0.05655512279561617\n",
      "Epoch 3/ 13630 train_loss  0.05656000474705901\n",
      "Epoch 3/ 13640 train_loss  0.056721607382038554\n",
      "Epoch 3/ 13650 train_loss  0.05672437113660354\n",
      "Epoch 3/ 13660 train_loss  0.056711780335177056\n",
      "Epoch 3/ 13670 train_loss  0.05678050335186408\n",
      "Epoch 3/ 13680 train_loss  0.0568003988829217\n",
      "Epoch 3/ 13690 train_loss  0.05701768360303316\n",
      "Epoch 3/ 13700 train_loss  0.057051313256728325\n",
      "Epoch 3/ 13710 train_loss  0.057216780368522756\n",
      "Epoch 3/ 13720 train_loss  0.057591673840653355\n",
      "Epoch 3/ 13730 train_loss  0.05783939929580964\n",
      "Epoch 3/ 13740 train_loss  0.057944470561420554\n",
      "Epoch 3/ 13750 train_loss  0.05792847308687386\n",
      "Epoch 3/ 13760 train_loss  0.05792158106011861\n",
      "Epoch 3/ 13770 train_loss  0.057923397084315556\n",
      "Epoch 3/ 13780 train_loss  0.058298248690843914\n",
      "Epoch 3/ 13790 train_loss  0.058476904022617064\n",
      "Epoch 3/ 13800 train_loss  0.058587220478354414\n",
      "Epoch 3/ 13810 train_loss  0.05872126454742725\n",
      "Epoch 3/ 13820 train_loss  0.05920825866894511\n",
      "Epoch 3/ 13830 train_loss  0.05942495218836172\n",
      "Epoch 3/ 13840 train_loss  0.059646675515947295\n",
      "Epoch 3/ 13850 train_loss  0.059912385605051494\n",
      "Epoch 3/ 13860 train_loss  0.05996700532770456\n",
      "Epoch 3/ 13870 train_loss  0.06007954320531027\n",
      "Epoch 3/ 13880 train_loss  0.06017994578719696\n",
      "Epoch 3/ 13890 train_loss  0.06018024415109597\n",
      "Epoch 3/ 13900 train_loss  0.06033683018543092\n",
      "Epoch 3/ 13910 train_loss  0.06032709561410492\n",
      "Epoch 3/ 13920 train_loss  0.06044529531816709\n",
      "Epoch 3/ 13930 train_loss  0.06063759755465493\n",
      "Epoch 3/ 13940 train_loss  0.06085261273055471\n",
      "Epoch 3/ 13950 train_loss  0.0609945419248168\n",
      "Epoch 3/ 13960 train_loss  0.061220532277735856\n",
      "Epoch 3/ 13970 train_loss  0.06122105230982397\n",
      "Epoch 3/ 13980 train_loss  0.061342575970764465\n",
      "Epoch 3/ 13990 train_loss  0.061377246866922135\n",
      "Epoch 3/ 14000 train_loss  0.06169022594395904\n",
      "Epoch 3/ 14010 train_loss  0.061784438968801404\n",
      "Epoch 3/ 14020 train_loss  0.06200180477914698\n",
      "Epoch 3/ 14030 train_loss  0.062109111755769164\n",
      "Epoch 3/ 14040 train_loss  0.062121887123606355\n",
      "Epoch 3/ 14050 train_loss  0.06218664526928897\n",
      "Epoch 3/ 14060 train_loss  0.06260399662605486\n",
      "Epoch 3/ 14070 train_loss  0.06264629138751542\n",
      "Epoch 3/ 14080 train_loss  0.06287710323993066\n",
      "Epoch 3/ 14090 train_loss  0.06290879591542418\n",
      "Epoch 3/ 14100 train_loss  0.06304019913900794\n",
      "Epoch 3/ 14110 train_loss  0.06309650567768735\n",
      "Epoch 3/ 14120 train_loss  0.06305845862861752\n",
      "Epoch 3/ 14130 train_loss  0.06307697104632645\n",
      "Epoch 3/ 14140 train_loss  0.06338604157416212\n",
      "Epoch 3/ 14150 train_loss  0.06344641679739393\n",
      "Epoch 3/ 14160 train_loss  0.06356251904802701\n",
      "Epoch 3/ 14170 train_loss  0.06365927413052673\n",
      "Epoch 3/ 14180 train_loss  0.06374385386074502\n",
      "Epoch 3/ 14190 train_loss  0.06373815510024451\n",
      "Epoch 3/ 14200 train_loss  0.06386072406725507\n",
      "Epoch 3/ 14210 train_loss  0.06386433067455295\n",
      "Epoch 3/ 14220 train_loss  0.06390720648695956\n",
      "Epoch 3/ 14230 train_loss  0.06424619333010916\n",
      "Epoch 3/ 14240 train_loss  0.06445186419338249\n",
      "Epoch 3/ 14250 train_loss  0.06463404381542796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 14260 train_loss  0.06473917470152933\n",
      "Epoch 3/ 14270 train_loss  0.064799523673695\n",
      "Epoch 3/ 14280 train_loss  0.06485257454480929\n",
      "Epoch 3/ 14290 train_loss  0.06502602239294802\n",
      "Epoch 3/ 14300 train_loss  0.06502513885525388\n",
      "Epoch 3/ 14310 train_loss  0.06503831837405676\n",
      "Epoch 3/ 14320 train_loss  0.06503952452338642\n",
      "Epoch 3/ 14330 train_loss  0.06549197015141313\n",
      "Epoch 3/ 14340 train_loss  0.06549919803498717\n",
      "Epoch 3/ 14350 train_loss  0.06548118116107605\n",
      "Epoch 3/ 14360 train_loss  0.06547119897388684\n",
      "Epoch 3/ 14370 train_loss  0.06564819132501247\n",
      "Epoch 3/ 14380 train_loss  0.06571109310950243\n",
      "Epoch 3/ 14390 train_loss  0.06582539149144902\n",
      "Epoch 3/ 14400 train_loss  0.06593416793746824\n",
      "Epoch 3/ 14410 train_loss  0.06599959712113171\n",
      "Epoch 3/ 14420 train_loss  0.06598223710105086\n",
      "Epoch 3/ 14430 train_loss  0.0660889229345134\n",
      "Epoch 3/ 14440 train_loss  0.06605297086829177\n",
      "Epoch 3/ 14450 train_loss  0.06617507650197256\n",
      "Epoch 3/ 14460 train_loss  0.0663802036310561\n",
      "Epoch 3/ 14470 train_loss  0.06665938493131963\n",
      "Epoch 3/ 14480 train_loss  0.06706176107401873\n",
      "Epoch 3/ 14490 train_loss  0.0671221863842247\n",
      "Epoch 3/ 14500 train_loss  0.06719484172396907\n",
      "Epoch 3/ 14510 train_loss  0.06722298760740804\n",
      "Epoch 3/ 14520 train_loss  0.06753564087775352\n",
      "Epoch 3/ 14530 train_loss  0.06782335443811252\n",
      "Epoch 3/ 14540 train_loss  0.06802048311406571\n",
      "Epoch 3/ 14550 train_loss  0.06804498094798588\n",
      "Epoch 3/ 14560 train_loss  0.06813707715932268\n",
      "Epoch 3/ 14570 train_loss  0.06852287045482619\n",
      "Epoch 3/ 14580 train_loss  0.06863718112058885\n",
      "Epoch 3/ 14590 train_loss  0.06879130625394668\n",
      "Epoch 3/ 14600 train_loss  0.06898552056155251\n",
      "Epoch 3/ 14610 train_loss  0.06921053467819921\n",
      "Epoch 3/ 14620 train_loss  0.0693561251958133\n",
      "Epoch 3/ 14630 train_loss  0.06952145380652064\n",
      "Epoch 3/ 14640 train_loss  0.06968857015162211\n",
      "Epoch 3/ 14650 train_loss  0.0697624335943294\n",
      "Epoch 3/ 14660 train_loss  0.06983201654860181\n",
      "Epoch 3/ 14670 train_loss  0.06987712146240165\n",
      "Epoch 3/ 14680 train_loss  0.0701003176423208\n",
      "Epoch 3/ 14690 train_loss  0.07036407133931734\n",
      "Epoch 3/ 14700 train_loss  0.07046822870101174\n",
      "Epoch 3/ 14710 train_loss  0.0706211044137361\n",
      "Epoch 3/ 14720 train_loss  0.07084428567138301\n",
      "Epoch 3/ 14730 train_loss  0.07089242463301537\n",
      "Epoch 3/ 14740 train_loss  0.07103192434549348\n",
      "Epoch 3/ 14750 train_loss  0.07120062774759625\n",
      "Epoch 3/ 14760 train_loss  0.0713075311551422\n",
      "Epoch 3/ 14770 train_loss  0.07143936868333897\n",
      "Epoch 3/ 14780 train_loss  0.07172105959574528\n",
      "Epoch 3/ 14790 train_loss  0.07187368267143754\n",
      "Epoch 3/ 14800 train_loss  0.07195429176625326\n",
      "Train Loss: 0.28815030970480315, Train Acc: 0.8806050783360346\n",
      "Test Loss: 0.44941776451662313, Test Acc: 0.8165584415584416\n",
      "保存模型成功! 路径为: ’model/word_sense_disambiguation.bin‘\n",
      "Epoch 4/ 14810 train_loss  0.0001764722001881972\n",
      "Epoch 4/ 14820 train_loss  0.00029869388630912814\n",
      "Epoch 4/ 14830 train_loss  0.0008818516360216339\n",
      "Epoch 4/ 14840 train_loss  0.0011977056568967146\n",
      "Epoch 4/ 14850 train_loss  0.0013934944057496412\n",
      "Epoch 4/ 14860 train_loss  0.0015675966776756544\n",
      "Epoch 4/ 14870 train_loss  0.0018514650918711332\n",
      "Epoch 4/ 14880 train_loss  0.0023746942708103825\n",
      "Epoch 4/ 14890 train_loss  0.0024677217377461367\n",
      "Epoch 4/ 14900 train_loss  0.0028234041550274356\n",
      "Epoch 4/ 14910 train_loss  0.002941199999916276\n",
      "Epoch 4/ 14920 train_loss  0.003160686360683616\n",
      "Epoch 4/ 14930 train_loss  0.003356526033994266\n",
      "Epoch 4/ 14940 train_loss  0.003544784567751041\n",
      "Epoch 4/ 14950 train_loss  0.003802802350299687\n",
      "Epoch 4/ 14960 train_loss  0.003944438688576414\n",
      "Epoch 4/ 14970 train_loss  0.004006385931908665\n",
      "Epoch 4/ 14980 train_loss  0.004094463941710036\n",
      "Epoch 4/ 14990 train_loss  0.004228367812048861\n",
      "Epoch 4/ 15000 train_loss  0.004397157685714028\n",
      "Epoch 4/ 15010 train_loss  0.004479018188699698\n",
      "Epoch 4/ 15020 train_loss  0.004709569216782155\n",
      "Epoch 4/ 15030 train_loss  0.004821377284660673\n",
      "Epoch 4/ 15040 train_loss  0.005165477163821582\n",
      "Epoch 4/ 15050 train_loss  0.005260612723293845\n",
      "Epoch 4/ 15060 train_loss  0.0052835393711587195\n",
      "Epoch 4/ 15070 train_loss  0.005919453973762038\n",
      "Epoch 4/ 15080 train_loss  0.006172167083105863\n",
      "Epoch 4/ 15090 train_loss  0.006651305325821225\n",
      "Epoch 4/ 15100 train_loss  0.006838983551321568\n",
      "Epoch 4/ 15110 train_loss  0.006976406096667573\n",
      "Epoch 4/ 15120 train_loss  0.007155190667534207\n",
      "Epoch 4/ 15130 train_loss  0.0073127850032579855\n",
      "Epoch 4/ 15140 train_loss  0.007541953043245138\n",
      "Epoch 4/ 15150 train_loss  0.0077995498081325255\n",
      "Epoch 4/ 15160 train_loss  0.00809358537441926\n",
      "Epoch 4/ 15170 train_loss  0.008363378483479185\n",
      "Epoch 4/ 15180 train_loss  0.008457371834696243\n",
      "Epoch 4/ 15190 train_loss  0.008534008919428805\n",
      "Epoch 4/ 15200 train_loss  0.00880107360452686\n",
      "Epoch 4/ 15210 train_loss  0.008889023809394363\n",
      "Epoch 4/ 15220 train_loss  0.008934149999081575\n",
      "Epoch 4/ 15230 train_loss  0.00898077657467365\n",
      "Epoch 4/ 15240 train_loss  0.009280586241517768\n",
      "Epoch 4/ 15250 train_loss  0.009539894518124915\n",
      "Epoch 4/ 15260 train_loss  0.009575586132451965\n",
      "Epoch 4/ 15270 train_loss  0.009624811815001593\n",
      "Epoch 4/ 15280 train_loss  0.009653107107468271\n",
      "Epoch 4/ 15290 train_loss  0.009899346018959282\n",
      "Epoch 4/ 15300 train_loss  0.009944702259989278\n",
      "Epoch 4/ 15310 train_loss  0.01001999625748794\n",
      "Epoch 4/ 15320 train_loss  0.01017901762573607\n",
      "Epoch 4/ 15330 train_loss  0.010369933056607154\n",
      "Epoch 4/ 15340 train_loss  0.010387713501360139\n",
      "Epoch 4/ 15350 train_loss  0.010498817215520456\n",
      "Epoch 4/ 15360 train_loss  0.010567153415713649\n",
      "Epoch 4/ 15370 train_loss  0.010715553986088013\n",
      "Epoch 4/ 15380 train_loss  0.010784226808786634\n",
      "Epoch 4/ 15390 train_loss  0.01119063597855411\n",
      "Epoch 4/ 15400 train_loss  0.011237381403201421\n",
      "Epoch 4/ 15410 train_loss  0.011277365238779893\n",
      "Epoch 4/ 15420 train_loss  0.011410310347611355\n",
      "Epoch 4/ 15430 train_loss  0.011444249130978338\n",
      "Epoch 4/ 15440 train_loss  0.011472610541487592\n",
      "Epoch 4/ 15450 train_loss  0.01176062596061731\n",
      "Epoch 4/ 15460 train_loss  0.011786451695433017\n",
      "Epoch 4/ 15470 train_loss  0.011825328277238584\n",
      "Epoch 4/ 15480 train_loss  0.011835900212654503\n",
      "Epoch 4/ 15490 train_loss  0.012286287060346733\n",
      "Epoch 4/ 15500 train_loss  0.0124765248117105\n",
      "Epoch 4/ 15510 train_loss  0.012820330279953458\n",
      "Epoch 4/ 15520 train_loss  0.01287264677499857\n",
      "Epoch 4/ 15530 train_loss  0.012919595333457044\n",
      "Epoch 4/ 15540 train_loss  0.013012395475307066\n",
      "Epoch 4/ 15550 train_loss  0.013175419606569018\n",
      "Epoch 4/ 15560 train_loss  0.013201697123529742\n",
      "Epoch 4/ 15570 train_loss  0.013519410819045025\n",
      "Epoch 4/ 15580 train_loss  0.013523194954041447\n",
      "Epoch 4/ 15590 train_loss  0.013769243698947789\n",
      "Epoch 4/ 15600 train_loss  0.013874737760788292\n",
      "Epoch 4/ 15610 train_loss  0.014074925539219314\n",
      "Epoch 4/ 15620 train_loss  0.014163079117930499\n",
      "Epoch 4/ 15630 train_loss  0.014251251687090268\n",
      "Epoch 4/ 15640 train_loss  0.014479150644376205\n",
      "Epoch 4/ 15650 train_loss  0.014514268218163658\n",
      "Epoch 4/ 15660 train_loss  0.01487018909870204\n",
      "Epoch 4/ 15670 train_loss  0.015085024549415224\n",
      "Epoch 4/ 15680 train_loss  0.015184786536283383\n",
      "Epoch 4/ 15690 train_loss  0.015269904401417273\n",
      "Epoch 4/ 15700 train_loss  0.015298840169255446\n",
      "Epoch 4/ 15710 train_loss  0.015445202518960968\n",
      "Epoch 4/ 15720 train_loss  0.015514159571873328\n",
      "Epoch 4/ 15730 train_loss  0.015678630636775055\n",
      "Epoch 4/ 15740 train_loss  0.01581888001009473\n",
      "Epoch 4/ 15750 train_loss  0.01606955735096526\n",
      "Epoch 4/ 15760 train_loss  0.01659441568513147\n",
      "Epoch 4/ 15770 train_loss  0.016882514706883327\n",
      "Epoch 4/ 15780 train_loss  0.0169859224873611\n",
      "Epoch 4/ 15790 train_loss  0.017144568005385878\n",
      "Epoch 4/ 15800 train_loss  0.017271402077728402\n",
      "Epoch 4/ 15810 train_loss  0.01745261599294098\n",
      "Epoch 4/ 15820 train_loss  0.017564792527475993\n",
      "Epoch 4/ 15830 train_loss  0.017776149382737354\n",
      "Epoch 4/ 15840 train_loss  0.017941000378213195\n",
      "Epoch 4/ 15850 train_loss  0.0181945006043165\n",
      "Epoch 4/ 15860 train_loss  0.01823705815931033\n",
      "Epoch 4/ 15870 train_loss  0.018392490994370608\n",
      "Epoch 4/ 15880 train_loss  0.018500649021561733\n",
      "Epoch 4/ 15890 train_loss  0.01859940945135693\n",
      "Epoch 4/ 15900 train_loss  0.018644133721285018\n",
      "Epoch 4/ 15910 train_loss  0.018642930204856883\n",
      "Epoch 4/ 15920 train_loss  0.01870158677881141\n",
      "Epoch 4/ 15930 train_loss  0.018725426494650664\n",
      "Epoch 4/ 15940 train_loss  0.01896453599107745\n",
      "Epoch 4/ 15950 train_loss  0.01910007870954546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/ 15960 train_loss  0.019150287226523927\n",
      "Epoch 4/ 15970 train_loss  0.019256397486569907\n",
      "Epoch 4/ 15980 train_loss  0.019444846251043514\n",
      "Epoch 4/ 15990 train_loss  0.01953378870833723\n",
      "Epoch 4/ 16000 train_loss  0.019717325680424648\n",
      "Epoch 4/ 16010 train_loss  0.019821212091367953\n",
      "Epoch 4/ 16020 train_loss  0.019911400591364183\n",
      "Epoch 4/ 16030 train_loss  0.02016854393192204\n",
      "Epoch 4/ 16040 train_loss  0.020220591860317563\n",
      "Epoch 4/ 16050 train_loss  0.020465331687302954\n",
      "Epoch 4/ 16060 train_loss  0.02061808841896922\n",
      "Epoch 4/ 16070 train_loss  0.020778599587127095\n",
      "Epoch 4/ 16080 train_loss  0.020844449003597252\n",
      "Epoch 4/ 16090 train_loss  0.020981045190842644\n",
      "Epoch 4/ 16100 train_loss  0.021005539579078747\n",
      "Epoch 4/ 16110 train_loss  0.02112245657587952\n",
      "Epoch 4/ 16120 train_loss  0.02139873410662659\n",
      "Epoch 4/ 16130 train_loss  0.021548926418288712\n",
      "Epoch 4/ 16140 train_loss  0.021596000398967752\n",
      "Epoch 4/ 16150 train_loss  0.021697237237899063\n",
      "Epoch 4/ 16160 train_loss  0.02195012259111971\n",
      "Epoch 4/ 16170 train_loss  0.021999809854532114\n",
      "Epoch 4/ 16180 train_loss  0.02202283794985955\n",
      "Epoch 4/ 16190 train_loss  0.022035638201974836\n",
      "Epoch 4/ 16200 train_loss  0.022061079589367826\n",
      "Epoch 4/ 16210 train_loss  0.022093359945280534\n",
      "Epoch 4/ 16220 train_loss  0.02210885668547389\n",
      "Epoch 4/ 16230 train_loss  0.02212110506616713\n",
      "Epoch 4/ 16240 train_loss  0.02224519908290364\n",
      "Epoch 4/ 16250 train_loss  0.02231338517823492\n",
      "Epoch 4/ 16260 train_loss  0.022539373852689653\n",
      "Epoch 4/ 16270 train_loss  0.02258532851564243\n",
      "Epoch 4/ 16280 train_loss  0.02264250409328176\n",
      "Epoch 4/ 16290 train_loss  0.022928157650214137\n",
      "Epoch 4/ 16300 train_loss  0.022962942271712486\n",
      "Epoch 4/ 16310 train_loss  0.022979621733423737\n",
      "Epoch 4/ 16320 train_loss  0.02305739635287296\n",
      "Epoch 4/ 16330 train_loss  0.023069792190973642\n",
      "Epoch 4/ 16340 train_loss  0.023170453373268434\n",
      "Epoch 4/ 16350 train_loss  0.02317543425740921\n",
      "Epoch 4/ 16360 train_loss  0.023519360383160885\n",
      "Epoch 4/ 16370 train_loss  0.02382301364226349\n",
      "Epoch 4/ 16380 train_loss  0.024355136026792058\n",
      "Epoch 4/ 16390 train_loss  0.024631882032935225\n",
      "Epoch 4/ 16400 train_loss  0.024633556251521155\n",
      "Epoch 4/ 16410 train_loss  0.02477872086823012\n",
      "Epoch 4/ 16420 train_loss  0.025223593737582647\n",
      "Epoch 4/ 16430 train_loss  0.0252795562088808\n",
      "Epoch 4/ 16440 train_loss  0.02560169557981371\n",
      "Epoch 4/ 16450 train_loss  0.025677513219580884\n",
      "Epoch 4/ 16460 train_loss  0.025702869305533393\n",
      "Epoch 4/ 16470 train_loss  0.02582455849382346\n",
      "Epoch 4/ 16480 train_loss  0.025872753823991207\n",
      "Epoch 4/ 16490 train_loss  0.02605247280856858\n",
      "Epoch 4/ 16500 train_loss  0.026282689535724216\n",
      "Epoch 4/ 16510 train_loss  0.026537931579077436\n",
      "Epoch 4/ 16520 train_loss  0.0265717905289251\n",
      "Epoch 4/ 16530 train_loss  0.026618086912760505\n",
      "Epoch 4/ 16540 train_loss  0.02665010574771577\n",
      "Epoch 4/ 16550 train_loss  0.0268215978239525\n",
      "Epoch 4/ 16560 train_loss  0.02700614089663221\n",
      "Epoch 4/ 16570 train_loss  0.02716142079000547\n",
      "Epoch 4/ 16580 train_loss  0.027236991508669085\n",
      "Epoch 4/ 16590 train_loss  0.027624838554196084\n",
      "Epoch 4/ 16600 train_loss  0.027634640316623226\n",
      "Epoch 4/ 16610 train_loss  0.027758679925622344\n",
      "Epoch 4/ 16620 train_loss  0.02791511394690918\n",
      "Epoch 4/ 16630 train_loss  0.028033944132482294\n",
      "Epoch 4/ 16640 train_loss  0.028086663399326976\n",
      "Epoch 4/ 16650 train_loss  0.028109671263653335\n",
      "Epoch 4/ 16660 train_loss  0.028140478749683322\n",
      "Epoch 4/ 16670 train_loss  0.02824529597225716\n",
      "Epoch 4/ 16680 train_loss  0.028324821198854377\n",
      "Epoch 4/ 16690 train_loss  0.028374074290904956\n",
      "Epoch 4/ 16700 train_loss  0.028569841658619005\n",
      "Epoch 4/ 16710 train_loss  0.028674395809798893\n",
      "Epoch 4/ 16720 train_loss  0.028812767106835675\n",
      "Epoch 4/ 16730 train_loss  0.029075677428104657\n",
      "Epoch 4/ 16740 train_loss  0.029148518312120054\n",
      "Epoch 4/ 16750 train_loss  0.029358891484339976\n",
      "Epoch 4/ 16760 train_loss  0.029379125440609496\n",
      "Epoch 4/ 16770 train_loss  0.029396456299369232\n",
      "Epoch 4/ 16780 train_loss  0.029492040174591005\n",
      "Epoch 4/ 16790 train_loss  0.029628573122037292\n",
      "Epoch 4/ 16800 train_loss  0.029714266696230947\n",
      "Epoch 4/ 16810 train_loss  0.030125962924557645\n",
      "Epoch 4/ 16820 train_loss  0.030137450365555343\n",
      "Epoch 4/ 16830 train_loss  0.03015805765176916\n",
      "Epoch 4/ 16840 train_loss  0.03030651384981231\n",
      "Epoch 4/ 16850 train_loss  0.030404641818946625\n",
      "Epoch 4/ 16860 train_loss  0.030449047255379568\n",
      "Epoch 4/ 16870 train_loss  0.030494297211423414\n",
      "Epoch 4/ 16880 train_loss  0.030520745191399305\n",
      "Epoch 4/ 16890 train_loss  0.030561466896738172\n",
      "Epoch 4/ 16900 train_loss  0.03057914361834564\n",
      "Epoch 4/ 16910 train_loss  0.03064732401575719\n",
      "Epoch 4/ 16920 train_loss  0.03075111202590553\n",
      "Epoch 4/ 16930 train_loss  0.03077936617283871\n",
      "Epoch 4/ 16940 train_loss  0.03081229730303453\n",
      "Epoch 4/ 16950 train_loss  0.031125047064802378\n",
      "Epoch 4/ 16960 train_loss  0.03129708417076801\n",
      "Epoch 4/ 16970 train_loss  0.03142170712256156\n",
      "Epoch 4/ 16980 train_loss  0.03171173619143009\n",
      "Epoch 4/ 16990 train_loss  0.031932163315088025\n",
      "Epoch 4/ 17000 train_loss  0.03205192196555284\n",
      "Epoch 4/ 17010 train_loss  0.032267504947057025\n",
      "Epoch 4/ 17020 train_loss  0.032412097236468644\n",
      "Epoch 4/ 17030 train_loss  0.03252853270641422\n",
      "Epoch 4/ 17040 train_loss  0.03274730454746646\n",
      "Epoch 4/ 17050 train_loss  0.03292280186341018\n",
      "Epoch 4/ 17060 train_loss  0.03304694812814445\n",
      "Epoch 4/ 17070 train_loss  0.03331556913541\n",
      "Epoch 4/ 17080 train_loss  0.03361507170332427\n",
      "Epoch 4/ 17090 train_loss  0.03371463608543172\n",
      "Epoch 4/ 17100 train_loss  0.0338246407252752\n",
      "Epoch 4/ 17110 train_loss  0.03404198781759887\n",
      "Epoch 4/ 17120 train_loss  0.03426070479229662\n",
      "Epoch 4/ 17130 train_loss  0.034396459376016944\n",
      "Epoch 4/ 17140 train_loss  0.0344710685195995\n",
      "Epoch 4/ 17150 train_loss  0.034639892052898635\n",
      "Epoch 4/ 17160 train_loss  0.03480186960856002\n",
      "Epoch 4/ 17170 train_loss  0.0349449199951233\n",
      "Epoch 4/ 17180 train_loss  0.03513116713643649\n",
      "Epoch 4/ 17190 train_loss  0.03520280747716955\n",
      "Epoch 4/ 17200 train_loss  0.035288866500216086\n",
      "Epoch 4/ 17210 train_loss  0.0354078273339759\n",
      "Epoch 4/ 17220 train_loss  0.0355098858859665\n",
      "Epoch 4/ 17230 train_loss  0.035538142152096414\n",
      "Epoch 4/ 17240 train_loss  0.0357817421216086\n",
      "Epoch 4/ 17250 train_loss  0.0358138894724368\n",
      "Epoch 4/ 17260 train_loss  0.03583622550972961\n",
      "Epoch 4/ 17270 train_loss  0.03586414935879156\n",
      "Epoch 4/ 17280 train_loss  0.03603736669328692\n",
      "Epoch 4/ 17290 train_loss  0.03607285933297345\n",
      "Epoch 4/ 17300 train_loss  0.0360951694376591\n",
      "Epoch 4/ 17310 train_loss  0.03619032035860754\n",
      "Epoch 4/ 17320 train_loss  0.036233757981048303\n",
      "Epoch 4/ 17330 train_loss  0.036245270283905444\n",
      "Epoch 4/ 17340 train_loss  0.03633670904031659\n",
      "Epoch 4/ 17350 train_loss  0.03634330842319744\n",
      "Epoch 4/ 17360 train_loss  0.036333335293537244\n",
      "Epoch 4/ 17370 train_loss  0.03634068094414018\n",
      "Epoch 4/ 17380 train_loss  0.03642656165484116\n",
      "Epoch 4/ 17390 train_loss  0.03642546053528957\n",
      "Epoch 4/ 17400 train_loss  0.0364236110202477\n",
      "Epoch 4/ 17410 train_loss  0.036435402626611754\n",
      "Epoch 4/ 17420 train_loss  0.036507210849325934\n",
      "Epoch 4/ 17430 train_loss  0.03668400685643429\n",
      "Epoch 4/ 17440 train_loss  0.03695416086508713\n",
      "Epoch 4/ 17450 train_loss  0.03694229593043526\n",
      "Epoch 4/ 17460 train_loss  0.03693136053049438\n",
      "Epoch 4/ 17470 train_loss  0.036935574879240825\n",
      "Epoch 4/ 17480 train_loss  0.03710974602343295\n",
      "Epoch 4/ 17490 train_loss  0.037432343098507556\n",
      "Epoch 4/ 17500 train_loss  0.03743589227018964\n",
      "Epoch 4/ 17510 train_loss  0.03745992031165211\n",
      "Epoch 4/ 17520 train_loss  0.037734330532173586\n",
      "Epoch 4/ 17530 train_loss  0.03782082161542918\n",
      "Epoch 4/ 17540 train_loss  0.037919553282679236\n",
      "Epoch 4/ 17550 train_loss  0.03799447219354847\n",
      "Epoch 4/ 17560 train_loss  0.03802087033446407\n",
      "Epoch 4/ 17570 train_loss  0.03804695576411298\n",
      "Epoch 4/ 17580 train_loss  0.038048770407265814\n",
      "Epoch 4/ 17590 train_loss  0.038051475593802536\n",
      "Epoch 4/ 17600 train_loss  0.03825764742624847\n",
      "Epoch 4/ 17610 train_loss  0.03831646195699303\n",
      "Epoch 4/ 17620 train_loss  0.038363171624586224\n",
      "Epoch 4/ 17630 train_loss  0.03842520191053668\n",
      "Epoch 4/ 17640 train_loss  0.038554587105014824\n",
      "Epoch 4/ 17650 train_loss  0.03872611713097233\n",
      "Epoch 4/ 17660 train_loss  0.03879949611846838\n",
      "Epoch 4/ 17670 train_loss  0.03882779235110773\n",
      "Epoch 4/ 17680 train_loss  0.03886068311712424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/ 17690 train_loss  0.038849153750179\n",
      "Epoch 4/ 17700 train_loss  0.03887276160492166\n",
      "Epoch 4/ 17710 train_loss  0.03893338749758632\n",
      "Epoch 4/ 17720 train_loss  0.03912026182310796\n",
      "Epoch 4/ 17730 train_loss  0.03921682987166966\n",
      "Epoch 4/ 17740 train_loss  0.039268770450287406\n",
      "Epoch 4/ 17750 train_loss  0.03937315685247315\n",
      "Epoch 4/ 17760 train_loss  0.03951573155802142\n",
      "Epoch 4/ 17770 train_loss  0.039536723951580026\n",
      "Epoch 4/ 17780 train_loss  0.03962976607854952\n",
      "Epoch 4/ 17790 train_loss  0.039778444401698865\n",
      "Epoch 4/ 17800 train_loss  0.039892620564955024\n",
      "Epoch 4/ 17810 train_loss  0.0399444808353347\n",
      "Epoch 4/ 17820 train_loss  0.039924409118899586\n",
      "Epoch 4/ 17830 train_loss  0.04019487650401573\n",
      "Epoch 4/ 17840 train_loss  0.040272714112961884\n",
      "Epoch 4/ 17850 train_loss  0.040265133801479974\n",
      "Epoch 4/ 17860 train_loss  0.04032443531535859\n",
      "Epoch 4/ 17870 train_loss  0.040403763102665996\n",
      "Epoch 4/ 17880 train_loss  0.04056359659416963\n",
      "Epoch 4/ 17890 train_loss  0.04055965782434115\n",
      "Epoch 4/ 17900 train_loss  0.04056417378148534\n",
      "Epoch 4/ 17910 train_loss  0.04068121033625315\n",
      "Epoch 4/ 17920 train_loss  0.04068122135876786\n",
      "Epoch 4/ 17930 train_loss  0.040703585302180025\n",
      "Epoch 4/ 17940 train_loss  0.04082964456297653\n",
      "Epoch 4/ 17950 train_loss  0.04114207142573413\n",
      "Epoch 4/ 17960 train_loss  0.04114716974856333\n",
      "Epoch 4/ 17970 train_loss  0.04118049152433984\n",
      "Epoch 4/ 17980 train_loss  0.041218743512372744\n",
      "Epoch 4/ 17990 train_loss  0.04141303187015459\n",
      "Epoch 4/ 18000 train_loss  0.041413834847737425\n",
      "Epoch 4/ 18010 train_loss  0.041443606591734246\n",
      "Epoch 4/ 18020 train_loss  0.04149546506704307\n",
      "Epoch 4/ 18030 train_loss  0.041713627979457046\n",
      "Epoch 4/ 18040 train_loss  0.04172607277286645\n",
      "Epoch 4/ 18050 train_loss  0.04173043435574424\n",
      "Epoch 4/ 18060 train_loss  0.041717189982789865\n",
      "Epoch 4/ 18070 train_loss  0.041770405730016974\n",
      "Epoch 4/ 18080 train_loss  0.04187980715793887\n",
      "Epoch 4/ 18090 train_loss  0.04191168323352478\n",
      "Epoch 4/ 18100 train_loss  0.04196078916624464\n",
      "Epoch 4/ 18110 train_loss  0.04194117205378611\n",
      "Epoch 4/ 18120 train_loss  0.04195726770402635\n",
      "Epoch 4/ 18130 train_loss  0.04204473068428309\n",
      "Epoch 4/ 18140 train_loss  0.0420246772175002\n",
      "Epoch 4/ 18150 train_loss  0.04208892986167854\n",
      "Epoch 4/ 18160 train_loss  0.04214567275289261\n",
      "Epoch 4/ 18170 train_loss  0.042430200509512\n",
      "Epoch 4/ 18180 train_loss  0.042630008465566055\n",
      "Epoch 4/ 18190 train_loss  0.04267596455761954\n",
      "Epoch 4/ 18200 train_loss  0.0427080821419726\n",
      "Epoch 4/ 18210 train_loss  0.042713506154335404\n",
      "Epoch 4/ 18220 train_loss  0.042861060195092116\n",
      "Epoch 4/ 18230 train_loss  0.04292976576469821\n",
      "Epoch 4/ 18240 train_loss  0.043247753512712504\n",
      "Epoch 4/ 18250 train_loss  0.04324094420829998\n",
      "Epoch 4/ 18260 train_loss  0.04333165184734523\n",
      "Epoch 4/ 18270 train_loss  0.043700139870647234\n",
      "Epoch 4/ 18280 train_loss  0.04375662450283566\n",
      "Epoch 4/ 18290 train_loss  0.043836116430910524\n",
      "Epoch 4/ 18300 train_loss  0.04393156007055213\n",
      "Epoch 4/ 18310 train_loss  0.04397231985916998\n",
      "Epoch 4/ 18320 train_loss  0.04403491624860469\n",
      "Epoch 4/ 18330 train_loss  0.0441003051126077\n",
      "Epoch 4/ 18340 train_loss  0.04424127935212673\n",
      "Epoch 4/ 18350 train_loss  0.044240243650881396\n",
      "Epoch 4/ 18360 train_loss  0.04438175703802349\n",
      "Epoch 4/ 18370 train_loss  0.04439822395349972\n",
      "Epoch 4/ 18380 train_loss  0.04451922495669494\n",
      "Epoch 4/ 18390 train_loss  0.04457421902772486\n",
      "Epoch 4/ 18400 train_loss  0.04458460111908913\n",
      "Epoch 4/ 18410 train_loss  0.04472355665564869\n",
      "Epoch 4/ 18420 train_loss  0.044765459367209375\n",
      "Epoch 4/ 18430 train_loss  0.04488251322622883\n",
      "Epoch 4/ 18440 train_loss  0.044973096489712296\n",
      "Epoch 4/ 18450 train_loss  0.04499390777082121\n",
      "Epoch 4/ 18460 train_loss  0.04507869579884189\n",
      "Epoch 4/ 18470 train_loss  0.04524513081255839\n",
      "Epoch 4/ 18480 train_loss  0.04550971995349212\n",
      "Epoch 4/ 18490 train_loss  0.04570697336980483\n",
      "Epoch 4/ 18500 train_loss  0.0457678118752067\n",
      "Epoch 4/ 18510 train_loss  0.04591035681267675\n",
      "Train Loss: 0.2294739476750248, Train Acc: 0.9135602377093462\n",
      "Test Loss: 0.46583202584506195, Test Acc: 0.810064935064935\n",
      "Epoch 5/ 18520 train_loss  0.00013414639969918377\n",
      "Epoch 5/ 18530 train_loss  0.0005317818044011894\n",
      "Epoch 5/ 18540 train_loss  0.0007928625208591971\n",
      "Epoch 5/ 18550 train_loss  0.0009128966777760026\n",
      "Epoch 5/ 18560 train_loss  0.0010973674274065619\n",
      "Epoch 5/ 18570 train_loss  0.0011930671307432382\n",
      "Epoch 5/ 18580 train_loss  0.001865264104320491\n",
      "Epoch 5/ 18590 train_loss  0.0020197800103725124\n",
      "Epoch 5/ 18600 train_loss  0.002266359541156281\n",
      "Epoch 5/ 18610 train_loss  0.002339342006712413\n",
      "Epoch 5/ 18620 train_loss  0.002380902664746818\n",
      "Epoch 5/ 18630 train_loss  0.002519839246650765\n",
      "Epoch 5/ 18640 train_loss  0.002727260126233101\n",
      "Epoch 5/ 18650 train_loss  0.002777789125533319\n",
      "Epoch 5/ 18660 train_loss  0.0031946889447776287\n",
      "Epoch 5/ 18670 train_loss  0.0032513917112060834\n",
      "Epoch 5/ 18680 train_loss  0.0033970080212162744\n",
      "Epoch 5/ 18690 train_loss  0.0035162283689053574\n",
      "Epoch 5/ 18700 train_loss  0.003945033577718921\n",
      "Epoch 5/ 18710 train_loss  0.003982231382968388\n",
      "Epoch 5/ 18720 train_loss  0.004204166266518833\n",
      "Epoch 5/ 18730 train_loss  0.004254972879992046\n",
      "Epoch 5/ 18740 train_loss  0.0045384990254673345\n",
      "Epoch 5/ 18750 train_loss  0.00457024877583224\n",
      "Epoch 5/ 18760 train_loss  0.004604581565919853\n",
      "Epoch 5/ 18770 train_loss  0.004979331568089911\n",
      "Epoch 5/ 18780 train_loss  0.005106793340716096\n",
      "Epoch 5/ 18790 train_loss  0.005333247671514945\n",
      "Epoch 5/ 18800 train_loss  0.0054978647269703735\n",
      "Epoch 5/ 18810 train_loss  0.005706000159859803\n",
      "Epoch 5/ 18820 train_loss  0.0058239368893628456\n",
      "Epoch 5/ 18830 train_loss  0.005983448049245524\n",
      "Epoch 5/ 18840 train_loss  0.006189229477793663\n",
      "Epoch 5/ 18850 train_loss  0.006290960487254549\n",
      "Epoch 5/ 18860 train_loss  0.006750449058589471\n",
      "Epoch 5/ 18870 train_loss  0.006970366022375049\n",
      "Epoch 5/ 18880 train_loss  0.007162715828189944\n",
      "Epoch 5/ 18890 train_loss  0.007215962969523676\n",
      "Epoch 5/ 18900 train_loss  0.007270332514184473\n",
      "Epoch 5/ 18910 train_loss  0.007366561171307919\n",
      "Epoch 5/ 18920 train_loss  0.007403477077188376\n",
      "Epoch 5/ 18930 train_loss  0.0074163353838370945\n",
      "Epoch 5/ 18940 train_loss  0.007519428617147479\n",
      "Epoch 5/ 18950 train_loss  0.007742241965657531\n",
      "Epoch 5/ 18960 train_loss  0.00787958754741259\n",
      "Epoch 5/ 18970 train_loss  0.007920849376241103\n",
      "Epoch 5/ 18980 train_loss  0.007946543902221891\n",
      "Epoch 5/ 18990 train_loss  0.007992172604811155\n",
      "Epoch 5/ 19000 train_loss  0.008093724636707225\n",
      "Epoch 5/ 19010 train_loss  0.00819808109597599\n",
      "Epoch 5/ 19020 train_loss  0.008262441299928616\n",
      "Epoch 5/ 19030 train_loss  0.008536523271825004\n",
      "Epoch 5/ 19040 train_loss  0.008544531907817858\n",
      "Epoch 5/ 19050 train_loss  0.008586436557273263\n",
      "Epoch 5/ 19060 train_loss  0.008657968065253058\n",
      "Epoch 5/ 19070 train_loss  0.008747333978111529\n",
      "Epoch 5/ 19080 train_loss  0.00877679055525923\n",
      "Epoch 5/ 19090 train_loss  0.00881029891042727\n",
      "Epoch 5/ 19100 train_loss  0.008827825397813363\n",
      "Epoch 5/ 19110 train_loss  0.008869920348526891\n",
      "Epoch 5/ 19120 train_loss  0.008918957313885834\n",
      "Epoch 5/ 19130 train_loss  0.008947811025673396\n",
      "Epoch 5/ 19140 train_loss  0.009268180574430844\n",
      "Epoch 5/ 19150 train_loss  0.009482468495849291\n",
      "Epoch 5/ 19160 train_loss  0.009539391429719718\n",
      "Epoch 5/ 19170 train_loss  0.009561118697046676\n",
      "Epoch 5/ 19180 train_loss  0.009561416344704652\n",
      "Epoch 5/ 19190 train_loss  0.009577622143205808\n",
      "Epoch 5/ 19200 train_loss  0.009910053684382131\n",
      "Epoch 5/ 19210 train_loss  0.01014032895077796\n",
      "Epoch 5/ 19220 train_loss  0.010176061414321144\n",
      "Epoch 5/ 19230 train_loss  0.010229592961282988\n",
      "Epoch 5/ 19240 train_loss  0.010272765095494593\n",
      "Epoch 5/ 19250 train_loss  0.010502944081640757\n",
      "Epoch 5/ 19260 train_loss  0.010629585128977704\n",
      "Epoch 5/ 19270 train_loss  0.010744347960805596\n",
      "Epoch 5/ 19280 train_loss  0.010753549590448856\n",
      "Epoch 5/ 19290 train_loss  0.010807323665018697\n",
      "Epoch 5/ 19300 train_loss  0.01086132102236976\n",
      "Epoch 5/ 19310 train_loss  0.011021736321508093\n",
      "Epoch 5/ 19320 train_loss  0.011064883691708763\n",
      "Epoch 5/ 19330 train_loss  0.011085450137654418\n",
      "Epoch 5/ 19340 train_loss  0.01120140193614847\n",
      "Epoch 5/ 19350 train_loss  0.011227366632868597\n",
      "Epoch 5/ 19360 train_loss  0.011559103536917503\n",
      "Epoch 5/ 19370 train_loss  0.01158503841690023\n",
      "Epoch 5/ 19380 train_loss  0.011705021449375333\n",
      "Epoch 5/ 19390 train_loss  0.011810335566467718\n",
      "Epoch 5/ 19400 train_loss  0.011848561708670217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/ 19410 train_loss  0.012030769520436128\n",
      "Epoch 5/ 19420 train_loss  0.012073892461153767\n",
      "Epoch 5/ 19430 train_loss  0.01245643868674521\n",
      "Epoch 5/ 19440 train_loss  0.012572005339632321\n",
      "Epoch 5/ 19450 train_loss  0.012683113176809008\n",
      "Epoch 5/ 19460 train_loss  0.012871315747076917\n",
      "Epoch 5/ 19470 train_loss  0.012927968305293854\n",
      "Epoch 5/ 19480 train_loss  0.013012952434480982\n",
      "Epoch 5/ 19490 train_loss  0.013091631387972376\n",
      "Epoch 5/ 19500 train_loss  0.013164059879420998\n",
      "Epoch 5/ 19510 train_loss  0.013218863116653268\n",
      "Epoch 5/ 19520 train_loss  0.013221531256268041\n",
      "Epoch 5/ 19530 train_loss  0.013234728458336037\n",
      "Epoch 5/ 19540 train_loss  0.013476768602923907\n",
      "Epoch 5/ 19550 train_loss  0.013622458083196742\n",
      "Epoch 5/ 19560 train_loss  0.013677572917473853\n",
      "Epoch 5/ 19570 train_loss  0.013908466566700363\n",
      "Epoch 5/ 19580 train_loss  0.013943380416335652\n",
      "Epoch 5/ 19590 train_loss  0.014003609460663882\n",
      "Epoch 5/ 19600 train_loss  0.014001829665405502\n",
      "Epoch 5/ 19610 train_loss  0.014013595247541156\n",
      "Epoch 5/ 19620 train_loss  0.014023451805805595\n",
      "Epoch 5/ 19630 train_loss  0.014073365134641743\n",
      "Epoch 5/ 19640 train_loss  0.01409934715812469\n",
      "Epoch 5/ 19650 train_loss  0.0141105138648961\n",
      "Epoch 5/ 19660 train_loss  0.01412394150949406\n",
      "Epoch 5/ 19670 train_loss  0.014146704695743407\n",
      "Epoch 5/ 19680 train_loss  0.014206566843379929\n",
      "Epoch 5/ 19690 train_loss  0.0143277875447682\n",
      "Epoch 5/ 19700 train_loss  0.014479515330290148\n",
      "Epoch 5/ 19710 train_loss  0.014530413564977427\n",
      "Epoch 5/ 19720 train_loss  0.014624847367534301\n",
      "Epoch 5/ 19730 train_loss  0.014720247027487882\n",
      "Epoch 5/ 19740 train_loss  0.01477487105987894\n",
      "Epoch 5/ 19750 train_loss  0.014782397456775913\n",
      "Epoch 5/ 19760 train_loss  0.014925473036132703\n",
      "Epoch 5/ 19770 train_loss  0.014962726727400677\n",
      "Epoch 5/ 19780 train_loss  0.014986750820904003\n",
      "Epoch 5/ 19790 train_loss  0.015076035191150606\n",
      "Epoch 5/ 19800 train_loss  0.015143948031083493\n",
      "Epoch 5/ 19810 train_loss  0.015272690930493956\n",
      "Epoch 5/ 19820 train_loss  0.015302461261931433\n",
      "Epoch 5/ 19830 train_loss  0.015465968562329534\n",
      "Epoch 5/ 19840 train_loss  0.015474343907935584\n",
      "Epoch 5/ 19850 train_loss  0.01548180398544971\n",
      "Epoch 5/ 19860 train_loss  0.015676056697811216\n",
      "Epoch 5/ 19870 train_loss  0.015718485734760035\n",
      "Epoch 5/ 19880 train_loss  0.015806009477531684\n",
      "Epoch 5/ 19890 train_loss  0.015812494274712277\n",
      "Epoch 5/ 19900 train_loss  0.0158400833387257\n",
      "Epoch 5/ 19910 train_loss  0.015856443335808455\n",
      "Epoch 5/ 19920 train_loss  0.015889148676050867\n",
      "Epoch 5/ 19930 train_loss  0.015914245543508457\n",
      "Epoch 5/ 19940 train_loss  0.01596265659249126\n",
      "Epoch 5/ 19950 train_loss  0.015982689706101456\n",
      "Epoch 5/ 19960 train_loss  0.01598306895516028\n",
      "Epoch 5/ 19970 train_loss  0.015981345743123668\n",
      "Epoch 5/ 19980 train_loss  0.016026165687490452\n",
      "Epoch 5/ 19990 train_loss  0.016031342344348266\n",
      "Epoch 5/ 20000 train_loss  0.01615587176073472\n",
      "Epoch 5/ 20010 train_loss  0.016227032820984328\n",
      "Epoch 5/ 20020 train_loss  0.016277407902922304\n",
      "Epoch 5/ 20030 train_loss  0.016282169585155947\n",
      "Epoch 5/ 20040 train_loss  0.01630486448742601\n",
      "Epoch 5/ 20050 train_loss  0.016299000873798524\n",
      "Epoch 5/ 20060 train_loss  0.016328416996804172\n",
      "Epoch 5/ 20070 train_loss  0.01668712311075336\n",
      "Epoch 5/ 20080 train_loss  0.017123221126614387\n",
      "Epoch 5/ 20090 train_loss  0.017253090004435673\n",
      "Epoch 5/ 20100 train_loss  0.017259353725284834\n",
      "Epoch 5/ 20110 train_loss  0.017283880037603176\n",
      "Epoch 5/ 20120 train_loss  0.017304879422151535\n",
      "Epoch 5/ 20130 train_loss  0.01741371120521644\n",
      "Epoch 5/ 20140 train_loss  0.01751056192144\n",
      "Epoch 5/ 20150 train_loss  0.017570936601192532\n",
      "Epoch 5/ 20160 train_loss  0.01759060050962217\n",
      "Epoch 5/ 20170 train_loss  0.017669709390649393\n",
      "Epoch 5/ 20180 train_loss  0.017672049509613916\n",
      "Epoch 5/ 20190 train_loss  0.017783578489634556\n",
      "Epoch 5/ 20200 train_loss  0.017783578665075173\n",
      "Epoch 5/ 20210 train_loss  0.017932051634340817\n",
      "Epoch 5/ 20220 train_loss  0.018066784016397136\n",
      "Epoch 5/ 20230 train_loss  0.01811747838569225\n",
      "Epoch 5/ 20240 train_loss  0.018114531672501523\n",
      "Epoch 5/ 20250 train_loss  0.018373015288318573\n",
      "Epoch 5/ 20260 train_loss  0.01839691319205\n",
      "Epoch 5/ 20270 train_loss  0.01852411341161194\n",
      "Epoch 5/ 20280 train_loss  0.018575200332890292\n",
      "Epoch 5/ 20290 train_loss  0.01879758046538153\n",
      "Epoch 5/ 20300 train_loss  0.01880314161149605\n",
      "Epoch 5/ 20310 train_loss  0.01883437635323313\n",
      "Epoch 5/ 20320 train_loss  0.019001256347053314\n",
      "Epoch 5/ 20330 train_loss  0.019047621202568057\n",
      "Epoch 5/ 20340 train_loss  0.019058038312058306\n",
      "Epoch 5/ 20350 train_loss  0.019060068336231004\n",
      "Epoch 5/ 20360 train_loss  0.019153000748472353\n",
      "Epoch 5/ 20370 train_loss  0.019275701031183794\n",
      "Epoch 5/ 20380 train_loss  0.019815566620351065\n",
      "Epoch 5/ 20390 train_loss  0.01984343924704248\n",
      "Epoch 5/ 20400 train_loss  0.01984432472460068\n",
      "Epoch 5/ 20410 train_loss  0.020421328657464023\n",
      "Epoch 5/ 20420 train_loss  0.020612087431571603\n",
      "Epoch 5/ 20430 train_loss  0.020671157264370386\n",
      "Epoch 5/ 20440 train_loss  0.020877214180824326\n",
      "Epoch 5/ 20450 train_loss  0.021280481187103885\n",
      "Epoch 5/ 20460 train_loss  0.021332525511253572\n",
      "Epoch 5/ 20470 train_loss  0.021406720619320644\n",
      "Epoch 5/ 20480 train_loss  0.02150146910301349\n",
      "Epoch 5/ 20490 train_loss  0.0216507612853529\n",
      "Epoch 5/ 20500 train_loss  0.021804083427373167\n",
      "Epoch 5/ 20510 train_loss  0.02226770352736102\n",
      "Epoch 5/ 20520 train_loss  0.02228082114860884\n",
      "Epoch 5/ 20530 train_loss  0.02233525657590932\n",
      "Epoch 5/ 20540 train_loss  0.022426536544436704\n",
      "Epoch 5/ 20550 train_loss  0.022490967615101685\n",
      "Epoch 5/ 20560 train_loss  0.022525023295185877\n",
      "Epoch 5/ 20570 train_loss  0.02256895526961352\n",
      "Epoch 5/ 20580 train_loss  0.02258244282311604\n",
      "Epoch 5/ 20590 train_loss  0.02266742932377302\n",
      "Epoch 5/ 20600 train_loss  0.02270511731131993\n",
      "Epoch 5/ 20610 train_loss  0.02277989704787315\n",
      "Epoch 5/ 20620 train_loss  0.022829114057857477\n",
      "Epoch 5/ 20630 train_loss  0.022835744481138624\n",
      "Epoch 5/ 20640 train_loss  0.022862696218025575\n",
      "Epoch 5/ 20650 train_loss  0.022902405820371223\n",
      "Epoch 5/ 20660 train_loss  0.022921855625825726\n",
      "Epoch 5/ 20670 train_loss  0.022936373106452107\n",
      "Epoch 5/ 20680 train_loss  0.023112461787948978\n",
      "Epoch 5/ 20690 train_loss  0.023240509824805126\n",
      "Epoch 5/ 20700 train_loss  0.023259590173651704\n",
      "Epoch 5/ 20710 train_loss  0.023332020369626137\n",
      "Epoch 5/ 20720 train_loss  0.02353557552458401\n",
      "Epoch 5/ 20730 train_loss  0.02354443783437761\n",
      "Epoch 5/ 20740 train_loss  0.023578852272845167\n",
      "Epoch 5/ 20750 train_loss  0.02358438928967699\n",
      "Epoch 5/ 20760 train_loss  0.02359835886970445\n",
      "Epoch 5/ 20770 train_loss  0.023653632461418744\n",
      "Epoch 5/ 20780 train_loss  0.023769531093772275\n",
      "Epoch 5/ 20790 train_loss  0.024011785101496975\n",
      "Epoch 5/ 20800 train_loss  0.024093432066438294\n",
      "Epoch 5/ 20810 train_loss  0.02436731167220197\n",
      "Epoch 5/ 20820 train_loss  0.0244194539686787\n",
      "Epoch 5/ 20830 train_loss  0.024657943349680845\n",
      "Epoch 5/ 20840 train_loss  0.024684793754843247\n",
      "Epoch 5/ 20850 train_loss  0.02479846466295827\n",
      "Epoch 5/ 20860 train_loss  0.024989788385501262\n",
      "Epoch 5/ 20870 train_loss  0.025092780051484544\n",
      "Epoch 5/ 20880 train_loss  0.02514503122090702\n",
      "Epoch 5/ 20890 train_loss  0.02516188391485801\n",
      "Epoch 5/ 20900 train_loss  0.025224782862461322\n",
      "Epoch 5/ 20910 train_loss  0.025341766082720114\n",
      "Epoch 5/ 20920 train_loss  0.02535867623285296\n",
      "Epoch 5/ 20930 train_loss  0.025360567058225862\n",
      "Epoch 5/ 20940 train_loss  0.025467272808878696\n",
      "Epoch 5/ 20950 train_loss  0.02549861638403756\n",
      "Epoch 5/ 20960 train_loss  0.02556116569083888\n",
      "Epoch 5/ 20970 train_loss  0.025626354713133008\n",
      "Epoch 5/ 20980 train_loss  0.025707769679853585\n",
      "Epoch 5/ 20990 train_loss  0.02587732753445466\n",
      "Epoch 5/ 21000 train_loss  0.025885839823026446\n",
      "Epoch 5/ 21010 train_loss  0.02599544444318183\n",
      "Epoch 5/ 21020 train_loss  0.026019690915347192\n",
      "Epoch 5/ 21030 train_loss  0.02618142558164937\n",
      "Epoch 5/ 21040 train_loss  0.026298932210909582\n",
      "Epoch 5/ 21050 train_loss  0.026347839033555812\n",
      "Epoch 5/ 21060 train_loss  0.026492042906387205\n",
      "Epoch 5/ 21070 train_loss  0.026525065768451435\n",
      "Epoch 5/ 21080 train_loss  0.02655171756297204\n",
      "Epoch 5/ 21090 train_loss  0.02657969744020014\n",
      "Epoch 5/ 21100 train_loss  0.026595473097093463\n",
      "Epoch 5/ 21110 train_loss  0.026590249345402613\n",
      "Epoch 5/ 21120 train_loss  0.026699982374561605\n",
      "Epoch 5/ 21130 train_loss  0.026789117674807156\n",
      "Epoch 5/ 21140 train_loss  0.026958396601398343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/ 21150 train_loss  0.026952668919406977\n",
      "Epoch 5/ 21160 train_loss  0.026955096579077197\n",
      "Epoch 5/ 21170 train_loss  0.026951480210698136\n",
      "Epoch 5/ 21180 train_loss  0.027096040014387625\n",
      "Epoch 5/ 21190 train_loss  0.02714825968048155\n",
      "Epoch 5/ 21200 train_loss  0.02718438727416576\n",
      "Epoch 5/ 21210 train_loss  0.027223588181255545\n",
      "Epoch 5/ 21220 train_loss  0.027473845360016045\n",
      "Epoch 5/ 21230 train_loss  0.027527400536998106\n",
      "Epoch 5/ 21240 train_loss  0.02765974189615834\n",
      "Epoch 5/ 21250 train_loss  0.02769242186367923\n",
      "Epoch 5/ 21260 train_loss  0.02773223536835386\n",
      "Epoch 5/ 21270 train_loss  0.027918230913069966\n",
      "Epoch 5/ 21280 train_loss  0.027918147983501562\n",
      "Epoch 5/ 21290 train_loss  0.02799099098936412\n",
      "Epoch 5/ 21300 train_loss  0.028062368663053165\n",
      "Epoch 5/ 21310 train_loss  0.028065275783886554\n",
      "Epoch 5/ 21320 train_loss  0.028084596590129882\n",
      "Epoch 5/ 21330 train_loss  0.02809923195797922\n",
      "Epoch 5/ 21340 train_loss  0.0281025969931921\n",
      "Epoch 5/ 21350 train_loss  0.028111687007439806\n",
      "Epoch 5/ 21360 train_loss  0.028276645678641233\n",
      "Epoch 5/ 21370 train_loss  0.028286978501102712\n",
      "Epoch 5/ 21380 train_loss  0.02833105021637851\n",
      "Epoch 5/ 21390 train_loss  0.028434710075894983\n",
      "Epoch 5/ 21400 train_loss  0.02861422981853444\n",
      "Epoch 5/ 21410 train_loss  0.028612818926914904\n",
      "Epoch 5/ 21420 train_loss  0.028607744333025983\n",
      "Epoch 5/ 21430 train_loss  0.02870038077780324\n",
      "Epoch 5/ 21440 train_loss  0.028751172978445195\n",
      "Epoch 5/ 21450 train_loss  0.02883602103809569\n",
      "Epoch 5/ 21460 train_loss  0.0289267725321564\n",
      "Epoch 5/ 21470 train_loss  0.029069977539929657\n",
      "Epoch 5/ 21480 train_loss  0.0292345420986857\n",
      "Epoch 5/ 21490 train_loss  0.029325765463789587\n",
      "Epoch 5/ 21500 train_loss  0.029410575980045435\n",
      "Epoch 5/ 21510 train_loss  0.029423764661169916\n",
      "Epoch 5/ 21520 train_loss  0.02948823012422599\n",
      "Epoch 5/ 21530 train_loss  0.02955271410128367\n",
      "Epoch 5/ 21540 train_loss  0.029750243520964902\n",
      "Epoch 5/ 21550 train_loss  0.0297493407409184\n",
      "Epoch 5/ 21560 train_loss  0.029773816309957613\n",
      "Epoch 5/ 21570 train_loss  0.029795595522790118\n",
      "Epoch 5/ 21580 train_loss  0.029814821854572078\n",
      "Epoch 5/ 21590 train_loss  0.029849743557155212\n",
      "Epoch 5/ 21600 train_loss  0.030034998917675925\n",
      "Epoch 5/ 21610 train_loss  0.03004272921378814\n",
      "Epoch 5/ 21620 train_loss  0.030078450185288187\n",
      "Epoch 5/ 21630 train_loss  0.03016785242789002\n",
      "Epoch 5/ 21640 train_loss  0.030202573380139822\n",
      "Epoch 5/ 21650 train_loss  0.0303887778435126\n",
      "Epoch 5/ 21660 train_loss  0.0303960928929669\n",
      "Epoch 5/ 21670 train_loss  0.030422049618650058\n",
      "Epoch 5/ 21680 train_loss  0.030504869765440413\n",
      "Epoch 5/ 21690 train_loss  0.03051602993299185\n",
      "Epoch 5/ 21700 train_loss  0.030651198912801943\n",
      "Epoch 5/ 21710 train_loss  0.03069416813909025\n",
      "Epoch 5/ 21720 train_loss  0.030722198084275048\n",
      "Epoch 5/ 21730 train_loss  0.030900322686763175\n",
      "Epoch 5/ 21740 train_loss  0.030898842958294814\n",
      "Epoch 5/ 21750 train_loss  0.03095199973620581\n",
      "Epoch 5/ 21760 train_loss  0.0309582634953873\n",
      "Epoch 5/ 21770 train_loss  0.030962003932897764\n",
      "Epoch 5/ 21780 train_loss  0.03105268184059304\n",
      "Epoch 5/ 21790 train_loss  0.031048801205258155\n",
      "Epoch 5/ 21800 train_loss  0.03104221011633019\n",
      "Epoch 5/ 21810 train_loss  0.031062009768403002\n",
      "Epoch 5/ 21820 train_loss  0.031082322301975034\n",
      "Epoch 5/ 21830 train_loss  0.03119837497993592\n",
      "Epoch 5/ 21840 train_loss  0.031185763136756618\n",
      "Epoch 5/ 21850 train_loss  0.031226947160515252\n",
      "Epoch 5/ 21860 train_loss  0.03138572388007274\n",
      "Epoch 5/ 21870 train_loss  0.03142239462871412\n",
      "Epoch 5/ 21880 train_loss  0.03167150182436132\n",
      "Epoch 5/ 21890 train_loss  0.03167939829126362\n",
      "Epoch 5/ 21900 train_loss  0.03177973077761776\n",
      "Epoch 5/ 21910 train_loss  0.03177039974372027\n",
      "Epoch 5/ 21920 train_loss  0.031822734778723105\n",
      "Epoch 5/ 21930 train_loss  0.031991172319255565\n",
      "Epoch 5/ 21940 train_loss  0.03209078560510346\n",
      "Epoch 5/ 21950 train_loss  0.032087982455575696\n",
      "Epoch 5/ 21960 train_loss  0.032135730698018956\n",
      "Epoch 5/ 21970 train_loss  0.03232881472642688\n",
      "Epoch 5/ 21980 train_loss  0.032626526719918635\n",
      "Epoch 5/ 21990 train_loss  0.032674055618869685\n",
      "Epoch 5/ 22000 train_loss  0.032830000898064836\n",
      "Epoch 5/ 22010 train_loss  0.033205070551832964\n",
      "Epoch 5/ 22020 train_loss  0.033396654234215645\n",
      "Epoch 5/ 22030 train_loss  0.03349129531128069\n",
      "Epoch 5/ 22040 train_loss  0.033646908333175005\n",
      "Epoch 5/ 22050 train_loss  0.03363933658506376\n",
      "Epoch 5/ 22060 train_loss  0.03373388030439808\n",
      "Epoch 5/ 22070 train_loss  0.03378231921601394\n",
      "Epoch 5/ 22080 train_loss  0.03382128722149994\n",
      "Epoch 5/ 22090 train_loss  0.03384303939847312\n",
      "Epoch 5/ 22100 train_loss  0.033905244058041795\n",
      "Epoch 5/ 22110 train_loss  0.033931029588994764\n",
      "Epoch 5/ 22120 train_loss  0.034158330637054896\n",
      "Epoch 5/ 22130 train_loss  0.03426247599886962\n",
      "Epoch 5/ 22140 train_loss  0.034314885337561996\n",
      "Epoch 5/ 22150 train_loss  0.034382317768160996\n",
      "Epoch 5/ 22160 train_loss  0.03451981894267748\n",
      "Epoch 5/ 22170 train_loss  0.034537374755744095\n",
      "Epoch 5/ 22180 train_loss  0.03461377860088566\n",
      "Epoch 5/ 22190 train_loss  0.034678566266374417\n",
      "Epoch 5/ 22200 train_loss  0.034851742887495006\n",
      "Epoch 5/ 22210 train_loss  0.034967049253811215\n",
      "Train Loss: 0.20988265035419607, Train Acc: 0.92247433819557\n",
      "Test Loss: 0.5153093867717395, Test Acc: 0.7954545454545454\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(embedding_size, hidden_size, num_layers, num_directions, num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.BCELoss()\n",
    "model, total_train_loss, mean_loss = train(model, train_set, test_set, optimizer, loss_func, epochs, model_save_path='model/word_sense_disambiguation.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "苹果，又称柰或林檎，是苹果树（学名：Malus domestica）的果实。苹果树是蔷薇科苹果亚科苹果属植物，为落叶乔木，果实富含矿物质和维生素，是人们最常食用的水果之一。\n"
     ]
    }
   ],
   "source": [
    "origin = '苹果12马上就要发布了。'\n",
    "entity_descs = ['《苹果》是由李玉执导，范冰冰、佟大为、梁家辉、金燕玲领衔主演的黑色幽默剧情电影。于2007年5月18日在中国大陆上映。影片讲述了两个贫富家庭之间离奇诡异的矛盾冲突和感情错位的戏剧性故事。'\n",
    ", 'iPhone 是美国苹果公司研发的iPhone手机，采用了直面边框设计，支持5G，搭载A14 Bionic芯片，双镜头后置摄像头系统。支持北斗导航 [1]  ，有黑色、白色、红色、绿色、蓝色五种配色。'\n",
    "]\n",
    "\n",
    "origin = '我们家的苹果快要熟了，你要不要来几斤？'\n",
    "entity_descs = ['《苹果》是由李玉执导，范冰冰、佟大为、梁家辉、金燕玲领衔主演的黑色幽默剧情电影。于2007年5月18日在中国大陆上映。影片讲述了两个贫富家庭之间离奇诡异的矛盾冲突和感情错位的戏剧性故事。'\n",
    ", 'iPhone 是美国苹果公司研发的iPhone手机，采用了直面边框设计，支持5G，搭载A14 Bionic芯片，双镜头后置摄像头系统。支持北斗导航 [1]  ，有黑色、白色、红色、绿色、蓝色五种配色。',\n",
    " '苹果，又称柰或林檎，是苹果 树（学名：Malus domestica）的果实。苹果树是蔷薇科苹果亚科苹果属植物，为落叶乔木，果实富含矿物质和维生素，是人们最常食用的水果之一。'                    \n",
    "]\n",
    "right_text = predict(model, origin, entity_descs)\n",
    "print(right_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
