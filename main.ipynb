{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思路\n",
    "\n",
    "- 将同一篇文章split为2段，相似度为1，不同的文章的相似度为0\n",
    "- jieba分词，将用词向量模型，得到每个词的词向量。\n",
    "- 将词向量输入到rnn，训练相似度模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、数据处理\n",
    "- 数据地址：链接:https://pan.baidu.com/s/1kQsaRgpEAMDib1LRa2QHqw  密码:bq52\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、处理数据\n",
    "import os\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import jieba\n",
    "\n",
    "\n",
    "data_dict = defaultdict(list)\n",
    "single_data_dict = defaultdict(list)\n",
    "path = 'data/train'\n",
    "root_dirs = os.listdir(path)\n",
    "\n",
    "\n",
    "for dir_ in root_dirs:\n",
    "    fir_dir = os.path.join(path, dir_)\n",
    "    for root, sec_dir, files in os.walk(fir_dir):\n",
    "        for file in files:\n",
    "            file_abs_path = os.path.join(root, file)\n",
    "            try:\n",
    "                data = open(file_abs_path, 'r', encoding='gbk')\n",
    "                for line in data:\n",
    "                    line = line.replace('$LOTOzf$', '')\n",
    "                    if len(line) < 80:\n",
    "                        single_data_dict[dir_].append(line)\n",
    "                        continue\n",
    "                    fir_sent = line[:len(line) // 2]\n",
    "                    sec_sent = line[len(line) // 2:]\n",
    "                    data_dict[dir_].append(copy.deepcopy([fir_sent, sec_sent]))\n",
    "            except:\n",
    "                print(file_abs_path)\n",
    "                continue          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "    \n",
    "classify_dict = ['文学', '体育', '女性', '校园']\n",
    "writer = open('data/semantic_disambiguation_lstm.txt', 'w')\n",
    "temp_dict = {}\n",
    "\n",
    "for k, v in data_dict.items():\n",
    "    temp = ['文学', '体育', '女性', '校园']\n",
    "    cur_classify = classify_dict.index(k)\n",
    "    temp.pop(cur_classify)\n",
    "    other_data = [d for i, data in data_dict.items() if i != k for da in data for d in da]\n",
    "    for sents in v:\n",
    "        fir_sent = sents[0]\n",
    "        sec_sent = sents[1]\n",
    "        single_data_dict[k].extend([fir_sent, sec_sent])\n",
    "        random_index = random.randint(0, len(single_data_dict[k])-1)\n",
    "        \n",
    "        random_index2 = random.randint(0,len(other_data)-1)\n",
    "        neg_fir_sent = single_data_dict[k].pop(random_index)\n",
    "        neg_sec_sent = other_data[random_index2]\n",
    "        \n",
    "        fir_sent = cut_words(fir_sent)\n",
    "        sec_sent = cut_words(sec_sent)\n",
    "        neg_fir_sent = cut_words(neg_fir_sent)\n",
    "        neg_sec_sent = cut_words(neg_sec_sent)\n",
    "        \n",
    "        writer_str = f'{k}\\t{fir_sent}\\t{sec_sent}\\t{1}\\n'\n",
    "        writer.write(writer_str)\n",
    "        writer_str = f'{k}\\t{neg_fir_sent}\\t{neg_sec_sent}\\t{0}\\n'\n",
    "        writer.write(writer_str)\n",
    "\n",
    "writer.close()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文词向量模型地址：\n",
    "- 链接:https://pan.baidu.com/s/1vYXKfkNjOSIhUICa_mgl6A  密码:lv08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nas/lishengping/jupyter/external_project/chinese_disambiguation'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2、处理词向量,作词向量字典\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "\n",
    "vec_path = 'word_vector/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5'\n",
    "vector_dict = {}\n",
    "with open(vec_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        line = line.split()\n",
    "        vector_dict[line[0]] = np.array(line[1:])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_set = open('data/semantic_disambiguation.txt', 'r').readlines()\n",
    "random.shuffle(data_set)\n",
    "\n",
    "data_array = []\n",
    "for d in data_set:\n",
    "    try:\n",
    "        temp_data = d.split('\\t')\n",
    "        a = eval(temp_data[1])\n",
    "        b = eval(temp_data[2])\n",
    "        lable = int(temp_data[-1].strip('\\n'))\n",
    "        vec_a = np.concatenate([vector_dict.get(word).reshape(300, -1).astype(float) for word in a if vector_dict.get(word) is not None], axis=1)\n",
    "        vec_b = np.concatenate([vector_dict.get(word).reshape(300, -1).astype(float) for word in b if vector_dict.get(word) is not None], axis=1)\n",
    "    except:\n",
    "        continue\n",
    "    data_array.append([vec_a, vec_b, lable])\n",
    "\n",
    "test_set = data_array[:len(data_array) // 7]\n",
    "train_set = data_array[len(data_array) // 7:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123) #保证每次运行初始化的随机数相同\n",
    "\n",
    "# vocab_size = 5000   #词表大小\n",
    "embedding_size = 300   #词向量维度\n",
    "num_classes = 2    #二分类\n",
    "# sentence_max_len = 64  #单个句子的长度\n",
    "hidden_size = 256\n",
    "\n",
    "num_layers = 1  #一层lstm\n",
    "num_directions = 2  #双向lstm\n",
    "lr = 1e-3\n",
    "batch_size = 1  \n",
    "epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Bi-LSTM模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embedding_size,hidden_size, num_layers, num_directions, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.input_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = (num_directions == 2))\n",
    "        self.attention_weights_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.liner = nn.Linear(hidden_size, num_classes)\n",
    "        self.act_func = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #lstm的输入维度为 [seq_len, batch, input_size]\n",
    "        #x [batch_size, sentence_length, embedding_size]\n",
    "        x = x.permute(1, 0, 2)         #[sentence_length, batch_size, embedding_size]\n",
    "        \n",
    "        #由于数据集不一定是预先设置的batch_size的整数倍，所以用size(1)获取当前数据实际的batch\n",
    "        batch_size = x.size(1)\n",
    "        \n",
    "        #设置lstm最初的前项输出\n",
    "        h_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        #out[seq_len, batch, num_directions * hidden_size]。多层lstm，out只保存最后一层每个时间步t的输出h_t\n",
    "        #h_n, c_n [num_layers * num_directions, batch, hidden_size]\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        #将双向lstm的输出拆分为前向输出和后向输出\n",
    "        (forward_out, backward_out) = torch.chunk(out, 2, dim = 2)\n",
    "        out = forward_out + backward_out  #[seq_len, batch, hidden_size]\n",
    "        out = out.permute(1, 0, 2)  #[batch, seq_len, hidden_size]\n",
    "        \n",
    "        #为了使用到lstm最后一个时间步时，每层lstm的表达，用h_n生成attention的权重\n",
    "        h_n = h_n.permute(1, 0, 2)  #[batch, num_layers * num_directions,  hidden_size]\n",
    "        h_n = torch.sum(h_n, dim=1) #[batch, 1,  hidden_size]\n",
    "        h_n = h_n.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        \n",
    "        attention_w = self.attention_weights_layer(h_n)  #[batch, hidden_size]\n",
    "        attention_w = attention_w.unsqueeze(dim=1) #[batch, 1, hidden_size]\n",
    "        # [batch, 1, hidden_size]  * [batch, seq_len, hidden_size]\n",
    "        attention_context = torch.bmm(attention_w, out.transpose(1, 2))  \n",
    "        softmax_w = F.softmax(attention_context, dim=-1)  #[batch, 1, seq_len],权重归一化\n",
    "        x = torch.bmm(softmax_w, out)  #[batch, 1, hidden_size]\n",
    "        x = x.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        x = self.liner(x)\n",
    "        x = self.act_func(x)\n",
    "        return x\n",
    "     \n",
    "        \n",
    "def test(model, test_loader, loss_func):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    corrects = 0.0\n",
    "    for datas in test_loader:\n",
    "        inputs_0 = torch.from_numpy(datas[0]).float()\n",
    "        inputs_1 = torch.from_numpy(datas[1]).float()\n",
    "        if datas[2] == 1:\n",
    "            labels = torch.LongTensor([0, 1]).unsqueeze(0).float()\n",
    "        else:\n",
    "            labels = torch.LongTensor([1, 0]).unsqueeze(0).float()\n",
    "        inputs = torch.cat([inputs_0, inputs_1], dim=1).permute(1, 0).unsqueeze(0) # len x 300\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        preds = model(inputs)\n",
    "        loss = loss_func(preds, labels)\n",
    "        \n",
    "        loss_val += loss.item() * inputs.size(0)\n",
    "        \n",
    "        #获取预测的最大概率出现的位置\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        corrects += torch.sum(preds == labels).item()\n",
    "    test_loss = loss_val / len(test_loader)\n",
    "    test_acc = corrects / len(test_loader)\n",
    "    print(\"Test Loss: {}, Test Acc: {}\".format(test_loss, test_acc))\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "def train(model, train_loader,test_loader, optimizer, loss_func, epochs, model_save_path):\n",
    "    best_val_acc = 0.0\n",
    "    best_model_params = copy.deepcopy(model.state_dict())\n",
    "    train_loss = 0\n",
    "    step = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_val = 0.0\n",
    "        corrects = 0.0\n",
    "        for datas in train_loader:\n",
    "            inputs_0 = torch.from_numpy(datas[0]).float()\n",
    "            inputs_1 = torch.from_numpy(datas[1]).float()\n",
    "            if datas[2] == 1:\n",
    "                labels = torch.LongTensor([0, 1]).unsqueeze(0).float()\n",
    "            else:\n",
    "                labels = torch.LongTensor([1, 0]).unsqueeze(0).float()\n",
    "            inputs = torch.cat([inputs_0, inputs_1], dim=1).permute(1, 0).unsqueeze(0) # len x 300\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            preds = model(inputs)\n",
    "            loss = loss_func(preds, labels)\n",
    "            step += 1\n",
    "            train_loss += loss.mean().item()\n",
    "            if step % 100 == 0:\n",
    "                print(f'Epoch {epoch}/ {step} train_loss  {train_loss / step}')\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_val += loss.item() * inputs.size(0)\n",
    "            \n",
    "            #获取预测的最大概率出现的位置\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            corrects += torch.sum(preds == labels).item()\n",
    "        train_loss = loss_val / len(train_loader)\n",
    "        train_acc = corrects / len(train_loader)\n",
    "        if(epoch % 1 == 0):\n",
    "            print(\"Train Loss: {}, Train Acc: {}\".format(train_loss, train_acc))\n",
    "            test_acc = test(model, test_loader, loss_func)\n",
    "            if(best_val_acc < test_acc):\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f'保存模型成功! 路径为: ’{model_save_path}‘')\n",
    "                best_val_acc = test_acc\n",
    "                best_model_params = copy.deepcopy(model.state_dict())\n",
    "    model.load_state_dict(best_model_params)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, origin, entity_descs:list):\n",
    "    \"\"\"\n",
    "    origin: 原始文本\n",
    "    entity_descs:原始文本中具有歧义的实体词的多个异项的相关描述放到entity_descs中。\n",
    "    比如：origin：苹果12马上就要发布了。\n",
    "    歧义实体为”苹果“\n",
    "    entity_descs：\n",
    "    ['《苹果》是由李玉执导，范冰冰、佟大为、梁家辉、金燕玲领衔主演的黑色幽默剧情电影。于2007年5月18日在中国大陆上映。影片讲述了两个贫富家庭之间离奇诡异的矛盾冲突和感情错位的戏剧性故事。'\n",
    ", 'iPhone 12是美国苹果公司研发的iPhone手机，采用了直面边框设计，支持5G，搭载A14 Bionic芯片，双镜头后置摄像头系统。支持北斗导航 [1]  ，有黑色、白色、红色、绿色、蓝色五种配色。'\n",
    "]\n",
    "    entity_descs中的句子分别和origin做相似度得分，得分高的为正确意项\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for entity_desc in entity_descs:\n",
    "        origin_vector = np.concatenate([vector_dict.get(word).reshape(300, -1).astype(float) for word in origin if vector_dict.get(word) is not None], axis=1)\n",
    "        origin_vector = torch.from_numpy(origin_vector).float()\n",
    "        entity_desc_vector = np.concatenate([vector_dict.get(word).reshape(300, -1).astype(float) for word in entity_desc if vector_dict.get(word) is not None], axis=1)\n",
    "        entity_desc_vector = torch.from_numpy(entity_desc_vector).float()\n",
    "        inputs = torch.cat([origin_vector, entity_desc_vector], dim=1).permute(1, 0).unsqueeze(0) # bsz x len x 300\n",
    "        inputs = inputs.to(device)\n",
    "        preds = model(inputs)\n",
    "        score = preds.view(-1).tolist()[1]\n",
    "#         print(preds)\n",
    "        scores.append(score)\n",
    "    scores = torch.tensor(scores)\n",
    "    index = scores.argmax()\n",
    "    return entity_descs[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/ 100 train_loss  0.7193091696500779\n",
      "Epoch 0/ 200 train_loss  0.7106882888078689\n",
      "Epoch 0/ 300 train_loss  0.7071341427167257\n",
      "Epoch 0/ 400 train_loss  0.7051481160521508\n",
      "Epoch 0/ 500 train_loss  0.7118875554800034\n",
      "Epoch 0/ 600 train_loss  0.7077106264233589\n",
      "Epoch 0/ 700 train_loss  0.7093562570214271\n",
      "Epoch 0/ 800 train_loss  0.7103243914991617\n",
      "Epoch 0/ 900 train_loss  0.7094369553526243\n",
      "Epoch 0/ 1000 train_loss  0.7111975549161434\n",
      "Epoch 0/ 1100 train_loss  0.7087956895069643\n",
      "Epoch 0/ 1200 train_loss  0.7094580914825201\n",
      "Epoch 0/ 1300 train_loss  0.7089266705283752\n",
      "Epoch 0/ 1400 train_loss  0.7097474454769066\n",
      "Epoch 0/ 1500 train_loss  0.7078287959297498\n",
      "Epoch 0/ 1600 train_loss  0.706283722082153\n",
      "Epoch 0/ 1700 train_loss  0.7057243898598586\n",
      "Epoch 0/ 1800 train_loss  0.7062952952997552\n",
      "Epoch 0/ 1900 train_loss  0.7058840322886643\n",
      "Epoch 0/ 2000 train_loss  0.7063149392530322\n",
      "Epoch 0/ 2100 train_loss  0.7053475776669524\n",
      "Epoch 0/ 2200 train_loss  0.7068743213672529\n",
      "Epoch 0/ 2300 train_loss  0.7078810142952463\n",
      "Epoch 0/ 2400 train_loss  0.7070111837734778\n",
      "Epoch 0/ 2500 train_loss  0.7075926798820495\n",
      "Epoch 0/ 2600 train_loss  0.7075074561971885\n",
      "Epoch 0/ 2700 train_loss  0.7069685310014971\n",
      "Epoch 0/ 2800 train_loss  0.7068585736517395\n",
      "Epoch 0/ 2900 train_loss  0.706862298291305\n",
      "Epoch 0/ 3000 train_loss  0.7065582552154859\n",
      "Epoch 0/ 3100 train_loss  0.7060077990831868\n",
      "Epoch 0/ 3200 train_loss  0.7053818361274898\n",
      "Epoch 0/ 3300 train_loss  0.7050044689575831\n",
      "Epoch 0/ 3400 train_loss  0.7047917984776637\n",
      "Epoch 0/ 3500 train_loss  0.7048975209678923\n",
      "Epoch 0/ 3600 train_loss  0.7047732445763217\n",
      "Train Loss: 0.7046338527242568, Train Acc: 0.5079751284130846\n",
      "Test Loss: 0.6942241197669661, Test Acc: 0.5211038961038961\n",
      "保存模型成功! 路径为: ’./word_sense_disambiguation.bin‘\n",
      "Epoch 1/ 3700 train_loss  0.00033654305335745693\n",
      "Epoch 1/ 3800 train_loss  0.01896564923380656\n",
      "Epoch 1/ 3900 train_loss  0.03636555423871168\n",
      "Epoch 1/ 4000 train_loss  0.052309349892259646\n",
      "Epoch 1/ 4100 train_loss  0.06797039613997162\n",
      "Epoch 1/ 4200 train_loss  0.08254502342354779\n",
      "Epoch 1/ 4300 train_loss  0.09658827881962848\n",
      "Epoch 1/ 4400 train_loss  0.10854959280108954\n",
      "Epoch 1/ 4500 train_loss  0.12247725457082945\n",
      "Epoch 1/ 4600 train_loss  0.1347060865451858\n",
      "Epoch 1/ 4700 train_loss  0.14638199878148664\n",
      "Epoch 1/ 4800 train_loss  0.15850388926730885\n",
      "Epoch 1/ 4900 train_loss  0.16916419567469515\n",
      "Epoch 1/ 5000 train_loss  0.1795805702265867\n",
      "Epoch 1/ 5100 train_loss  0.18965123127457054\n",
      "Epoch 1/ 5200 train_loss  0.19869366934635396\n",
      "Epoch 1/ 5300 train_loss  0.20813866018375213\n",
      "Epoch 1/ 5400 train_loss  0.21739526671638168\n",
      "Epoch 1/ 5500 train_loss  0.225457395956983\n",
      "Epoch 1/ 5600 train_loss  0.23401317883520326\n",
      "Epoch 1/ 5700 train_loss  0.24186322458975218\n",
      "Epoch 1/ 5800 train_loss  0.24912628181379487\n",
      "Epoch 1/ 5900 train_loss  0.25691303456080233\n",
      "Epoch 1/ 6000 train_loss  0.2635321875994212\n",
      "Epoch 1/ 6100 train_loss  0.27056168138465986\n",
      "Epoch 1/ 6200 train_loss  0.2777665906324797\n",
      "Epoch 1/ 6300 train_loss  0.2838565192972201\n",
      "Epoch 1/ 6400 train_loss  0.29036180739731593\n",
      "Epoch 1/ 6500 train_loss  0.2957974390614314\n",
      "Epoch 1/ 6600 train_loss  0.30192968303031154\n",
      "Epoch 1/ 6700 train_loss  0.30806025416524613\n",
      "Epoch 1/ 6800 train_loss  0.3134284270070324\n",
      "Epoch 1/ 6900 train_loss  0.3188349240411284\n",
      "Epoch 1/ 7000 train_loss  0.32388838750101046\n",
      "Epoch 1/ 7100 train_loss  0.3290565357491422\n",
      "Epoch 1/ 7200 train_loss  0.3340678453112068\n",
      "Epoch 1/ 7300 train_loss  0.3381540311135908\n",
      "Train Loss: 0.6864899120002251, Train Acc: 0.5636658556366586\n",
      "Test Loss: 0.6879995149112754, Test Acc: 0.5665584415584416\n",
      "保存模型成功! 路径为: ’./word_sense_disambiguation.bin‘\n",
      "Epoch 2/ 7400 train_loss  0.000264194305969644\n",
      "Epoch 2/ 7500 train_loss  0.009267868161483199\n",
      "Epoch 2/ 7600 train_loss  0.018124341549335725\n",
      "Epoch 2/ 7700 train_loss  0.02649799603947919\n",
      "Epoch 2/ 7800 train_loss  0.035029918773322906\n",
      "Epoch 2/ 7900 train_loss  0.043442933693607015\n",
      "Epoch 2/ 8000 train_loss  0.05115223383184857\n",
      "Epoch 2/ 8100 train_loss  0.05817817567149393\n",
      "Epoch 2/ 8200 train_loss  0.06596527159875813\n",
      "Epoch 2/ 8300 train_loss  0.07283327976020308\n",
      "Epoch 2/ 8400 train_loss  0.07982840298833202\n",
      "Epoch 2/ 8500 train_loss  0.0870148245493135\n",
      "Epoch 2/ 8600 train_loss  0.09389594559690599\n",
      "Epoch 2/ 8700 train_loss  0.10016801179079111\n",
      "Epoch 2/ 8800 train_loss  0.1062686265423247\n",
      "Epoch 2/ 8900 train_loss  0.11255868494618107\n",
      "Epoch 2/ 9000 train_loss  0.11879804150038593\n",
      "Epoch 2/ 9100 train_loss  0.125125354034956\n",
      "Epoch 2/ 9200 train_loss  0.13091431029892778\n",
      "Epoch 2/ 9300 train_loss  0.1369162695488651\n",
      "Epoch 2/ 9400 train_loss  0.1426213022046295\n",
      "Epoch 2/ 9500 train_loss  0.14782476307160258\n",
      "Epoch 2/ 9600 train_loss  0.1530721705680741\n",
      "Epoch 2/ 9700 train_loss  0.15848109523758722\n",
      "Epoch 2/ 9800 train_loss  0.16381731169129787\n",
      "Epoch 2/ 9900 train_loss  0.16931189112160813\n",
      "Epoch 2/ 10000 train_loss  0.17398964812031656\n",
      "Epoch 2/ 10100 train_loss  0.17927142598714835\n",
      "Epoch 2/ 10200 train_loss  0.18406600281470809\n",
      "Epoch 2/ 10300 train_loss  0.1892946322322207\n",
      "Epoch 2/ 10400 train_loss  0.1941823203419576\n",
      "Epoch 2/ 10500 train_loss  0.19880523739136655\n",
      "Epoch 2/ 10600 train_loss  0.2033783322729336\n",
      "Epoch 2/ 10700 train_loss  0.20784496730335097\n",
      "Epoch 2/ 10800 train_loss  0.21233216668878135\n",
      "Epoch 2/ 10900 train_loss  0.21674256558147836\n",
      "Epoch 2/ 11000 train_loss  0.22068225181257425\n",
      "Train Loss: 0.6738728470123436, Train Acc: 0.5925925925925926\n",
      "Test Loss: 0.6745870480338088, Test Acc: 0.5827922077922078\n",
      "保存模型成功! 路径为: ’./word_sense_disambiguation.bin‘\n",
      "Epoch 3/ 11100 train_loss  0.00018245058989128037\n",
      "Epoch 3/ 11200 train_loss  0.0059975934590657965\n",
      "Epoch 3/ 11300 train_loss  0.011861955550845012\n",
      "Epoch 3/ 11400 train_loss  0.017437220588644664\n",
      "Epoch 3/ 11500 train_loss  0.023135933991675073\n",
      "Epoch 3/ 11600 train_loss  0.028616218797468736\n",
      "Epoch 3/ 11700 train_loss  0.03422838681082115\n",
      "Epoch 3/ 11800 train_loss  0.03852660575091516\n",
      "Epoch 3/ 11900 train_loss  0.043437827565080786\n",
      "Epoch 3/ 12000 train_loss  0.04815373224764249\n",
      "Epoch 3/ 12100 train_loss  0.05275339314513148\n",
      "Epoch 3/ 12200 train_loss  0.05781065721002935\n",
      "Epoch 3/ 12300 train_loss  0.06273884922749803\n",
      "Epoch 3/ 12400 train_loss  0.06738314340100587\n",
      "Epoch 3/ 12500 train_loss  0.07181255916207248\n",
      "Epoch 3/ 12600 train_loss  0.07648494849113771\n",
      "Epoch 3/ 12700 train_loss  0.08111944825732384\n",
      "Epoch 3/ 12800 train_loss  0.0854684648835897\n",
      "Epoch 3/ 12900 train_loss  0.08978990937214193\n",
      "Epoch 3/ 13000 train_loss  0.0943334693771067\n",
      "Epoch 3/ 13100 train_loss  0.09818345841787407\n",
      "Epoch 3/ 13200 train_loss  0.10172912244289146\n",
      "Epoch 3/ 13300 train_loss  0.10594789550463893\n",
      "Epoch 3/ 13400 train_loss  0.10980645127649342\n",
      "Epoch 3/ 13500 train_loss  0.1136544020818129\n",
      "Epoch 3/ 13600 train_loss  0.11806951308752349\n",
      "Epoch 3/ 13700 train_loss  0.12186010383034812\n",
      "Epoch 3/ 13800 train_loss  0.12602077549369972\n",
      "Epoch 3/ 13900 train_loss  0.12984918443331286\n",
      "Epoch 3/ 14000 train_loss  0.133458895784064\n",
      "Epoch 3/ 14100 train_loss  0.13721495683119328\n",
      "Epoch 3/ 14200 train_loss  0.14095012492925868\n",
      "Epoch 3/ 14300 train_loss  0.14492355113281752\n",
      "Epoch 3/ 14400 train_loss  0.14845346899721867\n",
      "Epoch 3/ 14500 train_loss  0.15158452591575522\n",
      "Epoch 3/ 14600 train_loss  0.15502975735729785\n",
      "Epoch 3/ 14700 train_loss  0.15798693500388838\n",
      "Train Loss: 0.6439870474729148, Train Acc: 0.637199243038659\n",
      "Test Loss: 0.6778038784380784, Test Acc: 0.6071428571428571\n",
      "保存模型成功! 路径为: ’./word_sense_disambiguation.bin‘\n",
      "Epoch 4/ 14800 train_loss  0.000196466424967209\n",
      "Epoch 4/ 14900 train_loss  0.0043953808706445\n",
      "Epoch 4/ 15000 train_loss  0.008165957748702785\n",
      "Epoch 4/ 15100 train_loss  0.011971242683846254\n",
      "Epoch 4/ 15200 train_loss  0.01618995096761108\n",
      "Epoch 4/ 15300 train_loss  0.019982871246505038\n",
      "Epoch 4/ 15400 train_loss  0.023989207868753724\n",
      "Epoch 4/ 15500 train_loss  0.02716316786822929\n",
      "Epoch 4/ 15600 train_loss  0.030724314093851627\n",
      "Epoch 4/ 15700 train_loss  0.03381945407539998\n",
      "Epoch 4/ 15800 train_loss  0.037441617358074275\n",
      "Epoch 4/ 15900 train_loss  0.04076086163063856\n",
      "Epoch 4/ 16000 train_loss  0.044391881101647504\n",
      "Epoch 4/ 16100 train_loss  0.04771993063725681\n",
      "Epoch 4/ 16200 train_loss  0.050747606147907694\n",
      "Epoch 4/ 16300 train_loss  0.05427022103174435\n",
      "Epoch 4/ 16400 train_loss  0.05754451076624972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/ 16500 train_loss  0.0607889151420775\n",
      "Epoch 4/ 16600 train_loss  0.06417042167554249\n",
      "Epoch 4/ 16700 train_loss  0.06713288108260476\n",
      "Epoch 4/ 16800 train_loss  0.07008668708731342\n",
      "Epoch 4/ 16900 train_loss  0.07269320420380714\n",
      "Epoch 4/ 17000 train_loss  0.0757020097601165\n",
      "Epoch 4/ 17100 train_loss  0.07867656064470227\n",
      "Epoch 4/ 17200 train_loss  0.08193197029875515\n",
      "Epoch 4/ 17300 train_loss  0.08516799711503788\n",
      "Epoch 4/ 17400 train_loss  0.08814803578467942\n",
      "Epoch 4/ 17500 train_loss  0.091196888721361\n",
      "Epoch 4/ 17600 train_loss  0.09393407840488023\n",
      "Epoch 4/ 17700 train_loss  0.0972726060953089\n",
      "Epoch 4/ 17800 train_loss  0.1002165827286956\n",
      "Epoch 4/ 17900 train_loss  0.10324776622274161\n",
      "Epoch 4/ 18000 train_loss  0.10615057510654886\n",
      "Epoch 4/ 18100 train_loss  0.10907221116537145\n",
      "Epoch 4/ 18200 train_loss  0.11146565003381909\n",
      "Epoch 4/ 18300 train_loss  0.1138438214528586\n",
      "Epoch 4/ 18400 train_loss  0.11604941365306458\n",
      "Train Loss: 0.5919973503232504, Train Acc: 0.6774804001081374\n",
      "Test Loss: 0.7066348683566774, Test Acc: 0.6282467532467533\n",
      "保存模型成功! 路径为: ’./word_sense_disambiguation.bin‘\n",
      "Epoch 5/ 18500 train_loss  0.0001545809005331376\n",
      "Epoch 5/ 18600 train_loss  0.003231202603770403\n",
      "Epoch 5/ 18700 train_loss  0.006014595758679839\n",
      "Epoch 5/ 18800 train_loss  0.008850628103394063\n",
      "Epoch 5/ 18900 train_loss  0.011760782546907746\n",
      "Epoch 5/ 19000 train_loss  0.014399817278595899\n",
      "Epoch 5/ 19100 train_loss  0.01703225527996028\n",
      "Epoch 5/ 19200 train_loss  0.019381461316794316\n",
      "Epoch 5/ 19300 train_loss  0.021896808576550048\n",
      "Epoch 5/ 19400 train_loss  0.024246541647991303\n",
      "Epoch 5/ 19500 train_loss  0.026746770786276197\n",
      "Epoch 5/ 19600 train_loss  0.02921032511991426\n",
      "Epoch 5/ 19700 train_loss  0.031825495344564886\n",
      "Epoch 5/ 19800 train_loss  0.034198690334539074\n",
      "Epoch 5/ 19900 train_loss  0.03636303483850118\n",
      "Epoch 5/ 20000 train_loss  0.03886716753597089\n",
      "Epoch 5/ 20100 train_loss  0.041093272784566794\n",
      "Epoch 5/ 20200 train_loss  0.04343946790485709\n",
      "Epoch 5/ 20300 train_loss  0.04588427451080731\n",
      "Epoch 5/ 20400 train_loss  0.04797255744714145\n",
      "Epoch 5/ 20500 train_loss  0.05022573421831285\n",
      "Epoch 5/ 20600 train_loss  0.051838030314651166\n",
      "Epoch 5/ 20700 train_loss  0.053872363873923405\n",
      "Epoch 5/ 20800 train_loss  0.05610895231212755\n",
      "Epoch 5/ 20900 train_loss  0.05845941739843032\n",
      "Epoch 5/ 21000 train_loss  0.06088367131227646\n",
      "Epoch 5/ 21100 train_loss  0.06292674072108281\n",
      "Epoch 5/ 21200 train_loss  0.06502961122825637\n",
      "Epoch 5/ 21300 train_loss  0.06707476035918022\n",
      "Epoch 5/ 21400 train_loss  0.0694220224099945\n",
      "Epoch 5/ 21500 train_loss  0.07160357995886243\n",
      "Epoch 5/ 21600 train_loss  0.07377533289970874\n",
      "Epoch 5/ 21700 train_loss  0.07603202316940562\n",
      "Epoch 5/ 21800 train_loss  0.07848750967891743\n",
      "Epoch 5/ 21900 train_loss  0.0805867731624829\n",
      "Epoch 5/ 22000 train_loss  0.08244303375868271\n",
      "Epoch 5/ 22100 train_loss  0.08404475987916746\n",
      "Train Loss: 0.5155463714102408, Train Acc: 0.739389024060557\n",
      "Test Loss: 0.6738497753251138, Test Acc: 0.6737012987012987\n",
      "保存模型成功! 路径为: ’./word_sense_disambiguation.bin‘\n",
      "Epoch 6/ 22200 train_loss  0.0001359971215493127\n",
      "Epoch 6/ 22300 train_loss  0.0024700659138519954\n",
      "Epoch 6/ 22400 train_loss  0.004339715699615132\n",
      "Epoch 6/ 22500 train_loss  0.006270694834001967\n",
      "Epoch 6/ 22600 train_loss  0.00839468532178809\n",
      "Epoch 6/ 22700 train_loss  0.010073011466140723\n",
      "Epoch 6/ 22800 train_loss  0.012667366469495202\n",
      "Epoch 6/ 22900 train_loss  0.013965753515311765\n",
      "Epoch 6/ 23000 train_loss  0.016015938378422828\n",
      "Epoch 6/ 23100 train_loss  0.017514325536301976\n",
      "Epoch 6/ 23200 train_loss  0.019034530755839243\n",
      "Epoch 6/ 23300 train_loss  0.021381828979265313\n",
      "Epoch 6/ 23400 train_loss  0.0233802589989192\n",
      "Epoch 6/ 23500 train_loss  0.02513617275879583\n",
      "Epoch 6/ 23600 train_loss  0.026377708806704182\n",
      "Epoch 6/ 23700 train_loss  0.028357271380430045\n",
      "Epoch 6/ 23800 train_loss  0.030007970302332737\n",
      "Epoch 6/ 23900 train_loss  0.031813844724729784\n",
      "Epoch 6/ 24000 train_loss  0.0335257050740271\n",
      "Epoch 6/ 24100 train_loss  0.034917864603234906\n",
      "Epoch 6/ 24200 train_loss  0.0369513485837172\n",
      "Epoch 6/ 24300 train_loss  0.038282817488405\n",
      "Epoch 6/ 24400 train_loss  0.039596835618248394\n",
      "Epoch 6/ 24500 train_loss  0.041363912981350066\n",
      "Epoch 6/ 24600 train_loss  0.04288603255779089\n",
      "Epoch 6/ 24700 train_loss  0.04427697629214851\n",
      "Epoch 6/ 24800 train_loss  0.04556840683140848\n",
      "Epoch 6/ 24900 train_loss  0.04654770832789991\n",
      "Epoch 6/ 25000 train_loss  0.047841843301434685\n",
      "Epoch 6/ 25100 train_loss  0.049535427208009984\n",
      "Epoch 6/ 25200 train_loss  0.05064455703855045\n",
      "Epoch 6/ 25300 train_loss  0.05237033016591075\n",
      "Epoch 6/ 25400 train_loss  0.05351985380133238\n",
      "Epoch 6/ 25500 train_loss  0.055762969343219664\n",
      "Epoch 6/ 25600 train_loss  0.05723556103255274\n",
      "Epoch 6/ 25700 train_loss  0.058421114483881424\n",
      "Epoch 6/ 25800 train_loss  0.05947068819119167\n",
      "Train Loss: 0.4248324240470223, Train Acc: 0.7958907812922411\n",
      "Test Loss: 0.6737293410193845, Test Acc: 0.7159090909090909\n",
      "保存模型成功! 路径为: ’./word_sense_disambiguation.bin‘\n",
      "Epoch 7/ 25900 train_loss  0.00010778468436236429\n",
      "Epoch 7/ 26000 train_loss  0.0016467126640183347\n",
      "Epoch 7/ 26100 train_loss  0.0030781271121888614\n",
      "Epoch 7/ 26200 train_loss  0.004787002239651903\n",
      "Epoch 7/ 26300 train_loss  0.006223129339918636\n",
      "Epoch 7/ 26400 train_loss  0.007177841204101598\n",
      "Epoch 7/ 26500 train_loss  0.008993845114617866\n",
      "Epoch 7/ 26600 train_loss  0.009922117136719814\n",
      "Epoch 7/ 26700 train_loss  0.01128373176392858\n",
      "Epoch 7/ 26800 train_loss  0.012307829238863036\n",
      "Epoch 7/ 26900 train_loss  0.013675862944815945\n",
      "Epoch 7/ 27000 train_loss  0.01500253337189928\n",
      "Epoch 7/ 27100 train_loss  0.016629100945214692\n",
      "Epoch 7/ 27200 train_loss  0.018212570380854446\n",
      "Epoch 7/ 27300 train_loss  0.01930372398861843\n",
      "Epoch 7/ 27400 train_loss  0.0208172959916768\n",
      "Epoch 7/ 27500 train_loss  0.022113643032376423\n",
      "Epoch 7/ 27600 train_loss  0.02307828891937643\n",
      "Epoch 7/ 27700 train_loss  0.02445348663602349\n",
      "Epoch 7/ 27800 train_loss  0.02571242498626345\n",
      "Epoch 7/ 27900 train_loss  0.027205347632693555\n",
      "Epoch 7/ 28000 train_loss  0.028274625191735788\n",
      "Epoch 7/ 28100 train_loss  0.029605229661379566\n",
      "Epoch 7/ 28200 train_loss  0.030541558054449504\n",
      "Epoch 7/ 28300 train_loss  0.03183749426017397\n",
      "Epoch 7/ 28400 train_loss  0.032773195076358244\n",
      "Epoch 7/ 28500 train_loss  0.03386573735124119\n",
      "Epoch 7/ 28600 train_loss  0.034554070526151615\n",
      "Epoch 7/ 28700 train_loss  0.03557612493613892\n",
      "Epoch 7/ 28800 train_loss  0.03677539629618524\n",
      "Epoch 7/ 28900 train_loss  0.03792474103692808\n",
      "Epoch 7/ 29000 train_loss  0.039247070317795646\n",
      "Epoch 7/ 29100 train_loss  0.04019253126756104\n",
      "Epoch 7/ 29200 train_loss  0.04139408264358081\n",
      "Epoch 7/ 29300 train_loss  0.042613604203760876\n",
      "Epoch 7/ 29400 train_loss  0.04394218633075194\n",
      "Epoch 7/ 29500 train_loss  0.0447446258301347\n",
      "Train Loss: 0.3665001603660062, Train Acc: 0.8304947283049473\n",
      "Test Loss: 0.5229123266075145, Test Acc: 0.7775974025974026\n",
      "保存模型成功! 路径为: ’./word_sense_disambiguation.bin‘\n",
      "Epoch 8/ 29600 train_loss  9.465514583713526e-05\n",
      "Epoch 8/ 29700 train_loss  0.0010629943177603426\n",
      "Epoch 8/ 29800 train_loss  0.0017204265556088863\n",
      "Epoch 8/ 29900 train_loss  0.002958284801498675\n",
      "Epoch 8/ 30000 train_loss  0.004003825397504751\n",
      "Epoch 8/ 30100 train_loss  0.00498287589246173\n",
      "Epoch 8/ 30200 train_loss  0.005953663503868736\n",
      "Epoch 8/ 30300 train_loss  0.006694730607554147\n",
      "Epoch 8/ 30400 train_loss  0.0074342556740901716\n",
      "Epoch 8/ 30500 train_loss  0.007829541619488742\n",
      "Epoch 8/ 30600 train_loss  0.008842198023336885\n",
      "Epoch 8/ 30700 train_loss  0.009770246497243543\n",
      "Epoch 8/ 30800 train_loss  0.010932505698464498\n",
      "Epoch 8/ 30900 train_loss  0.011763581492384097\n",
      "Epoch 8/ 31000 train_loss  0.012263388476556551\n",
      "Epoch 8/ 31100 train_loss  0.01298429738625943\n",
      "Epoch 8/ 31200 train_loss  0.014017882405126638\n",
      "Epoch 8/ 31300 train_loss  0.014904390995144033\n",
      "Epoch 8/ 31400 train_loss  0.01592607966922608\n",
      "Epoch 8/ 31500 train_loss  0.016741970162251547\n",
      "Epoch 8/ 31600 train_loss  0.017693448532506727\n",
      "Epoch 8/ 31700 train_loss  0.018387209352674672\n",
      "Epoch 8/ 31800 train_loss  0.019218944276017863\n",
      "Epoch 8/ 31900 train_loss  0.020166167663628137\n",
      "Epoch 8/ 32000 train_loss  0.021117284731918182\n",
      "Epoch 8/ 32100 train_loss  0.02185092109010525\n",
      "Epoch 8/ 32200 train_loss  0.022921199804932717\n",
      "Epoch 8/ 32300 train_loss  0.023824707985886033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/ 32400 train_loss  0.024673371682549804\n",
      "Epoch 8/ 32500 train_loss  0.025925305109039568\n",
      "Epoch 8/ 32600 train_loss  0.02678580051896344\n",
      "Epoch 8/ 32700 train_loss  0.027810337589596324\n",
      "Epoch 8/ 32800 train_loss  0.028659807354129936\n",
      "Epoch 8/ 32900 train_loss  0.029516054628507066\n",
      "Epoch 8/ 33000 train_loss  0.030414708972749043\n",
      "Epoch 8/ 33100 train_loss  0.031041423172141955\n",
      "Epoch 8/ 33200 train_loss  0.03158712500258351\n",
      "Train Loss: 0.2901974600573689, Train Acc: 0.8729386320627196\n",
      "Test Loss: 0.4974842619690857, Test Acc: 0.8051948051948052\n",
      "保存模型成功! 路径为: ’./word_sense_disambiguation.bin‘\n",
      "Epoch 9/ 33300 train_loss  0.00017507369939311014\n",
      "Epoch 9/ 33400 train_loss  0.0007342588195522101\n",
      "Epoch 9/ 33500 train_loss  0.0013882202651127368\n",
      "Epoch 9/ 33600 train_loss  0.0025603138692876933\n",
      "Epoch 9/ 33700 train_loss  0.00341127187522285\n",
      "Epoch 9/ 33800 train_loss  0.0038957739473321137\n",
      "Epoch 9/ 33900 train_loss  0.00467366146921989\n",
      "Epoch 9/ 34000 train_loss  0.004947377615907876\n",
      "Epoch 9/ 34100 train_loss  0.005627822338574016\n",
      "Epoch 9/ 34200 train_loss  0.005945383023272639\n",
      "Epoch 9/ 34300 train_loss  0.00680696531865242\n",
      "Epoch 9/ 34400 train_loss  0.00755413926268703\n",
      "Epoch 9/ 34500 train_loss  0.008404011832628098\n",
      "Epoch 9/ 34600 train_loss  0.008946322358555695\n",
      "Epoch 9/ 34700 train_loss  0.009155294662417279\n",
      "Epoch 9/ 34800 train_loss  0.009826705227109805\n",
      "Epoch 9/ 34900 train_loss  0.010301754035249024\n",
      "Epoch 9/ 35000 train_loss  0.0109029660363101\n",
      "Epoch 9/ 35100 train_loss  0.01174400092824155\n",
      "Epoch 9/ 35200 train_loss  0.012159500825574118\n",
      "Epoch 9/ 35300 train_loss  0.012978124827459232\n",
      "Epoch 9/ 35400 train_loss  0.013682163380490074\n",
      "Epoch 9/ 35500 train_loss  0.014375828675771364\n",
      "Epoch 9/ 35600 train_loss  0.015122345918943063\n",
      "Epoch 9/ 35700 train_loss  0.015699921169225837\n",
      "Epoch 9/ 35800 train_loss  0.016381658388732217\n",
      "Epoch 9/ 35900 train_loss  0.01685782828800575\n",
      "Epoch 9/ 36000 train_loss  0.017525020334862213\n",
      "Epoch 9/ 36100 train_loss  0.0187483579195366\n",
      "Epoch 9/ 36200 train_loss  0.019680332589868833\n",
      "Epoch 9/ 36300 train_loss  0.020403206356926513\n",
      "Epoch 9/ 36400 train_loss  0.02102165858559268\n",
      "Epoch 9/ 36500 train_loss  0.021615806136247473\n",
      "Epoch 9/ 36600 train_loss  0.022645187172910166\n",
      "Epoch 9/ 36700 train_loss  0.023176784524052595\n",
      "Epoch 9/ 36800 train_loss  0.02353537579082282\n",
      "Epoch 9/ 36900 train_loss  0.024057229617625624\n",
      "Train Loss: 0.2462405382772547, Train Acc: 0.9010543390105434\n",
      "Test Loss: 0.39206468432821684, Test Acc: 0.8327922077922078\n",
      "保存模型成功! 路径为: ’./word_sense_disambiguation.bin‘\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(embedding_size, hidden_size, num_layers, num_directions, num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.BCELoss()\n",
    "model = train(model, train_set, test_set, optimizer, loss_func, epochs, model_save_path='model/word_sense_disambiguation.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "苹果，又称柰或林檎，是苹果树（学名：Malus domestica）的果实。苹果树是蔷薇科苹果亚科苹果属植物，为落叶乔木，果实富含矿物质和维生素，是人们最常食用的水果之一。\n"
     ]
    }
   ],
   "source": [
    "origin = '苹果12马上就要发布了。'\n",
    "entity_descs = ['《苹果》是由李玉执导，范冰冰、佟大为、梁家辉、金燕玲领衔主演的黑色幽默剧情电影。于2007年5月18日在中国大陆上映。影片讲述了两个贫富家庭之间离奇诡异的矛盾冲突和感情错位的戏剧性故事。'\n",
    ", 'iPhone 是美国苹果公司研发的iPhone手机，采用了直面边框设计，支持5G，搭载A14 Bionic芯片，双镜头后置摄像头系统。支持北斗导航 [1]  ，有黑色、白色、红色、绿色、蓝色五种配色。'\n",
    "]\n",
    "\n",
    "origin = '我们家的苹果快要熟了，你要不要来几斤？'\n",
    "entity_descs = ['《苹果》是由李玉执导，范冰冰、佟大为、梁家辉、金燕玲领衔主演的黑色幽默剧情电影。于2007年5月18日在中国大陆上映。影片讲述了两个贫富家庭之间离奇诡异的矛盾冲突和感情错位的戏剧性故事。'\n",
    ", 'iPhone 是美国苹果公司研发的iPhone手机，采用了直面边框设计，支持5G，搭载A14 Bionic芯片，双镜头后置摄像头系统。支持北斗导航 [1]  ，有黑色、白色、红色、绿色、蓝色五种配色。',\n",
    " '苹果，又称柰或林檎，是苹果 树（学名：Malus domestica）的果实。苹果树是蔷薇科苹果亚科苹果属植物，为落叶乔木，果实富含矿物质和维生素，是人们最常食用的水果之一。'                    \n",
    "]\n",
    "right_text = predict(model, origin, entity_descs)\n",
    "print(right_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
